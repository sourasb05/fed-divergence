============================================================
Summary of training process:
FL Algorithm: FedProx
model: MCLR
optimizer: SGD
Batch size: 16
Global_iters: 10
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
Proximal hyperparameter 1.0
============================================================
/proj/bhuyan24/fed-divergence
Mclr_Logistic(
  (fc1): Linear(in_features=784, out_features=10, bias=True)
)
CrossEntropyLoss()
MNIST
10
10
from fedbase client: Parameter containing:
tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],
        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],
        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],
        ...,
        [ 0.0237,  0.0103, -0.0219,  ...,  0.0088, -0.0009,  0.0009],
        [ 0.0144, -0.0336, -0.0346,  ..., -0.0222, -0.0025, -0.0138],
        [-0.0196, -0.0118,  0.0230,  ..., -0.0202,  0.0172,  0.0355]],
       device='cuda:0', requires_grad=True)
from fedbase client: Parameter containing:
tensor([-0.0092,  0.0277, -0.0110, -0.0335, -0.0282, -0.0120,  0.0338,  0.0150,
         0.0308,  0.0181], device='cuda:0', requires_grad=True)
number of selected users 10
