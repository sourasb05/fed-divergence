============================================================
Summary of training process:
FL Algorithm: MOON_KL
model: CNN
optimizer: SGD
Batch size: 124
Global_iters: 200
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
============================================================
/proj/bhuyan24/fed-divergence
CIFAR10
./data/data/noisy/0.4_50_3/train/cifa_train.json
  0%|          | 0/200 [00:00<?, ?it/s]============================================================
Summary of training process:
FL Algorithm: MOON_KL
model: CNN
optimizer: SGD
Batch size: 124
Global_iters: 200
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
============================================================
/proj/bhuyan24/fed-divergence
CIFAR10
./data/data/noisy/0.4_50_3/train/cifa_train.json
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:30<1:39:48, 30.09s/it]  1%|          | 2/200 [00:50<1:20:54, 24.52s/it]  0%|          | 1/200 [00:42<2:21:23, 42.63s/it]  2%|▏         | 3/200 [01:11<1:14:34, 22.71s/it]  1%|          | 2/200 [01:02<1:36:33, 29.26s/it]  2%|▏         | 4/200 [01:31<1:11:21, 21.84s/it]  2%|▏         | 3/200 [01:22<1:22:19, 25.07s/it]  2%|▎         | 5/200 [01:52<1:09:35, 21.41s/it]  2%|▏         | 4/200 [01:42<1:15:26, 23.10s/it]  3%|▎         | 6/200 [02:13<1:08:28, 21.18s/it]  2%|▎         | 5/200 [02:03<1:12:01, 22.16s/it]  4%|▎         | 7/200 [02:34<1:07:48, 21.08s/it]  3%|▎         | 6/200 [02:23<1:09:58, 21.64s/it]  4%|▍         | 8/200 [02:55<1:07:29, 21.09s/it]  4%|▎         | 7/200 [02:44<1:08:37, 21.34s/it]  4%|▍         | 9/200 [03:16<1:07:24, 21.17s/it]  4%|▍         | 8/200 [03:05<1:07:35, 21.12s/it]  5%|▌         | 10/200 [03:38<1:07:23, 21.28s/it]  4%|▍         | 9/200 [03:26<1:07:24, 21.18s/it]  6%|▌         | 11/200 [03:59<1:07:21, 21.38s/it]  5%|▌         | 10/200 [03:48<1:07:25, 21.29s/it]  6%|▌         | 12/200 [04:21<1:07:31, 21.55s/it]  6%|▌         | 11/200 [04:11<1:08:51, 21.86s/it]  6%|▋         | 13/200 [04:43<1:07:32, 21.67s/it]  6%|▌         | 12/200 [04:33<1:08:42, 21.93s/it]  7%|▋         | 14/200 [05:05<1:07:31, 21.78s/it]  6%|▋         | 13/200 [04:55<1:08:10, 21.88s/it]  8%|▊         | 15/200 [05:27<1:07:19, 21.83s/it]  7%|▋         | 14/200 [05:16<1:07:49, 21.88s/it]  8%|▊         | 16/200 [05:49<1:06:48, 21.79s/it]  8%|▊         | 15/200 [05:38<1:07:20, 21.84s/it]  8%|▊         | 17/200 [06:10<1:05:34, 21.50s/it]  8%|▊         | 16/200 [06:00<1:06:44, 21.76s/it]  9%|▉         | 18/200 [06:30<1:04:27, 21.25s/it]  8%|▊         | 17/200 [06:21<1:05:48, 21.58s/it] 10%|▉         | 19/200 [06:51<1:03:26, 21.03s/it]  9%|▉         | 18/200 [06:42<1:04:39, 21.32s/it] 10%|█         | 20/200 [07:11<1:02:34, 20.86s/it] 10%|▉         | 19/200 [07:02<1:03:47, 21.15s/it] 10%|█         | 21/200 [07:32<1:01:54, 20.75s/it] 10%|█         | 20/200 [07:23<1:03:10, 21.06s/it] 11%|█         | 22/200 [07:52<1:01:36, 20.77s/it] 10%|█         | 21/200 [07:44<1:02:21, 20.90s/it] 12%|█▏        | 23/200 [08:13<1:01:17, 20.78s/it] 11%|█         | 22/200 [08:04<1:01:36, 20.77s/it] 12%|█▏        | 24/200 [08:34<1:00:46, 20.72s/it] 12%|█▏        | 23/200 [08:25<1:01:17, 20.78s/it] 12%|█▎        | 25/200 [08:55<1:00:21, 20.70s/it] 12%|█▏        | 24/200 [08:46<1:00:45, 20.71s/it] 13%|█▎        | 26/200 [09:16<1:00:31, 20.87s/it] 12%|█▎        | 25/200 [09:06<1:00:16, 20.66s/it] 14%|█▎        | 27/200 [09:37<1:00:12, 20.88s/it] 13%|█▎        | 26/200 [09:27<59:50, 20.64s/it]   14%|█▍        | 28/200 [09:57<59:45, 20.85s/it]   14%|█▎        | 27/200 [09:47<59:24, 20.60s/it] 14%|█▍        | 29/200 [10:18<59:25, 20.85s/it] 14%|█▍        | 28/200 [10:08<59:02, 20.60s/it] 15%|█▌        | 30/200 [10:39<59:04, 20.85s/it] 14%|█▍        | 29/200 [10:28<58:45, 20.62s/it] 16%|█▌        | 31/200 [11:00<58:43, 20.85s/it] 15%|█▌        | 30/200 [10:49<58:30, 20.65s/it] 16%|█▌        | 32/200 [11:21<58:29, 20.89s/it] 16%|█▌        | 31/200 [11:10<58:27, 20.76s/it] 16%|█▋        | 33/200 [11:42<58:20, 20.96s/it] 16%|█▌        | 32/200 [11:31<58:22, 20.85s/it] 17%|█▋        | 34/200 [12:03<58:06, 21.00s/it] 16%|█▋        | 33/200 [11:52<57:58, 20.83s/it] 18%|█▊        | 35/200 [12:24<57:40, 20.97s/it] 17%|█▋        | 34/200 [12:13<57:50, 20.90s/it] 18%|█▊        | 36/200 [12:45<57:22, 20.99s/it] 18%|█▊        | 35/200 [12:34<57:37, 20.96s/it] 18%|█▊        | 37/200 [13:06<56:57, 20.97s/it] 18%|█▊        | 36/200 [12:55<57:25, 21.01s/it] 19%|█▉        | 38/200 [13:27<56:37, 20.97s/it] 18%|█▊        | 37/200 [13:16<57:02, 20.99s/it] 20%|█▉        | 39/200 [13:48<56:05, 20.90s/it] 19%|█▉        | 38/200 [13:37<56:35, 20.96s/it] 20%|██        | 40/200 [14:08<55:33, 20.83s/it] 20%|█▉        | 39/200 [13:58<56:09, 20.93s/it] 20%|██        | 41/200 [14:29<55:06, 20.79s/it] 20%|██        | 40/200 [14:19<55:43, 20.90s/it] 21%|██        | 42/200 [14:50<54:38, 20.75s/it] 20%|██        | 41/200 [14:40<55:18, 20.87s/it] 22%|██▏       | 43/200 [15:10<54:13, 20.72s/it] 21%|██        | 42/200 [15:01<55:07, 20.93s/it] 22%|██▏       | 44/200 [15:31<53:45, 20.68s/it] 22%|██▏       | 43/200 [15:22<54:52, 20.97s/it] 22%|██▎       | 45/200 [15:52<53:18, 20.64s/it] 22%|██▏       | 44/200 [15:43<54:21, 20.91s/it] 23%|██▎       | 46/200 [16:12<52:53, 20.60s/it] 22%|██▎       | 45/200 [16:03<53:55, 20.87s/it] 24%|██▎       | 47/200 [16:33<52:28, 20.58s/it] 23%|██▎       | 46/200 [16:24<53:28, 20.83s/it] 24%|██▍       | 48/200 [16:53<52:14, 20.62s/it] 24%|██▎       | 47/200 [16:45<53:01, 20.80s/it] 24%|██▍       | 49/200 [17:15<52:17, 20.78s/it] 24%|██▍       | 48/200 [17:05<52:29, 20.72s/it] 25%|██▌       | 50/200 [17:36<52:09, 20.87s/it] 24%|██▍       | 49/200 [17:26<52:10, 20.73s/it] 26%|██▌       | 51/200 [17:56<51:51, 20.88s/it] 25%|██▌       | 50/200 [17:47<51:48, 20.72s/it] 26%|██▌       | 52/200 [18:18<51:38, 20.93s/it] 26%|██▌       | 51/200 [18:07<51:21, 20.68s/it] 26%|██▋       | 53/200 [18:39<51:19, 20.95s/it] 26%|██▌       | 52/200 [18:28<51:00, 20.68s/it] 27%|██▋       | 54/200 [18:59<50:57, 20.94s/it] 26%|██▋       | 53/200 [18:49<50:52, 20.77s/it] 28%|██▊       | 55/200 [19:20<50:19, 20.83s/it] 27%|██▋       | 54/200 [19:10<50:28, 20.74s/it] 28%|██▊       | 56/200 [19:40<49:39, 20.69s/it] 28%|██▊       | 55/200 [19:30<49:59, 20.69s/it] 28%|██▊       | 57/200 [20:01<49:03, 20.58s/it] 28%|██▊       | 56/200 [19:51<49:35, 20.67s/it] 29%|██▉       | 58/200 [20:21<48:35, 20.53s/it] 28%|██▊       | 57/200 [20:12<49:23, 20.72s/it] 30%|██▉       | 59/200 [20:42<48:08, 20.48s/it] 29%|██▉       | 58/200 [20:33<49:04, 20.74s/it] 30%|███       | 60/200 [21:02<47:38, 20.42s/it] 30%|██▉       | 59/200 [20:53<48:38, 20.70s/it] 30%|███       | 61/200 [21:22<47:13, 20.38s/it] 30%|███       | 60/200 [21:14<48:22, 20.73s/it] 31%|███       | 62/200 [21:42<46:49, 20.36s/it] 30%|███       | 61/200 [21:34<47:54, 20.68s/it] 32%|███▏      | 63/200 [22:03<46:30, 20.37s/it] 31%|███       | 62/200 [21:55<47:31, 20.67s/it] 32%|███▏      | 64/200 [22:23<46:19, 20.44s/it] 32%|███▏      | 63/200 [22:16<47:11, 20.67s/it] 32%|███▎      | 65/200 [22:44<46:16, 20.57s/it] 32%|███▏      | 64/200 [22:36<46:42, 20.60s/it] 33%|███▎      | 66/200 [23:05<46:01, 20.61s/it] 32%|███▎      | 65/200 [22:57<46:17, 20.58s/it] 34%|███▎      | 67/200 [23:26<45:48, 20.67s/it] 33%|███▎      | 66/200 [23:17<45:55, 20.56s/it] 34%|███▍      | 68/200 [23:47<45:37, 20.74s/it] 34%|███▎      | 67/200 [23:38<45:35, 20.57s/it] 34%|███▍      | 69/200 [24:08<45:23, 20.79s/it] 34%|███▍      | 68/200 [23:58<45:10, 20.54s/it] 35%|███▌      | 70/200 [24:28<45:04, 20.81s/it] 34%|███▍      | 69/200 [24:19<44:49, 20.53s/it] 36%|███▌      | 71/200 [24:49<44:37, 20.76s/it] 35%|███▌      | 70/200 [24:39<44:23, 20.49s/it] 36%|███▌      | 72/200 [25:10<44:10, 20.71s/it] 36%|███▌      | 71/200 [25:00<44:02, 20.48s/it] 36%|███▋      | 73/200 [25:30<43:42, 20.65s/it] 36%|███▌      | 72/200 [25:20<43:36, 20.44s/it] 37%|███▋      | 74/200 [25:51<43:11, 20.57s/it] 36%|███▋      | 73/200 [25:40<43:10, 20.40s/it] 38%|███▊      | 75/200 [26:11<42:46, 20.53s/it] 37%|███▋      | 74/200 [26:01<42:56, 20.45s/it] 38%|███▊      | 76/200 [26:32<42:37, 20.62s/it] 38%|███▊      | 75/200 [26:21<42:31, 20.41s/it] 38%|███▊      | 77/200 [26:53<42:25, 20.70s/it] 38%|███▊      | 76/200 [26:42<42:12, 20.42s/it] 39%|███▉      | 78/200 [27:13<42:06, 20.71s/it] 38%|███▊      | 77/200 [27:02<41:59, 20.48s/it] 40%|███▉      | 79/200 [27:34<41:48, 20.73s/it] 39%|███▉      | 78/200 [27:23<41:46, 20.55s/it] 40%|████      | 80/200 [27:55<41:22, 20.69s/it] 40%|███▉      | 79/200 [27:44<41:25, 20.54s/it] 40%|████      | 81/200 [28:16<41:07, 20.73s/it] 40%|████      | 80/200 [28:04<40:59, 20.49s/it] 41%|████      | 82/200 [28:36<40:44, 20.72s/it] 40%|████      | 81/200 [28:25<40:45, 20.55s/it] 42%|████▏     | 83/200 [28:57<40:27, 20.75s/it] 41%|████      | 82/200 [28:45<40:29, 20.59s/it] 42%|████▏     | 84/200 [29:18<40:10, 20.78s/it] 42%|████▏     | 83/200 [29:06<39:56, 20.48s/it] 42%|████▎     | 85/200 [29:39<39:45, 20.75s/it] 42%|████▏     | 84/200 [29:26<39:39, 20.52s/it] 43%|████▎     | 86/200 [29:59<39:18, 20.69s/it] 42%|████▎     | 85/200 [29:47<39:22, 20.54s/it] 44%|████▎     | 87/200 [30:20<38:57, 20.69s/it] 43%|████▎     | 86/200 [30:07<39:02, 20.55s/it] 44%|████▍     | 88/200 [30:41<38:37, 20.69s/it] 44%|████▎     | 87/200 [30:28<38:38, 20.52s/it] 44%|████▍     | 89/200 [31:01<38:18, 20.71s/it] 44%|████▍     | 88/200 [30:48<38:18, 20.52s/it] 45%|████▌     | 90/200 [31:22<37:56, 20.70s/it] 44%|████▍     | 89/200 [31:09<37:58, 20.52s/it] 46%|████▌     | 91/200 [31:42<37:27, 20.62s/it] 45%|████▌     | 90/200 [31:29<37:28, 20.44s/it] 46%|████▌     | 92/200 [32:03<37:02, 20.58s/it] 46%|████▌     | 91/200 [31:49<36:57, 20.34s/it] 46%|████▋     | 93/200 [32:23<36:36, 20.53s/it] 46%|████▌     | 92/200 [32:09<36:30, 20.28s/it] 47%|████▋     | 94/200 [32:44<36:09, 20.47s/it] 46%|████▋     | 93/200 [32:30<36:10, 20.28s/it] 48%|████▊     | 95/200 [33:04<35:47, 20.45s/it] 47%|████▋     | 94/200 [32:50<35:53, 20.31s/it] 48%|████▊     | 96/200 [33:25<35:26, 20.44s/it] 48%|████▊     | 95/200 [33:10<35:30, 20.29s/it] 48%|████▊     | 97/200 [33:45<35:06, 20.45s/it] 48%|████▊     | 96/200 [33:31<35:12, 20.31s/it] 49%|████▉     | 98/200 [34:05<34:46, 20.46s/it] 48%|████▊     | 97/200 [33:51<34:59, 20.38s/it] 50%|████▉     | 99/200 [34:26<34:35, 20.55s/it] 49%|████▉     | 98/200 [34:12<34:41, 20.41s/it] 50%|█████     | 100/200 [34:47<34:07, 20.47s/it] 50%|████▉     | 99/200 [34:32<34:27, 20.47s/it] 50%|█████     | 101/200 [35:07<33:41, 20.42s/it] 50%|█████     | 100/200 [34:53<34:12, 20.53s/it] 51%|█████     | 102/200 [35:27<33:18, 20.39s/it] 50%|█████     | 101/200 [35:13<33:50, 20.51s/it] 52%|█████▏    | 103/200 [35:48<32:57, 20.38s/it] 51%|█████     | 102/200 [35:34<33:21, 20.43s/it] 52%|█████▏    | 104/200 [36:08<32:27, 20.29s/it] 52%|█████▏    | 103/200 [35:54<33:01, 20.43s/it] 52%|█████▎    | 105/200 [36:28<32:02, 20.23s/it] 52%|█████▏    | 104/200 [36:15<32:44, 20.46s/it] 53%|█████▎    | 106/200 [36:48<31:37, 20.19s/it] 52%|█████▎    | 105/200 [36:35<32:21, 20.44s/it] 54%|█████▎    | 107/200 [37:08<31:19, 20.21s/it] 53%|█████▎    | 106/200 [36:56<32:06, 20.49s/it] 54%|█████▍    | 108/200 [37:28<30:55, 20.17s/it] 54%|█████▎    | 107/200 [37:16<31:45, 20.49s/it] 55%|█████▍    | 109/200 [37:48<30:38, 20.20s/it] 54%|█████▍    | 108/200 [37:36<31:17, 20.40s/it] 55%|█████▌    | 110/200 [38:08<30:15, 20.17s/it] 55%|█████▍    | 109/200 [37:56<30:51, 20.34s/it] 56%|█████▌    | 111/200 [38:28<29:50, 20.12s/it] 55%|█████▌    | 110/200 [38:17<30:31, 20.35s/it] 56%|█████▌    | 112/200 [38:48<29:24, 20.05s/it] 56%|█████▌    | 111/200 [38:37<30:10, 20.35s/it] 56%|█████▋    | 113/200 [39:08<29:00, 20.01s/it] 56%|█████▌    | 112/200 [38:58<29:54, 20.39s/it] 57%|█████▋    | 114/200 [39:28<28:45, 20.06s/it] 56%|█████▋    | 113/200 [39:18<29:28, 20.33s/it] 57%|█████▊    | 115/200 [39:48<28:18, 19.98s/it] 57%|█████▋    | 114/200 [39:38<29:04, 20.29s/it] 58%|█████▊    | 116/200 [40:08<27:57, 19.97s/it] 57%|█████▊    | 115/200 [39:58<28:41, 20.26s/it] 58%|█████▊    | 117/200 [40:29<27:46, 20.08s/it] 58%|█████▊    | 116/200 [40:18<28:11, 20.13s/it] 59%|█████▉    | 118/200 [40:49<27:30, 20.13s/it] 58%|█████▊    | 117/200 [40:38<27:42, 20.03s/it] 60%|█████▉    | 119/200 [41:09<27:11, 20.14s/it] 59%|█████▉    | 118/200 [40:58<27:17, 19.97s/it] 60%|██████    | 120/200 [41:29<26:51, 20.14s/it] 60%|█████▉    | 119/200 [41:18<27:01, 20.02s/it] 60%|██████    | 121/200 [41:49<26:34, 20.19s/it] 60%|██████    | 120/200 [41:38<26:43, 20.05s/it] 61%|██████    | 122/200 [42:09<26:08, 20.10s/it] 60%|██████    | 121/200 [41:58<26:22, 20.03s/it] 62%|██████▏   | 123/200 [42:29<25:41, 20.01s/it] 61%|██████    | 122/200 [42:18<25:59, 19.99s/it] 62%|██████▏   | 124/200 [42:49<25:15, 19.94s/it] 62%|██████▏   | 123/200 [42:38<25:37, 19.97s/it] 62%|██████▎   | 125/200 [43:09<24:51, 19.89s/it] 62%|██████▏   | 124/200 [42:57<25:12, 19.90s/it] 63%|██████▎   | 126/200 [43:28<24:27, 19.83s/it] 62%|██████▎   | 125/200 [43:17<24:47, 19.84s/it] 64%|██████▎   | 127/200 [43:48<24:04, 19.79s/it] 63%|██████▎   | 126/200 [43:37<24:22, 19.77s/it] 64%|██████▍   | 128/200 [44:08<23:56, 19.95s/it] 64%|██████▎   | 127/200 [43:56<24:01, 19.75s/it] 64%|██████▍   | 129/200 [44:29<23:45, 20.07s/it] 64%|██████▍   | 128/200 [44:16<23:38, 19.71s/it] 65%|██████▌   | 130/200 [44:49<23:22, 20.03s/it] 64%|██████▍   | 129/200 [44:36<23:17, 19.68s/it] 66%|██████▌   | 131/200 [45:08<22:57, 19.96s/it] 65%|██████▌   | 130/200 [44:56<23:02, 19.76s/it] 66%|██████▌   | 132/200 [45:28<22:32, 19.90s/it] 66%|██████▌   | 131/200 [45:15<22:39, 19.70s/it] 66%|██████▋   | 133/200 [45:48<22:05, 19.78s/it] 66%|██████▌   | 132/200 [45:35<22:17, 19.66s/it] 67%|██████▋   | 134/200 [46:07<21:42, 19.73s/it] 66%|██████▋   | 133/200 [45:54<21:57, 19.66s/it] 68%|██████▊   | 135/200 [46:27<21:26, 19.79s/it] 67%|██████▋   | 134/200 [46:14<21:38, 19.67s/it] 68%|██████▊   | 136/200 [46:47<21:06, 19.79s/it] 68%|██████▊   | 135/200 [46:34<21:16, 19.64s/it] 68%|██████▊   | 137/200 [47:07<20:48, 19.82s/it] 68%|██████▊   | 136/200 [46:54<21:03, 19.74s/it] 69%|██████▉   | 138/200 [47:27<20:29, 19.82s/it] 68%|██████▊   | 137/200 [47:13<20:44, 19.76s/it] 70%|██████▉   | 139/200 [47:47<20:12, 19.87s/it] 69%|██████▉   | 138/200 [47:33<20:24, 19.74s/it] 70%|███████   | 140/200 [48:07<19:54, 19.90s/it] 70%|██████▉   | 139/200 [47:53<20:02, 19.71s/it] 70%|███████   | 141/200 [48:27<19:34, 19.91s/it] 70%|███████   | 140/200 [48:13<19:43, 19.73s/it] 71%|███████   | 142/200 [48:46<19:13, 19.89s/it] 70%|███████   | 141/200 [48:32<19:25, 19.75s/it] 72%|███████▏  | 143/200 [49:06<18:51, 19.85s/it] 71%|███████   | 142/200 [48:52<19:03, 19.71s/it] 72%|███████▏  | 144/200 [49:26<18:29, 19.81s/it] 72%|███████▏  | 143/200 [49:12<18:45, 19.74s/it] 72%|███████▎  | 145/200 [49:46<18:11, 19.85s/it] 72%|███████▏  | 144/200 [49:32<18:25, 19.75s/it] 73%|███████▎  | 146/200 [50:06<17:51, 19.83s/it] 72%|███████▎  | 145/200 [49:51<18:02, 19.69s/it] 74%|███████▎  | 147/200 [50:25<17:30, 19.82s/it] 73%|███████▎  | 146/200 [50:11<17:39, 19.62s/it] 74%|███████▍  | 148/200 [50:45<17:06, 19.75s/it] 74%|███████▎  | 147/200 [50:30<17:17, 19.57s/it] 74%|███████▍  | 148/200 [50:49<16:53, 19.50s/it] 74%|███████▍  | 149/200 [51:05<16:44, 19.70s/it] 74%|███████▍  | 149/200 [51:09<16:34, 19.49s/it] 75%|███████▌  | 150/200 [51:24<16:23, 19.67s/it] 75%|███████▌  | 150/200 [51:28<16:13, 19.48s/it] 76%|███████▌  | 151/200 [51:44<16:03, 19.66s/it] 76%|███████▌  | 151/200 [51:48<15:53, 19.46s/it] 76%|███████▌  | 152/200 [52:03<15:42, 19.63s/it] 76%|███████▌  | 152/200 [52:07<15:37, 19.53s/it] 76%|███████▋  | 153/200 [52:23<15:21, 19.60s/it] 76%|███████▋  | 153/200 [52:27<15:15, 19.48s/it] 77%|███████▋  | 154/200 [52:43<15:01, 19.60s/it] 77%|███████▋  | 154/200 [52:46<14:56, 19.50s/it] 78%|███████▊  | 155/200 [53:02<14:44, 19.65s/it] 78%|███████▊  | 155/200 [53:06<14:35, 19.46s/it] 78%|███████▊  | 156/200 [53:22<14:24, 19.65s/it] 78%|███████▊  | 156/200 [53:25<14:15, 19.45s/it] 78%|███████▊  | 157/200 [53:42<14:04, 19.64s/it] 78%|███████▊  | 157/200 [53:44<13:55, 19.43s/it] 79%|███████▉  | 158/200 [54:01<13:44, 19.63s/it] 79%|███████▉  | 158/200 [54:04<13:36, 19.44s/it] 80%|███████▉  | 159/200 [54:21<13:24, 19.62s/it] 80%|███████▉  | 159/200 [54:23<13:16, 19.43s/it] 80%|████████  | 160/200 [54:40<13:04, 19.62s/it] 80%|████████  | 160/200 [54:43<12:55, 19.40s/it] 80%|████████  | 161/200 [55:00<12:45, 19.63s/it] 80%|████████  | 161/200 [55:02<12:36, 19.39s/it] 81%|████████  | 162/200 [55:20<12:25, 19.63s/it] 81%|████████  | 162/200 [55:21<12:16, 19.39s/it] 82%|████████▏ | 163/200 [55:39<12:05, 19.61s/it] 82%|████████▏ | 163/200 [55:41<11:56, 19.37s/it] 82%|████████▏ | 164/200 [55:59<11:47, 19.64s/it] 82%|████████▏ | 164/200 [56:00<11:36, 19.36s/it] 82%|████████▎ | 165/200 [56:19<11:26, 19.61s/it] 82%|████████▎ | 165/200 [56:19<11:17, 19.35s/it] 83%|████████▎ | 166/200 [56:38<11:06, 19.60s/it] 83%|████████▎ | 166/200 [56:39<10:57, 19.35s/it] 84%|████████▎ | 167/200 [56:58<10:44, 19.54s/it] 84%|████████▎ | 167/200 [56:58<10:39, 19.39s/it] 84%|████████▍ | 168/200 [57:17<10:25, 19.55s/it] 84%|████████▍ | 168/200 [57:18<10:19, 19.36s/it] 84%|████████▍ | 169/200 [57:36<10:03, 19.46s/it] 84%|████████▍ | 169/200 [57:37<09:59, 19.33s/it] 85%|████████▌ | 170/200 [57:56<09:41, 19.39s/it] 85%|████████▌ | 170/200 [57:56<09:39, 19.31s/it] 86%|████████▌ | 171/200 [58:15<09:20, 19.33s/it] 86%|████████▌ | 171/200 [58:16<09:22, 19.39s/it] 86%|████████▌ | 172/200 [58:34<08:59, 19.28s/it] 86%|████████▌ | 172/200 [58:35<09:04, 19.45s/it] 86%|████████▋ | 173/200 [58:53<08:39, 19.24s/it] 86%|████████▋ | 173/200 [58:55<08:45, 19.46s/it] 87%|████████▋ | 174/200 [59:12<08:20, 19.26s/it] 87%|████████▋ | 174/200 [59:14<08:25, 19.44s/it] 88%|████████▊ | 175/200 [59:32<08:02, 19.30s/it] 88%|████████▊ | 175/200 [59:33<08:04, 19.38s/it] 88%|████████▊ | 176/200 [59:51<07:43, 19.33s/it] 88%|████████▊ | 176/200 [59:53<07:44, 19.34s/it] 88%|████████▊ | 177/200 [1:00:10<07:24, 19.33s/it] 88%|████████▊ | 177/200 [1:00:12<07:25, 19.36s/it] 89%|████████▉ | 178/200 [1:00:30<07:05, 19.36s/it] 89%|████████▉ | 178/200 [1:00:31<07:06, 19.38s/it] 90%|████████▉ | 179/200 [1:00:49<06:46, 19.36s/it] 90%|████████▉ | 179/200 [1:00:51<06:47, 19.38s/it] 90%|█████████ | 180/200 [1:01:09<06:27, 19.37s/it] 90%|█████████ | 180/200 [1:01:10<06:27, 19.36s/it] 90%|█████████ | 181/200 [1:01:28<06:07, 19.36s/it] 90%|█████████ | 181/200 [1:01:29<06:08, 19.37s/it] 91%|█████████ | 182/200 [1:01:48<05:49, 19.44s/it] 91%|█████████ | 182/200 [1:01:49<05:48, 19.35s/it] 92%|█████████▏| 183/200 [1:02:07<05:31, 19.49s/it] 92%|█████████▏| 183/200 [1:02:08<05:28, 19.33s/it] 92%|█████████▏| 184/200 [1:02:27<05:11, 19.48s/it] 92%|█████████▏| 184/200 [1:02:27<05:08, 19.30s/it] 92%|█████████▎| 185/200 [1:02:46<04:51, 19.44s/it] 92%|█████████▎| 185/200 [1:02:47<04:49, 19.28s/it] 93%|█████████▎| 186/200 [1:03:05<04:31, 19.41s/it] 93%|█████████▎| 186/200 [1:03:06<04:29, 19.27s/it] 94%|█████████▎| 187/200 [1:03:25<04:12, 19.40s/it] 94%|█████████▎| 187/200 [1:03:25<04:10, 19.24s/it] 94%|█████████▍| 188/200 [1:03:44<03:52, 19.37s/it] 94%|█████████▍| 188/200 [1:03:44<03:50, 19.24s/it] 94%|█████████▍| 189/200 [1:04:03<03:32, 19.35s/it] 94%|█████████▍| 189/200 [1:04:03<03:31, 19.21s/it] 95%|█████████▌| 190/200 [1:04:23<03:13, 19.34s/it] 95%|█████████▌| 190/200 [1:04:23<03:12, 19.26s/it] 96%|█████████▌| 191/200 [1:04:42<02:54, 19.36s/it] 96%|█████████▌| 191/200 [1:04:42<02:54, 19.35s/it] 96%|█████████▌| 192/200 [1:05:01<02:34, 19.33s/it] 96%|█████████▌| 192/200 [1:05:02<02:35, 19.43s/it] 96%|█████████▋| 193/200 [1:05:21<02:15, 19.32s/it] 96%|█████████▋| 193/200 [1:05:22<02:16, 19.50s/it] 97%|█████████▋| 194/200 [1:05:40<01:55, 19.30s/it] 97%|█████████▋| 194/200 [1:05:41<01:56, 19.44s/it] 98%|█████████▊| 195/200 [1:05:59<01:36, 19.28s/it] 98%|█████████▊| 195/200 [1:06:00<01:36, 19.36s/it] 98%|█████████▊| 196/200 [1:06:18<01:17, 19.30s/it] 98%|█████████▊| 196/200 [1:06:19<01:17, 19.29s/it] 98%|█████████▊| 197/200 [1:06:38<00:58, 19.35s/it] 98%|█████████▊| 197/200 [1:06:38<00:57, 19.26s/it] 99%|█████████▉| 198/200 [1:06:57<00:38, 19.38s/it] 99%|█████████▉| 198/200 [1:06:58<00:38, 19.23s/it]100%|█████████▉| 199/200 [1:07:17<00:19, 19.42s/it]100%|█████████▉| 199/200 [1:07:17<00:19, 19.21s/it]100%|██████████| 200/200 [1:07:36<00:00, 19.45s/it]100%|██████████| 200/200 [1:07:36<00:00, 20.28s/it]
50
50
number of selected users 50
Global Trainning Accurancy: 0.10111646519415564
Global Trainning Loss: 2.3036317729949953
Global test accurancy: 0.09976566423236043
Global test_loss: 2.303669853210449
Global Precision: 0.0196944641697668
Global Recall: 0.09976566423236043
Global f1score: 0.031088996192530183
50
50
number of selected users 50
Global Trainning Accurancy: 0.10111646519415564
Global Trainning Loss: 2.3030344533920286
Global test accurancy: 0.09976566423236043
Global test_loss: 2.3031059980392454
Global Precision: 0.0196944641697668
Global Recall: 0.09976566423236043
Global f1score: 0.031088996192530183
50
50
number of selected users 50
Global Trainning Accurancy: 0.10610702038548804
Global Trainning Loss: 2.302463297843933
Global test accurancy: 0.10558161001187112
Global test_loss: 2.3025619792938232
Global Precision: 0.04394496462780924
Global Recall: 0.10558161001187112
Global f1score: 0.043819360382422574
50
50
number of selected users 50
Global Trainning Accurancy: 0.11642745604443297
Global Trainning Loss: 2.3018926048278807
Global test accurancy: 0.11229471526200636
Global test_loss: 2.302023410797119
Global Precision: 0.042239973406322254
Global Recall: 0.11229471526200636
Global f1score: 0.05359653233331422
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.3013076829910277
Global test accurancy: 0.10385790459408586
Global test_loss: 2.301471996307373
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.300731534957886
Global test accurancy: 0.10385790459408586
Global test_loss: 2.3009497833251955
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.3001342105865477
Global test accurancy: 0.10385790459408586
Global test_loss: 2.3004188346862793
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10896463978137869
Global Trainning Loss: 2.2995554065704344
Global test accurancy: 0.10855456453271321
Global test_loss: 2.2999228048324585
Global Precision: 0.06071338545965268
Global Recall: 0.10855456453271321
Global f1score: 0.044693740452879695
50
50
number of selected users 50
Global Trainning Accurancy: 0.12812567426524135
Global Trainning Loss: 2.299065070152283
Global test accurancy: 0.12909211630756054
Global test_loss: 2.2995197677612307
Global Precision: 0.055734219992628946
Global Recall: 0.12909211630756054
Global f1score: 0.0724390593533739
50
50
number of selected users 50
Global Trainning Accurancy: 0.11248069093137691
Global Trainning Loss: 2.2988475799560546
Global test accurancy: 0.11289049938128629
Global test_loss: 2.299388203620911
Global Precision: 0.05701985519094416
Global Recall: 0.11289049938128629
Global f1score: 0.044261097386880784
50
50
number of selected users 50
Global Trainning Accurancy: 0.11025140956303454
Global Trainning Loss: 2.2988225984573365
Global test accurancy: 0.1104427360643162
Global test_loss: 2.2994522762298586
Global Precision: 0.03267497364330619
Global Recall: 0.1104427360643162
Global f1score: 0.0383042675077101
50
50
number of selected users 50
Global Trainning Accurancy: 0.11016131947294446
Global Trainning Loss: 2.2985675144195556
Global test accurancy: 0.1106674551654398
Global test_loss: 2.299241724014282
Global Precision: 0.038751503602487226
Global Recall: 0.1106674551654398
Global f1score: 0.03874944089960176
50
50
number of selected users 50
Global Trainning Accurancy: 0.11053458647571414
Global Trainning Loss: 2.2978644609451293
Global test accurancy: 0.1110308803960377
Global test_loss: 2.2985356187820436
Global Precision: 0.04774562446133545
Global Recall: 0.1110308803960377
Global f1score: 0.03943926656571915
50
50
number of selected users 50
Global Trainning Accurancy: 0.1473659173032745
Global Trainning Loss: 2.2966998195648194
Global test accurancy: 0.1481373017143523
Global test_loss: 2.297296280860901
Global Precision: 0.07796472941575136
Global Recall: 0.1481373017143523
Global f1score: 0.07957730837644371
50
50
number of selected users 50
Global Trainning Accurancy: 0.14193779338387671
Global Trainning Loss: 2.2956577825546263
Global test accurancy: 0.14399251095098312
Global test_loss: 2.2961110830307008
Global Precision: 0.05993284915548062
Global Recall: 0.14399251095098312
Global f1score: 0.07593712895981714
50
50
number of selected users 50
Global Trainning Accurancy: 0.13298997347348043
Global Trainning Loss: 2.2949772596359255
Global test accurancy: 0.1360545927336744
Global test_loss: 2.295308346748352
Global Precision: 0.06286068304004315
Global Recall: 0.1360545927336744
Global f1score: 0.07023327174662296
50
50
number of selected users 50
Global Trainning Accurancy: 0.13135203386967198
Global Trainning Loss: 2.2943314695358277
Global test accurancy: 0.13470077249714715
Global test_loss: 2.2946022939682007
Global Precision: 0.06339947562464841
Global Recall: 0.13470077249714715
Global f1score: 0.06935531905537931
50
50
number of selected users 50
Global Trainning Accurancy: 0.1333658400666559
Global Trainning Loss: 2.2935651683807374
Global test accurancy: 0.13628768729365262
Global test_loss: 2.29381178855896
Global Precision: 0.06327592272194792
Global Recall: 0.13628768729365262
Global f1score: 0.07073571167649592
50
50
number of selected users 50
Global Trainning Accurancy: 0.13604960204594355
Global Trainning Loss: 2.292724151611328
Global test accurancy: 0.1384478662289373
Global test_loss: 2.292961530685425
Global Precision: 0.06307763868796687
Global Recall: 0.1384478662289373
Global f1score: 0.07280938126203437
50
50
number of selected users 50
Global Trainning Accurancy: 0.13754653581685253
Global Trainning Loss: 2.2918327951431277
Global test accurancy: 0.14012815633246717
Global test_loss: 2.2920621633529663
Global Precision: 0.06294768291738954
Global Recall: 0.14012815633246717
Global f1score: 0.07416281951910181
50
50
number of selected users 50
Global Trainning Accurancy: 0.13947255580286705
Global Trainning Loss: 2.290887532234192
Global test accurancy: 0.14145595455645482
Global test_loss: 2.2911066436767578
Global Precision: 0.07309028838854256
Global Recall: 0.14145595455645482
Global f1score: 0.07513661649644474
50
50
number of selected users 50
Global Trainning Accurancy: 0.14131035841195666
Global Trainning Loss: 2.2899018478393556
Global test accurancy: 0.14387489543621199
Global test_loss: 2.2901001930236817
Global Precision: 0.0729313815967293
Global Recall: 0.14387489543621199
Global f1score: 0.07644365535594955
50
50
number of selected users 50
Global Trainning Accurancy: 0.14335056238697225
Global Trainning Loss: 2.2888489627838133
Global test accurancy: 0.14624794374189834
Global test_loss: 2.2890323877334593
Global Precision: 0.08744624738737702
Global Recall: 0.14624794374189834
Global f1score: 0.0782067591773419
50
50
number of selected users 50
Global Trainning Accurancy: 0.14432673813331207
Global Trainning Loss: 2.2877593755722048
Global test accurancy: 0.14729065773267702
Global test_loss: 2.287925400733948
Global Precision: 0.0876287325767692
Global Recall: 0.14729065773267702
Global f1score: 0.0784932841181849
50
50
number of selected users 50
Global Trainning Accurancy: 0.14515693045962985
Global Trainning Loss: 2.2866322422027587
Global test accurancy: 0.14859231849004356
Global test_loss: 2.2867824697494505
Global Precision: 0.09909730156101242
Global Recall: 0.14859231849004356
Global f1score: 0.08041665659066471
50
50
number of selected users 50
Global Trainning Accurancy: 0.14632128042700795
Global Trainning Loss: 2.2854567480087282
Global test accurancy: 0.14798549922300833
Global test_loss: 2.2855909490585327
Global Precision: 0.09428396954086034
Global Recall: 0.14798549922300833
Global f1score: 0.08001467196756815
50
50
number of selected users 50
Global Trainning Accurancy: 0.14703604976793277
Global Trainning Loss: 2.2842366790771482
Global test accurancy: 0.14833418406212867
Global test_loss: 2.2843538427352907
Global Precision: 0.08906319812177889
Global Recall: 0.14833418406212867
Global f1score: 0.08083127873214642
50
50
number of selected users 50
Global Trainning Accurancy: 0.14777663665523516
Global Trainning Loss: 2.2829564094543455
Global test accurancy: 0.14880982955709368
Global test_loss: 2.2830550622940065
Global Precision: 0.08284175670152236
Global Recall: 0.14880982955709368
Global f1score: 0.08100352825084575
50
50
number of selected users 50
Global Trainning Accurancy: 0.14813166041081918
Global Trainning Loss: 2.2816165924072265
Global test accurancy: 0.1510235700996656
Global test_loss: 2.2816953372955324
Global Precision: 0.09191159521919122
Global Recall: 0.1510235700996656
Global f1score: 0.08333605367052409
50
50
number of selected users 50
Global Trainning Accurancy: 0.14934751784817168
Global Trainning Loss: 2.280218729972839
Global test accurancy: 0.15101090301479753
Global test_loss: 2.2802761793136597
Global Precision: 0.09317239721137008
Global Recall: 0.15101090301479753
Global f1score: 0.08403768117691349
50
50
number of selected users 50
Global Trainning Accurancy: 0.14934004845527077
Global Trainning Loss: 2.278770694732666
Global test accurancy: 0.151208949163173
Global test_loss: 2.278801317214966
Global Precision: 0.09223444780668426
Global Recall: 0.151208949163173
Global f1score: 0.08427399826648416
50
50
number of selected users 50
Global Trainning Accurancy: 0.1499664502534979
Global Trainning Loss: 2.2772589826583864
Global test accurancy: 0.15254482948403028
Global test_loss: 2.2772566747665404
Global Precision: 0.08889562386127915
Global Recall: 0.15254482948403028
Global f1score: 0.08627387927546605
50
50
number of selected users 50
Global Trainning Accurancy: 0.15162783007395134
Global Trainning Loss: 2.2756918144226073
Global test accurancy: 0.15347332726731627
Global test_loss: 2.275657000541687
Global Precision: 0.09142453904666385
Global Recall: 0.15347332726731627
Global f1score: 0.08807806941804346
50
50
number of selected users 50
Global Trainning Accurancy: 0.15275330027566628
Global Trainning Loss: 2.2740807151794433
Global test accurancy: 0.15486862890187206
Global test_loss: 2.27401141166687
Global Precision: 0.09452287278412227
Global Recall: 0.15486862890187206
Global f1score: 0.09057565708045731
50
50
number of selected users 50
Global Trainning Accurancy: 0.15378555684151815
Global Trainning Loss: 2.2724272060394286
Global test accurancy: 0.15669955607966699
Global test_loss: 2.2723206615448
Global Precision: 0.11245381335179694
Global Recall: 0.15669955607966699
Global f1score: 0.09478657990731969
50
50
number of selected users 50
Global Trainning Accurancy: 0.15606965717880347
Global Trainning Loss: 2.2707303380966186
Global test accurancy: 0.15901325085393467
Global test_loss: 2.2705884647369383
Global Precision: 0.11836220071764461
Global Recall: 0.15901325085393467
Global f1score: 0.09845586959394864
50
50
number of selected users 50
Global Trainning Accurancy: 0.1572177711620863
Global Trainning Loss: 2.269006552696228
Global test accurancy: 0.16044044139186664
Global test_loss: 2.2688260507583617
Global Precision: 0.140216205719386
Global Recall: 0.16044044139186664
Global f1score: 0.10104733497363887
50
50
number of selected users 50
Global Trainning Accurancy: 0.15939799235532984
Global Trainning Loss: 2.2672599935531617
Global test accurancy: 0.16195843187736397
Global test_loss: 2.2670357418060303
Global Precision: 0.14927292868344808
Global Recall: 0.16195843187736397
Global f1score: 0.10435024454690985
50
50
number of selected users 50
Global Trainning Accurancy: 0.16057998459026523
Global Trainning Loss: 2.2655090713500976
Global test accurancy: 0.1630744377143198
Global test_loss: 2.265236563682556
Global Precision: 0.15043432696967116
Global Recall: 0.1630744377143198
Global f1score: 0.10700192419480263
50
50
number of selected users 50
Global Trainning Accurancy: 0.1617430138081979
Global Trainning Loss: 2.2637495374679566
Global test accurancy: 0.16526818395913528
Global test_loss: 2.263430371284485
Global Precision: 0.1561060755127347
Global Recall: 0.16526818395913528
Global f1score: 0.11018509541201732
50
50
number of selected users 50
Global Trainning Accurancy: 0.16448859144029118
Global Trainning Loss: 2.262013874053955
Global test accurancy: 0.16639300152711547
Global test_loss: 2.2616457986831664
Global Precision: 0.16854537648752815
Global Recall: 0.16639300152711547
Global f1score: 0.11238064214597981
50
50
number of selected users 50
Global Trainning Accurancy: 0.16602511502319794
Global Trainning Loss: 2.2602861070632936
Global test accurancy: 0.16747871350818264
Global test_loss: 2.2598622846603393
Global Precision: 0.1857146030125542
Global Recall: 0.16747871350818264
Global f1score: 0.11598110097623789
50
50
number of selected users 50
Global Trainning Accurancy: 0.16757407687774245
Global Trainning Loss: 2.258591237068176
Global test accurancy: 0.1704653957607949
Global test_loss: 2.258106565475464
Global Precision: 0.2022350687280899
Global Recall: 0.1704653957607949
Global f1score: 0.12151110542155842
50
50
number of selected users 50
Global Trainning Accurancy: 0.169399122801689
Global Trainning Loss: 2.2569317293167113
Global test accurancy: 0.1692987280116524
Global test_loss: 2.2563823461532593
Global Precision: 0.1977539401322259
Global Recall: 0.1692987280116524
Global f1score: 0.12107455384616408
50
50
number of selected users 50
Global Trainning Accurancy: 0.17029688719360567
Global Trainning Loss: 2.255298080444336
Global test accurancy: 0.17075679055198423
Global test_loss: 2.2546787214279176
Global Precision: 0.19580830746942643
Global Recall: 0.17075679055198423
Global f1score: 0.12521742502691305
50
50
number of selected users 50
Global Trainning Accurancy: 0.17185613130855926
Global Trainning Loss: 2.253715443611145
Global test accurancy: 0.17161557120889245
Global test_loss: 2.2530292749404905
Global Precision: 0.19990620626161484
Global Recall: 0.17161557120889245
Global f1score: 0.1273569615464061
50
50
number of selected users 50
Global Trainning Accurancy: 0.17303915485955962
Global Trainning Loss: 2.2521801233291625
Global test accurancy: 0.17376749353722293
Global test_loss: 2.2514360094070436
Global Precision: 0.20798762551731445
Global Recall: 0.17376749353722293
Global f1score: 0.1308650075936678
50
50
number of selected users 50
Global Trainning Accurancy: 0.17439037716161876
Global Trainning Loss: 2.2506889295578003
Global test accurancy: 0.17507086360953053
Global test_loss: 2.2498834562301635
Global Precision: 0.21146221038689403
Global Recall: 0.17507086360953053
Global f1score: 0.13428418375542625
50
50
number of selected users 50
Global Trainning Accurancy: 0.17497635488556476
Global Trainning Loss: 2.2492299032211305
Global test accurancy: 0.17609649516278442
Global test_loss: 2.2483709621429444
Global Precision: 0.20846004275013147
Global Recall: 0.17609649516278442
Global f1score: 0.13610809697552423
50
50
number of selected users 50
Global Trainning Accurancy: 0.1767501615204636
Global Trainning Loss: 2.247809886932373
Global test accurancy: 0.1774595993462415
Global test_loss: 2.246899280548096
Global Precision: 0.21862028419521717
Global Recall: 0.1774595993462415
Global f1score: 0.1389486919616287
50
50
number of selected users 50
Global Trainning Accurancy: 0.17798301844750133
Global Trainning Loss: 2.246418957710266
Global test accurancy: 0.17936464296464466
Global test_loss: 2.2454612159729006
Global Precision: 0.22199203586923993
Global Recall: 0.17936464296464466
Global f1score: 0.14195290754374248
50
50
number of selected users 50
Global Trainning Accurancy: 0.17833554403213903
Global Trainning Loss: 2.2450634622573853
Global test accurancy: 0.18262178549627003
Global test_loss: 2.2440598249435424
Global Precision: 0.22724141627903197
Global Recall: 0.18262178549627003
Global f1score: 0.14723879067915657
50
50
number of selected users 50
Global Trainning Accurancy: 0.18109671122936735
Global Trainning Loss: 2.2437336158752443
Global test accurancy: 0.1834010671569339
Global test_loss: 2.24268159866333
Global Precision: 0.22418133664308826
Global Recall: 0.1834010671569339
Global f1score: 0.1486984575148682
50
50
number of selected users 50
Global Trainning Accurancy: 0.183479061186188
Global Trainning Loss: 2.242445344924927
Global test accurancy: 0.18430007030807985
Global test_loss: 2.2413493824005126
Global Precision: 0.22519759683742271
Global Recall: 0.18430007030807985
Global f1score: 0.15057534183775023
50
50
number of selected users 50
Global Trainning Accurancy: 0.18453717684542245
Global Trainning Loss: 2.2411562967300416
Global test accurancy: 0.18753730587453252
Global test_loss: 2.2400271987915037
Global Precision: 0.22931556123071306
Global Recall: 0.18753730587453252
Global f1score: 0.1548319465208188
50
50
number of selected users 50
Global Trainning Accurancy: 0.1853557997582668
Global Trainning Loss: 2.2399385738372803
Global test accurancy: 0.18971354157912643
Global test_loss: 2.2387605905532837
Global Precision: 0.23360353542014128
Global Recall: 0.18971354157912643
Global f1score: 0.1580973441992341
50
50
number of selected users 50
Global Trainning Accurancy: 0.18638163856668935
Global Trainning Loss: 2.238735680580139
Global test accurancy: 0.1898411176095386
Global test_loss: 2.237496008872986
Global Precision: 0.2288775289834962
Global Recall: 0.1898411176095386
Global f1score: 0.15856043779351833
50
50
number of selected users 50
Global Trainning Accurancy: 0.18654166760906968
Global Trainning Loss: 2.2375935697555542
Global test accurancy: 0.1904810219712012
Global test_loss: 2.236306223869324
Global Precision: 0.23003799439545877
Global Recall: 0.1904810219712012
Global f1score: 0.159554046444918
50
50
number of selected users 50
Global Trainning Accurancy: 0.18809433083408708
Global Trainning Loss: 2.2364282274246214
Global test accurancy: 0.19204212898998552
Global test_loss: 2.235094118118286
Global Precision: 0.23251834580586717
Global Recall: 0.19204212898998552
Global f1score: 0.16241836861441816
50
50
number of selected users 50
Global Trainning Accurancy: 0.18860239761512285
Global Trainning Loss: 2.235302186012268
Global test accurancy: 0.19342307973576062
Global test_loss: 2.2339222621917725
Global Precision: 0.2328441063395059
Global Recall: 0.19342307973576062
Global f1score: 0.1645893377652206
50
50
number of selected users 50
Global Trainning Accurancy: 0.18972075445820297
Global Trainning Loss: 2.2342314291000367
Global test accurancy: 0.19316906664011146
Global test_loss: 2.2328089618682863
Global Precision: 0.22984681581244665
Global Recall: 0.19316906664011146
Global f1score: 0.16500267798253748
50
50
number of selected users 50
Global Trainning Accurancy: 0.19086244274205486
Global Trainning Loss: 2.2331532621383667
Global test accurancy: 0.19487616934735658
Global test_loss: 2.2316771268844606
Global Precision: 0.23922355229248132
Global Recall: 0.19487616934735658
Global f1score: 0.16732394832976313
50
50
number of selected users 50
Global Trainning Accurancy: 0.1906961870346698
Global Trainning Loss: 2.232110233306885
Global test accurancy: 0.19790240681900265
Global test_loss: 2.230591311454773
Global Precision: 0.24310224340170572
Global Recall: 0.19790240681900265
Global f1score: 0.17064173353395656
50
50
number of selected users 50
Global Trainning Accurancy: 0.1914584044012923
Global Trainning Loss: 2.231093153953552
Global test accurancy: 0.1992213233423531
Global test_loss: 2.2295303249359133
Global Precision: 0.24495635790065368
Global Recall: 0.1992213233423531
Global f1score: 0.17243803846378816
50
50
number of selected users 50
Global Trainning Accurancy: 0.19263148956252496
Global Trainning Loss: 2.2300665378570557
Global test accurancy: 0.1992681574513274
Global test_loss: 2.228462610244751
Global Precision: 0.24970534827737997
Global Recall: 0.1992681574513274
Global f1score: 0.17336399843849898
50
50
number of selected users 50
Global Trainning Accurancy: 0.19247998852422024
Global Trainning Loss: 2.2290601205825804
Global test accurancy: 0.2005507059343609
Global test_loss: 2.2274189710617067
Global Precision: 0.2519668094487071
Global Recall: 0.2005507059343609
Global f1score: 0.17511659018790318
50
50
number of selected users 50
Global Trainning Accurancy: 0.1941343670410888
Global Trainning Loss: 2.2280802869796754
Global test accurancy: 0.20116574107396015
Global test_loss: 2.226389083862305
Global Precision: 0.2516133919952855
Global Recall: 0.20116574107396015
Global f1score: 0.17629130122142878
50
50
number of selected users 50
Global Trainning Accurancy: 0.19526206592653297
Global Trainning Loss: 2.2270725679397585
Global test accurancy: 0.20253416814343148
Global test_loss: 2.225341320037842
Global Precision: 0.26039568917446887
Global Recall: 0.20253416814343148
Global f1score: 0.17836987608147925
50
50
number of selected users 50
Global Trainning Accurancy: 0.195647154897749
Global Trainning Loss: 2.226104497909546
Global test accurancy: 0.20329331725580732
Global test_loss: 2.224362306594849
Global Precision: 0.27086440363170644
Global Recall: 0.20329331725580732
Global f1score: 0.18021953054030487
50
50
number of selected users 50
Global Trainning Accurancy: 0.19579760921381117
Global Trainning Loss: 2.2251357364654543
Global test accurancy: 0.20382102095517438
Global test_loss: 2.2233826160430907
Global Precision: 0.27565514876756736
Global Recall: 0.20382102095517438
Global f1score: 0.18119867308736104
50
50
number of selected users 50
Global Trainning Accurancy: 0.196890341424106
Global Trainning Loss: 2.2242069244384766
Global test accurancy: 0.20448670953225043
Global test_loss: 2.222459897994995
Global Precision: 0.27414471996589235
Global Recall: 0.20448670953225043
Global f1score: 0.1822513160867301
50
50
number of selected users 50
Global Trainning Accurancy: 0.1972134007441391
Global Trainning Loss: 2.223216800689697
Global test accurancy: 0.20632996113480864
Global test_loss: 2.221474123001099
Global Precision: 0.27335893460829563
Global Recall: 0.20632996113480864
Global f1score: 0.18454576657418628
50
50
number of selected users 50
Global Trainning Accurancy: 0.19945876925171258
Global Trainning Loss: 2.222213096618652
Global test accurancy: 0.2073799862020145
Global test_loss: 2.2204878997802733
Global Precision: 0.2798122481882531
Global Recall: 0.2073799862020145
Global f1score: 0.18575932965278805
50
50
number of selected users 50
Global Trainning Accurancy: 0.2000000930171851
Global Trainning Loss: 2.2212808513641358
Global test accurancy: 0.2069730208458644
Global test_loss: 2.219591484069824
Global Precision: 0.28006104945742416
Global Recall: 0.2069730208458644
Global f1score: 0.18639402299698427
50
50
number of selected users 50
Global Trainning Accurancy: 0.20073257294733143
Global Trainning Loss: 2.220315432548523
Global test accurancy: 0.20789992683348366
Global test_loss: 2.218656134605408
Global Precision: 0.27618702018515934
Global Recall: 0.20789992683348366
Global f1score: 0.18818376385055027
50
50
number of selected users 50
Global Trainning Accurancy: 0.20107614103952146
Global Trainning Loss: 2.2193268299102784
Global test accurancy: 0.2091056685885151
Global test_loss: 2.2177003526687624
Global Precision: 0.2857061934102626
Global Recall: 0.2091056685885151
Global f1score: 0.1903193960053204
50
50
number of selected users 50
Global Trainning Accurancy: 0.20203064258835396
Global Trainning Loss: 2.218358316421509
Global test accurancy: 0.2102307267216842
Global test_loss: 2.216774034500122
Global Precision: 0.2930645864204763
Global Recall: 0.2102307267216842
Global f1score: 0.19260768583446305
50
50
number of selected users 50
Global Trainning Accurancy: 0.20298823267826527
Global Trainning Loss: 2.2173934745788575
Global test accurancy: 0.20898631490881064
Global test_loss: 2.2158575582504274
Global Precision: 0.2920994626759227
Global Recall: 0.20898631490881064
Global f1score: 0.19182106578014227
50
50
number of selected users 50
Global Trainning Accurancy: 0.2029874905622316
Global Trainning Loss: 2.2164584302902224
Global test accurancy: 0.2085338924209752
Global test_loss: 2.2149764633178712
Global Precision: 0.2926889066690201
Global Recall: 0.2085338924209752
Global f1score: 0.19213082397266776
50
50
number of selected users 50
Global Trainning Accurancy: 0.20404310409263485
Global Trainning Loss: 2.215518345832825
Global test accurancy: 0.2086264244152778
Global test_loss: 2.2140907716751097
Global Precision: 0.2865432831156645
Global Recall: 0.2086264244152778
Global f1score: 0.19251411126850945
50
50
number of selected users 50
Global Trainning Accurancy: 0.20456730527147035
Global Trainning Loss: 2.2146021795272826
Global test accurancy: 0.20795246605852294
Global test_loss: 2.2132301998138426
Global Precision: 0.2898599936142866
Global Recall: 0.20795246605852294
Global f1score: 0.1918712656173771
50
50
number of selected users 50
Global Trainning Accurancy: 0.20475596894238046
Global Trainning Loss: 2.213644976615906
Global test accurancy: 0.20961373591164215
Global test_loss: 2.2123240756988527
Global Precision: 0.2917089824763725
Global Recall: 0.20961373591164215
Global f1score: 0.19447063023236547
50
50
number of selected users 50
Global Trainning Accurancy: 0.205157120965685
Global Trainning Loss: 2.2127448463439943
Global test accurancy: 0.20905463372955774
Global test_loss: 2.2114808559417725
Global Precision: 0.29057903514445943
Global Recall: 0.20905463372955774
Global f1score: 0.19524833468593053
50
50
number of selected users 50
Global Trainning Accurancy: 0.2060180511665677
Global Trainning Loss: 2.211851077079773
Global test accurancy: 0.20993452690925068
Global test_loss: 2.2106560850143433
Global Precision: 0.29395356496128006
Global Recall: 0.20993452690925068
Global f1score: 0.19686025471131827
50
50
number of selected users 50
Global Trainning Accurancy: 0.20698273664238864
Global Trainning Loss: 2.2109728336334227
Global test accurancy: 0.21186759450047496
Global test_loss: 2.2098582983016968
Global Precision: 0.3000129744416616
Global Recall: 0.21186759450047496
Global f1score: 0.20003022836935086
50
50
number of selected users 50
Global Trainning Accurancy: 0.20798885184807347
Global Trainning Loss: 2.2100905752182007
Global test accurancy: 0.21356374374259396
Global test_loss: 2.2090668439865113
Global Precision: 0.30304714914548403
Global Recall: 0.21356374374259396
Global f1score: 0.20225814163836076
50
50
number of selected users 50
Global Trainning Accurancy: 0.2076278205426606
Global Trainning Loss: 2.2092060899734496
Global test accurancy: 0.21554955918693583
Global test_loss: 2.208283863067627
Global Precision: 0.3069679297477528
Global Recall: 0.21554955918693583
Global f1score: 0.20507017443865064
50
50
number of selected users 50
Global Trainning Accurancy: 0.2086383778956035
Global Trainning Loss: 2.2083250141143798
Global test accurancy: 0.2174262876599733
Global test_loss: 2.20751266002655
Global Precision: 0.3151555851657944
Global Recall: 0.2174262876599733
Global f1score: 0.2082289684799018
50
50
number of selected users 50
Global Trainning Accurancy: 0.20952886378388466
Global Trainning Loss: 2.207401351928711
Global test accurancy: 0.21741899463733433
Global test_loss: 2.2067000102996825
Global Precision: 0.3148779715185505
Global Recall: 0.21741899463733433
Global f1score: 0.2093736257923386
50
50
number of selected users 50
Global Trainning Accurancy: 0.21061181213904465
Global Trainning Loss: 2.2065112686157224
Global test accurancy: 0.2170645523830824
Global test_loss: 2.2059227514266966
Global Precision: 0.31463016878024014
Global Recall: 0.2170645523830824
Global f1score: 0.2099681065161908
50
50
number of selected users 50
Global Trainning Accurancy: 0.21163360003786352
Global Trainning Loss: 2.205594220161438
Global test accurancy: 0.21785339043342514
Global test_loss: 2.20512834072113
Global Precision: 0.3148196279542139
Global Recall: 0.21785339043342514
Global f1score: 0.21259967013462933
50
50
number of selected users 50
Global Trainning Accurancy: 0.21305399284347867
Global Trainning Loss: 2.2046729564666747
Global test accurancy: 0.2183367631396002
Global test_loss: 2.204334788322449
Global Precision: 0.31727475848903786
Global Recall: 0.2183367631396002
Global f1score: 0.21369387282411884
50
50
number of selected users 50
Global Trainning Accurancy: 0.21494079237743802
Global Trainning Loss: 2.2037101650238036
Global test accurancy: 0.21837601425335382
Global test_loss: 2.203501510620117
Global Precision: 0.31819838010188767
Global Recall: 0.21837601425335382
Global f1score: 0.21446171586241783
50
50
number of selected users 50
Global Trainning Accurancy: 0.21530496432941432
Global Trainning Loss: 2.2028016948699953
Global test accurancy: 0.21807782494608732
Global test_loss: 2.2027288007736208
Global Precision: 0.313165853282574
Global Recall: 0.21807782494608732
Global f1score: 0.2144263511455896
50
50
number of selected users 50
Global Trainning Accurancy: 0.2158397872723652
Global Trainning Loss: 2.2018790149688723
Global test accurancy: 0.21781277124889006
Global test_loss: 2.2019481801986696
Global Precision: 0.3153593318707024
Global Recall: 0.21781277124889006
Global f1score: 0.21467255264536106
50
50
number of selected users 50
Global Trainning Accurancy: 0.21673626364327783
Global Trainning Loss: 2.2009531974792482
Global test accurancy: 0.2178060662071529
Global test_loss: 2.2011925172805786
Global Precision: 0.3146944910297896
Global Recall: 0.2178060662071529
Global f1score: 0.21539274398664865
50
50
number of selected users 50
Global Trainning Accurancy: 0.21783738791487275
Global Trainning Loss: 2.20002628326416
Global test accurancy: 0.2193997295960058
Global test_loss: 2.200432291030884
Global Precision: 0.3179483488362286
Global Recall: 0.2193997295960058
Global f1score: 0.21798049195384622
50
50
number of selected users 50
Global Trainning Accurancy: 0.21896596960519382
Global Trainning Loss: 2.1990370035171507
Global test accurancy: 0.22085641337653947
Global test_loss: 2.199604845046997
Global Precision: 0.3190544110873527
Global Recall: 0.22085641337653947
Global f1score: 0.22093818215909722
50
50
number of selected users 50
Global Trainning Accurancy: 0.22011422337237455
Global Trainning Loss: 2.1980803871154784
Global test accurancy: 0.22005576291508017
Global test_loss: 2.1988270902633666
Global Precision: 0.3184727803650854
Global Recall: 0.22005576291508017
Global f1score: 0.22028563056929684
50
50
number of selected users 50
Global Trainning Accurancy: 0.22135260827305925
Global Trainning Loss: 2.197128186225891
Global test accurancy: 0.22039029792142323
Global test_loss: 2.1980486249923707
Global Precision: 0.31569448822716395
Global Recall: 0.22039029792142323
Global f1score: 0.22120933904155102
50
50
number of selected users 50
Global Trainning Accurancy: 0.22187767992608004
Global Trainning Loss: 2.196233105659485
Global test accurancy: 0.22239105415522606
Global test_loss: 2.197347583770752
Global Precision: 0.3172953197175021
Global Recall: 0.22239105415522606
Global f1score: 0.2242669326298219
50
50
number of selected users 50
Global Trainning Accurancy: 0.22267410445236832
Global Trainning Loss: 2.1952983856201174
Global test accurancy: 0.2237321206461116
Global test_loss: 2.1966322565078737
Global Precision: 0.31822280960136695
Global Recall: 0.2237321206461116
Global f1score: 0.2263362553996382
50
50
number of selected users 50
Global Trainning Accurancy: 0.22280447679501364
Global Trainning Loss: 2.1943842649459837
Global test accurancy: 0.22468433662085427
Global test_loss: 2.195924425125122
Global Precision: 0.32337329574300766
Global Recall: 0.22468433662085427
Global f1score: 0.2290696461524945
50
50
number of selected users 50
Global Trainning Accurancy: 0.22507293944093373
Global Trainning Loss: 2.1934826469421385
Global test accurancy: 0.22456091867609013
Global test_loss: 2.195230956077576
Global Precision: 0.32379603537086854
Global Recall: 0.22456091867609013
Global f1score: 0.22937181098931014
50
50
number of selected users 50
Global Trainning Accurancy: 0.22592251717432899
Global Trainning Loss: 2.1925997877120973
Global test accurancy: 0.22521719178361224
Global test_loss: 2.1945609045028687
Global Precision: 0.3257359532121984
Global Recall: 0.22521719178361224
Global f1score: 0.23094990064393583
50
50
number of selected users 50
Global Trainning Accurancy: 0.22716178664393932
Global Trainning Loss: 2.1917305040359496
Global test accurancy: 0.2259269383790344
Global test_loss: 2.193928074836731
Global Precision: 0.32616382213997497
Global Recall: 0.2259269383790344
Global f1score: 0.23337574680662287
50
50
number of selected users 50
Global Trainning Accurancy: 0.22848203645359064
Global Trainning Loss: 2.1908593559265137
Global test accurancy: 0.22572326231988143
Global test_loss: 2.193313674926758
Global Precision: 0.3247258698321323
Global Recall: 0.22572326231988143
Global f1score: 0.23399046870414564
50
50
number of selected users 50
Global Trainning Accurancy: 0.2299509035368157
Global Trainning Loss: 2.1899292516708373
Global test accurancy: 0.22769735018601373
Global test_loss: 2.192639856338501
Global Precision: 0.32792555920091204
Global Recall: 0.22769735018601373
Global f1score: 0.23682318099552926
50
50
number of selected users 50
Global Trainning Accurancy: 0.22996311704804193
Global Trainning Loss: 2.1890517902374267
Global test accurancy: 0.22880532968506104
Global test_loss: 2.1920340347290037
Global Precision: 0.32882496400352595
Global Recall: 0.22880532968506104
Global f1score: 0.23813151347604203
50
50
number of selected users 50
Global Trainning Accurancy: 0.23124862717714992
Global Trainning Loss: 2.1881259441375733
Global test accurancy: 0.2295855379277709
Global test_loss: 2.1913882064819337
Global Precision: 0.32866969657215334
Global Recall: 0.2295855379277709
Global f1score: 0.23943873554524184
50
50
number of selected users 50
Global Trainning Accurancy: 0.23282749403058625
Global Trainning Loss: 2.1872736549377443
Global test accurancy: 0.22985898703717922
Global test_loss: 2.19084979057312
Global Precision: 0.32844165479168974
Global Recall: 0.22985898703717922
Global f1score: 0.23963980628360554
50
50
number of selected users 50
Global Trainning Accurancy: 0.23348198029914957
Global Trainning Loss: 2.186378674507141
Global test accurancy: 0.2305599077574697
Global test_loss: 2.1902361488342286
Global Precision: 0.3284826964234178
Global Recall: 0.2305599077574697
Global f1score: 0.24023348889467955
50
50
number of selected users 50
Global Trainning Accurancy: 0.2334492775669017
Global Trainning Loss: 2.185502986907959
Global test accurancy: 0.23223423361985981
Global test_loss: 2.189652681350708
Global Precision: 0.33128776786296776
Global Recall: 0.23223423361985981
Global f1score: 0.24184546023481007
50
50
number of selected users 50
Global Trainning Accurancy: 0.23379200209877568
Global Trainning Loss: 2.1847099351882933
Global test accurancy: 0.23281895940248357
Global test_loss: 2.189157819747925
Global Precision: 0.3315657299549077
Global Recall: 0.23281895940248357
Global f1score: 0.24293464114274382
50
50
number of selected users 50
Global Trainning Accurancy: 0.23487085398523155
Global Trainning Loss: 2.1839436435699464
Global test accurancy: 0.233391479211061
Global test_loss: 2.1887031984329224
Global Precision: 0.3354770041533014
Global Recall: 0.233391479211061
Global f1score: 0.24394667902695716
50
50
number of selected users 50
Global Trainning Accurancy: 0.23590433404062613
Global Trainning Loss: 2.183093934059143
Global test accurancy: 0.2350878535472398
Global test_loss: 2.188172307014465
Global Precision: 0.34311961404681995
Global Recall: 0.2350878535472398
Global f1score: 0.24638419252353264
50
50
number of selected users 50
Global Trainning Accurancy: 0.23534740747004637
Global Trainning Loss: 2.1823280477523803
Global test accurancy: 0.23491070935924643
Global test_loss: 2.1877269840240476
Global Precision: 0.3435417346859945
Global Recall: 0.23491070935924643
Global f1score: 0.24727755973768617
50
50
number of selected users 50
Global Trainning Accurancy: 0.2355836795579249
Global Trainning Loss: 2.1815751218795776
Global test accurancy: 0.2345782860233781
Global test_loss: 2.187296471595764
Global Precision: 0.34126985027778844
Global Recall: 0.2345782860233781
Global f1score: 0.2472651345135867
50
50
number of selected users 50
Global Trainning Accurancy: 0.23580581456345645
Global Trainning Loss: 2.1807887172698974
Global test accurancy: 0.23414414978587547
Global test_loss: 2.1868367862701414
Global Precision: 0.34006749934598646
Global Recall: 0.23414414978587547
Global f1score: 0.24754937316707307
50
50
number of selected users 50
Global Trainning Accurancy: 0.23633955897204592
Global Trainning Loss: 2.1800011157989503
Global test accurancy: 0.2366082641602318
Global test_loss: 2.1864012384414675
Global Precision: 0.34044682200212617
Global Recall: 0.2366082641602318
Global f1score: 0.2498326792058918
50
50
number of selected users 50
Global Trainning Accurancy: 0.2377705250280438
Global Trainning Loss: 2.179217987060547
Global test accurancy: 0.23737375918153458
Global test_loss: 2.1859883308410644
Global Precision: 0.3424504893632664
Global Recall: 0.23737375918153458
Global f1score: 0.25130215664370553
50
50
number of selected users 50
Global Trainning Accurancy: 0.23866182001882152
Global Trainning Loss: 2.1783922147750854
Global test accurancy: 0.2385383584040788
Global test_loss: 2.1855244493484496
Global Precision: 0.3388620616855083
Global Recall: 0.2385383584040788
Global f1score: 0.25245612507248394
50
50
number of selected users 50
Global Trainning Accurancy: 0.23874031002807508
Global Trainning Loss: 2.1775831604003906
Global test accurancy: 0.2389326128377728
Global test_loss: 2.185110902786255
Global Precision: 0.34061435996583883
Global Recall: 0.2389326128377728
Global f1score: 0.2538315187300734
50
50
number of selected users 50
Global Trainning Accurancy: 0.23939686576715222
Global Trainning Loss: 2.176747751235962
Global test accurancy: 0.23984406449673826
Global test_loss: 2.1846416425704955
Global Precision: 0.3396812592148603
Global Recall: 0.23984406449673826
Global f1score: 0.2549930876201518
50
50
number of selected users 50
Global Trainning Accurancy: 0.2405403984089804
Global Trainning Loss: 2.17600510597229
Global test accurancy: 0.2398109892207633
Global test_loss: 2.184286551475525
Global Precision: 0.3397775562085303
Global Recall: 0.2398109892207633
Global f1score: 0.25549706527876165
50
50
number of selected users 50
Global Trainning Accurancy: 0.24150600380200124
Global Trainning Loss: 2.175243787765503
Global test accurancy: 0.23957035185004522
Global test_loss: 2.183898825645447
Global Precision: 0.33779555386281357
Global Recall: 0.23957035185004522
Global f1score: 0.255035828481679
50
50
number of selected users 50
Global Trainning Accurancy: 0.24198019773778326
Global Trainning Loss: 2.1744373512268065
Global test accurancy: 0.23896272996578155
Global test_loss: 2.183452582359314
Global Precision: 0.3376551710568614
Global Recall: 0.23896272996578155
Global f1score: 0.25487932131518726
50
50
number of selected users 50
Global Trainning Accurancy: 0.24263714541253528
Global Trainning Loss: 2.1736831665039062
Global test accurancy: 0.23853613663358672
Global test_loss: 2.183109383583069
Global Precision: 0.3387299378902525
Global Recall: 0.23853613663358672
Global f1score: 0.25448475564472167
50
50
number of selected users 50
Global Trainning Accurancy: 0.24216628430725964
Global Trainning Loss: 2.172905158996582
Global test accurancy: 0.23853388686183163
Global test_loss: 2.1827248668670656
Global Precision: 0.3381122908890866
Global Recall: 0.23853388686183163
Global f1score: 0.2542058490921244
50
50
number of selected users 50
Global Trainning Accurancy: 0.2429703510670699
Global Trainning Loss: 2.172125301361084
Global test accurancy: 0.23839225393397973
Global test_loss: 2.1823748111724854
Global Precision: 0.3404630126188223
Global Recall: 0.23839225393397973
Global f1score: 0.25388238017686593
50
50
number of selected users 50
Global Trainning Accurancy: 0.2437940937229194
Global Trainning Loss: 2.17146710395813
Global test accurancy: 0.2394723392923028
Global test_loss: 2.182134065628052
Global Precision: 0.34117871554696505
Global Recall: 0.2394723392923028
Global f1score: 0.25480579594326463
50
50
number of selected users 50
Global Trainning Accurancy: 0.24450108975806584
Global Trainning Loss: 2.1706592416763306
Global test accurancy: 0.23609610326182037
Global test_loss: 2.181739692687988
Global Precision: 0.33272061911765977
Global Recall: 0.23609610326182037
Global f1score: 0.25149556075070006
50
50
number of selected users 50
Global Trainning Accurancy: 0.24519883165871867
Global Trainning Loss: 2.1699606704711916
Global test accurancy: 0.23563234645362904
Global test_loss: 2.1814470911026
Global Precision: 0.33294034041785125
Global Recall: 0.23563234645362904
Global f1score: 0.2512611786797755
50
50
number of selected users 50
Global Trainning Accurancy: 0.2456284951326779
Global Trainning Loss: 2.1692778968811037
Global test accurancy: 0.23755684299010382
Global test_loss: 2.181184620857239
Global Precision: 0.3381634405763182
Global Recall: 0.23755684299010382
Global f1score: 0.25414731095246407
50
50
number of selected users 50
Global Trainning Accurancy: 0.2452810008224074
Global Trainning Loss: 2.168574457168579
Global test accurancy: 0.23755680842822463
Global test_loss: 2.1809032917022706
Global Precision: 0.3378767010979956
Global Recall: 0.23755680842822463
Global f1score: 0.25431655128489106
50
50
number of selected users 50
Global Trainning Accurancy: 0.24645017070426448
Global Trainning Loss: 2.167793560028076
Global test accurancy: 0.23784813387974998
Global test_loss: 2.180530242919922
Global Precision: 0.3379067252132757
Global Recall: 0.23784813387974998
Global f1score: 0.25499174210371295
50
50
number of selected users 50
Global Trainning Accurancy: 0.24663537456559262
Global Trainning Loss: 2.1670754528045655
Global test accurancy: 0.23869005853242622
Global test_loss: 2.180224027633667
Global Precision: 0.339695735821203
Global Recall: 0.23869005853242622
Global f1score: 0.2558996177124292
50
50
number of selected users 50
Global Trainning Accurancy: 0.24670978149218423
Global Trainning Loss: 2.166396822929382
Global test accurancy: 0.23858631560797916
Global test_loss: 2.179963049888611
Global Precision: 0.3378240050418995
Global Recall: 0.23858631560797916
Global f1score: 0.2562575894977623
50
50
number of selected users 50
Global Trainning Accurancy: 0.24735158553544562
Global Trainning Loss: 2.1656972312927247
Global test accurancy: 0.23862523997587937
Global test_loss: 2.1796999168395996
Global Precision: 0.3380151652360838
Global Recall: 0.23862523997587937
Global f1score: 0.25658562985871836
50
50
number of selected users 50
Global Trainning Accurancy: 0.2482218388090896
Global Trainning Loss: 2.165027828216553
Global test accurancy: 0.23870132149686948
Global test_loss: 2.179484543800354
Global Precision: 0.338090002702665
Global Recall: 0.23870132149686948
Global f1score: 0.2566547228859138
50
50
number of selected users 50
Global Trainning Accurancy: 0.2483065552240154
Global Trainning Loss: 2.1642630434036256
Global test accurancy: 0.23738163255966976
Global test_loss: 2.1791674518585205
Global Precision: 0.3362169536912233
Global Recall: 0.23738163255966976
Global f1score: 0.25540834324062717
50
50
number of selected users 50
Global Trainning Accurancy: 0.2483343581481436
Global Trainning Loss: 2.1635190200805665
Global test accurancy: 0.2377436309598819
Global test_loss: 2.178852300643921
Global Precision: 0.33609370450955994
Global Recall: 0.2377436309598819
Global f1score: 0.2560153899736866
50
50
number of selected users 50
Global Trainning Accurancy: 0.24834334360010907
Global Trainning Loss: 2.162814106941223
Global test accurancy: 0.23814542229866545
Global test_loss: 2.1785573673248293
Global Precision: 0.3369533742712084
Global Recall: 0.23814542229866545
Global f1score: 0.2568341772747278
50
50
number of selected users 50
Global Trainning Accurancy: 0.24917672696181328
Global Trainning Loss: 2.1621872711181642
Global test accurancy: 0.2407008721752582
Global test_loss: 2.178316044807434
Global Precision: 0.3392566603649106
Global Recall: 0.2407008721752582
Global f1score: 0.25996314149230726
50
50
number of selected users 50
Global Trainning Accurancy: 0.2493974311067826
Global Trainning Loss: 2.1615245389938353
Global test accurancy: 0.24085503236864897
Global test_loss: 2.1780543756484985
Global Precision: 0.339843782892388
Global Recall: 0.24085503236864897
Global f1score: 0.2601037528900726
50
50
number of selected users 50
Global Trainning Accurancy: 0.24890060262540661
Global Trainning Loss: 2.1608399391174316
Global test accurancy: 0.2393510879937115
Global test_loss: 2.1777838325500487
Global Precision: 0.33789260146594774
Global Recall: 0.2393510879937115
Global f1score: 0.25870962816445614
50
50
number of selected users 50
Global Trainning Accurancy: 0.2496619791346919
Global Trainning Loss: 2.1602674245834352
Global test accurancy: 0.23955651333045222
Global test_loss: 2.1776379919052125
Global Precision: 0.3376242056646771
Global Recall: 0.23955651333045222
Global f1score: 0.2594658134379765
50
50
number of selected users 50
Global Trainning Accurancy: 0.2495622145198448
Global Trainning Loss: 2.1596408700942993
Global test accurancy: 0.241268791494507
Global test_loss: 2.177449278831482
Global Precision: 0.338481501709835
Global Recall: 0.241268791494507
Global f1score: 0.2610961798257725
50
50
number of selected users 50
Global Trainning Accurancy: 0.25048822557644634
Global Trainning Loss: 2.1589200592041013
Global test accurancy: 0.24161344893165632
Global test_loss: 2.1771746015548707
Global Precision: 0.33888523143217447
Global Recall: 0.24161344893165632
Global f1score: 0.261745874355911
50
50
number of selected users 50
Global Trainning Accurancy: 0.25038337986861975
Global Trainning Loss: 2.15820698261261
Global test accurancy: 0.2409340508154244
Global test_loss: 2.176895089149475
Global Precision: 0.3377841201310988
Global Recall: 0.2409340508154244
Global f1score: 0.2609868488670913
50
50
number of selected users 50
Global Trainning Accurancy: 0.2513607742170141
Global Trainning Loss: 2.1575694417953493
Global test accurancy: 0.24153387561207737
Global test_loss: 2.1767434787750246
Global Precision: 0.3402838298734282
Global Recall: 0.24153387561207737
Global f1score: 0.26203757862653526
50
50
number of selected users 50
Global Trainning Accurancy: 0.2514352147036299
Global Trainning Loss: 2.156641182899475
Global test accurancy: 0.24215360270824884
Global test_loss: 2.1762835025787353
Global Precision: 0.3392737217015155
Global Recall: 0.24215360270824884
Global f1score: 0.26238896910151294
50
50
number of selected users 50
Global Trainning Accurancy: 0.2518290266785687
Global Trainning Loss: 2.1560830163955687
Global test accurancy: 0.2412233974544323
Global test_loss: 2.176196460723877
Global Precision: 0.33878999175404345
Global Recall: 0.2412233974544323
Global f1score: 0.26249750468029825
50
50
number of selected users 50
Global Trainning Accurancy: 0.2521310875645255
Global Trainning Loss: 2.1554261350631716
Global test accurancy: 0.24042260440627006
Global test_loss: 2.176012010574341
Global Precision: 0.3378764406457034
Global Recall: 0.24042260440627006
Global f1score: 0.2615472618860095
50
50
number of selected users 50
Global Trainning Accurancy: 0.2522728432903408
Global Trainning Loss: 2.154716215133667
Global test accurancy: 0.2419868328242226
Global test_loss: 2.1758251953125
Global Precision: 0.3399679652305434
Global Recall: 0.2419868328242226
Global f1score: 0.2634842218503367
50
50
number of selected users 50
Global Trainning Accurancy: 0.2535269818323598
Global Trainning Loss: 2.15387188911438
Global test accurancy: 0.24196841846520364
Global test_loss: 2.1754914045333864
Global Precision: 0.3386191443016227
Global Recall: 0.24196841846520364
Global f1score: 0.26309002042289265
50
50
number of selected users 50
Global Trainning Accurancy: 0.25377570113807246
Global Trainning Loss: 2.1533003854751587
Global test accurancy: 0.24111560470436993
Global test_loss: 2.1754831981658938
Global Precision: 0.336302706108992
Global Recall: 0.24111560470436993
Global f1score: 0.26230776832073005
50
50
number of selected users 50
Global Trainning Accurancy: 0.25425397159818225
Global Trainning Loss: 2.1524729490280152
Global test accurancy: 0.24124968114262868
Global test_loss: 2.17519926071167
Global Precision: 0.3352427891203867
Global Recall: 0.24124968114262868
Global f1score: 0.26219132981729026
50
50
number of selected users 50
Global Trainning Accurancy: 0.25454696985101155
Global Trainning Loss: 2.1517802333831786
Global test accurancy: 0.24189828342620787
Global test_loss: 2.1750415658950804
Global Precision: 0.33590321954959995
Global Recall: 0.24189828342620787
Global f1score: 0.26240828514236986
50
50
number of selected users 50
Global Trainning Accurancy: 0.2566076539767044
Global Trainning Loss: 2.151255879402161
Global test accurancy: 0.24002072626135326
Global test_loss: 2.1750195360183717
Global Precision: 0.33514634449711866
Global Recall: 0.24002072626135326
Global f1score: 0.2615860736627658
50
50
number of selected users 50
Global Trainning Accurancy: 0.2563493382868774
Global Trainning Loss: 2.1505447101593016
Global test accurancy: 0.24092684848057214
Global test_loss: 2.1748048448562622
Global Precision: 0.3359860317827762
Global Recall: 0.24092684848057214
Global f1score: 0.2626590094064003
50
50
number of selected users 50
Global Trainning Accurancy: 0.25491551849923744
Global Trainning Loss: 2.149664058685303
Global test accurancy: 0.241237603424182
Global test_loss: 2.174424591064453
Global Precision: 0.33812788618946876
Global Recall: 0.241237603424182
Global f1score: 0.26310199775324816
50
50
number of selected users 50
Global Trainning Accurancy: 0.2557681803670013
Global Trainning Loss: 2.1492189741134644
Global test accurancy: 0.24133377017515556
Global test_loss: 2.1745463609695435
Global Precision: 0.33907024972398087
Global Recall: 0.24133377017515556
Global f1score: 0.2634796392148127
50
50
number of selected users 50
Global Trainning Accurancy: 0.25630576207424566
Global Trainning Loss: 2.148442029953003
Global test accurancy: 0.24035995615334496
Global test_loss: 2.1743164014816285
Global Precision: 0.33801929065321923
Global Recall: 0.24035995615334496
Global f1score: 0.2626444909427955
50
50
number of selected users 50
Global Trainning Accurancy: 0.2575902819982091
Global Trainning Loss: 2.147858762741089
Global test accurancy: 0.24032136087652456
Global test_loss: 2.174302306175232
Global Precision: 0.33778609069048837
Global Recall: 0.24032136087652456
Global f1score: 0.2624613157468584
50
50
number of selected users 50
Global Trainning Accurancy: 0.25809625263666264
Global Trainning Loss: 2.1470554542541502
Global test accurancy: 0.2415827337555446
Global test_loss: 2.1740813636779786
Global Precision: 0.3379628310168495
Global Recall: 0.2415827337555446
Global f1score: 0.2637574679851764
50
50
number of selected users 50
Global Trainning Accurancy: 0.25766352079223126
Global Trainning Loss: 2.1462807130813597
Global test accurancy: 0.24161736985073032
Global test_loss: 2.17393506526947
Global Precision: 0.337978155732442
Global Recall: 0.24161736985073032
Global f1score: 0.26410215566404555
50
50
number of selected users 50
Global Trainning Accurancy: 0.2572886955138307
Global Trainning Loss: 2.1457674503326416
Global test accurancy: 0.24148726081795024
Global test_loss: 2.1740917778015136
Global Precision: 0.33696000061170556
Global Recall: 0.24148726081795024
Global f1score: 0.263889673071709
50
50
number of selected users 50
Global Trainning Accurancy: 0.25801155668864295
Global Trainning Loss: 2.1452934455871584
Global test accurancy: 0.24211105869940797
Global test_loss: 2.17425407409668
Global Precision: 0.33603477277044563
Global Recall: 0.24211105869940797
Global f1score: 0.2644008328878496
50
50
number of selected users 50
Global Trainning Accurancy: 0.2573997912814275
Global Trainning Loss: 2.144580059051514
Global test accurancy: 0.24109424730560924
Global test_loss: 2.1741878175735474
Global Precision: 0.3337227028130992
Global Recall: 0.24109424730560924
Global f1score: 0.26299906171737125
50
50
number of selected users 50
Global Trainning Accurancy: 0.25754184440581696
Global Trainning Loss: 2.1438557386398314
Global test accurancy: 0.24088290781957947
Global test_loss: 2.174122748374939
Global Precision: 0.33470137887449336
Global Recall: 0.24088290781957947
Global f1score: 0.2632652717794095
50
50
number of selected users 50
Global Trainning Accurancy: 0.2577726801651841
Global Trainning Loss: 2.1432155084609987
Global test accurancy: 0.24008827749852424
Global test_loss: 2.174132752418518
Global Precision: 0.3348070896662754
Global Recall: 0.24008827749852424
Global f1score: 0.26271322017014964
50
50
number of selected users 50
Global Trainning Accurancy: 0.2576892408483796
Global Trainning Loss: 2.1424999809265137
Global test accurancy: 0.24008491584509867
Global test_loss: 2.1740313148498536
Global Precision: 0.3350639295277833
Global Recall: 0.24008491584509867
Global f1score: 0.26287435871153275
50
50
number of selected users 50
Global Trainning Accurancy: 0.25811101713926254
Global Trainning Loss: 2.142112693786621
Global test accurancy: 0.2397972851422798
Global test_loss: 2.174266700744629
Global Precision: 0.33547086583343544
Global Recall: 0.2397972851422798
Global f1score: 0.26242712295145043
50
50
number of selected users 50
Global Trainning Accurancy: 0.2589047602472356
Global Trainning Loss: 2.1414606523513795
Global test accurancy: 0.23835355436340683
Global test_loss: 2.174217200279236
Global Precision: 0.3349139400157354
Global Recall: 0.23835355436340683
Global f1score: 0.26074755445239134
50
50
number of selected users 50
Global Trainning Accurancy: 0.2582546194143624
Global Trainning Loss: 2.1406983089447023
Global test accurancy: 0.23858125300153818
Global test_loss: 2.1740669870376585
Global Precision: 0.3356183309414756
Global Recall: 0.23858125300153818
Global f1score: 0.2610541958496247
50
50
number of selected users 50
Global Trainning Accurancy: 0.25882507450317616
Global Trainning Loss: 2.140121440887451
Global test accurancy: 0.2370461231556607
Global test_loss: 2.174145917892456
Global Precision: 0.33554318389854654
Global Recall: 0.2370461231556607
Global f1score: 0.26028743657197095
50
50
number of selected users 50
Global Trainning Accurancy: 0.2591322294694297
Global Trainning Loss: 2.1396135807037355
Global test accurancy: 0.237630760749968
Global test_loss: 2.1742705345153808
Global Precision: 0.3354469389952107
Global Recall: 0.237630760749968
Global f1score: 0.26082186413177444
50
50
number of selected users 50
Global Trainning Accurancy: 0.2584167901290186
Global Trainning Loss: 2.1391957998275757
Global test accurancy: 0.23710692264224953
Global test_loss: 2.1744733905792235
Global Precision: 0.33646734394523603
Global Recall: 0.23710692264224953
Global f1score: 0.2604837876447825
50
50
number of selected users 50
Global Trainning Accurancy: 0.2586502821099459
Global Trainning Loss: 2.1384944248199464
Global test accurancy: 0.23870763360853117
Global test_loss: 2.1743800926208494
Global Precision: 0.3385485585204206
Global Recall: 0.23870763360853117
Global f1score: 0.26213494461834597
50
50
number of selected users 50
Global Trainning Accurancy: 0.2587727906346204
Global Trainning Loss: 2.1377836561203
Global test accurancy: 0.23883626437815392
Global test_loss: 2.1743111658096312
Global Precision: 0.33915205944244775
Global Recall: 0.23883626437815392
Global f1score: 0.2628203179201958
50
50
number of selected users 50
Global Trainning Accurancy: 0.25929471698194656
Global Trainning Loss: 2.1371757078170774
Global test accurancy: 0.2366250097111494
Global test_loss: 2.174451079368591
Global Precision: 0.337694408353288
Global Recall: 0.2366250097111494
Global f1score: 0.2604066146257813
50
50
number of selected users 50
Global Trainning Accurancy: 0.25853432310656116
Global Trainning Loss: 2.136559386253357
Global test accurancy: 0.23824591701123907
Global test_loss: 2.1746285152435303
Global Precision: 0.3399004240095655
Global Recall: 0.23824591701123907
Global f1score: 0.26260675861388033
50
50
number of selected users 50
Global Trainning Accurancy: 0.2592189464674675
Global Trainning Loss: 2.1359056711196898
Global test accurancy: 0.23722162842965142
Global test_loss: 2.1746886205673217
Global Precision: 0.33825118728580084
Global Recall: 0.23722162842965142
Global f1score: 0.26131934473865953
50
50
number of selected users 50
Global Trainning Accurancy: 0.25950507127900563
Global Trainning Loss: 2.1354793071746827
Global test accurancy: 0.23692051966498298
Global test_loss: 2.175050401687622
Global Precision: 0.33619912993971696
Global Recall: 0.23692051966498298
Global f1score: 0.2612311227148107
50
50
number of selected users 50
Global Trainning Accurancy: 0.2596593532620591
Global Trainning Loss: 2.1344395780563357
Global test accurancy: 0.23670679275696038
Global test_loss: 2.174783315658569
Global Precision: 0.33564182263249
Global Recall: 0.23670679275696038
Global f1score: 0.26055881776190837
50
50
number of selected users 50
Global Trainning Accurancy: 0.259605240031173
Global Trainning Loss: 2.1337196731567385
Global test accurancy: 0.23743804616282
Global test_loss: 2.174882941246033
Global Precision: 0.33527952071148004
Global Recall: 0.23743804616282
Global f1score: 0.26076815263736086
50
50
number of selected users 50
Global Trainning Accurancy: 0.25946089102875597
Global Trainning Loss: 2.1330532169342042
Global test accurancy: 0.23648017162077167
Global test_loss: 2.1750350046157836
Global Precision: 0.3342044476965983
Global Recall: 0.23648017162077167
Global f1score: 0.2598088851747533
50
50
number of selected users 50
Global Trainning Accurancy: 0.26005941287110645
Global Trainning Loss: 2.1327344226837157
Global test accurancy: 0.23840883759560746
Global test_loss: 2.175535078048706
Global Precision: 0.3351867659998081
Global Recall: 0.23840883759560746
Global f1score: 0.26184961445397537
50
50
number of selected users 50
Global Trainning Accurancy: 0.2599505131804097
Global Trainning Loss: 2.1320559787750244
Global test accurancy: 0.23749594786818187
Global test_loss: 2.175751280784607
Global Precision: 0.3356031454729039
Global Recall: 0.23749594786818187
Global f1score: 0.26084129994419664
50
50
number of selected users 50
Global Trainning Accurancy: 0.26000442022829534
Global Trainning Loss: 2.13126033782959
Global test accurancy: 0.238131928851776
Global test_loss: 2.175845594406128
Global Precision: 0.3367509078311992
Global Recall: 0.238131928851776
Global f1score: 0.26146428045669196
50
50
number of selected users 50
Global Trainning Accurancy: 0.26229187340931764
Global Trainning Loss: 2.130477876663208
Global test accurancy: 0.2381288149063435
Global test_loss: 2.17604238986969
Global Precision: 0.33703127853983095
Global Recall: 0.2381288149063435
Global f1score: 0.26140798497111095
50
50
number of selected users 50
Global Trainning Accurancy: 0.26226354804567986
Global Trainning Loss: 2.129911346435547
Global test accurancy: 0.2364742965530424
Global test_loss: 2.1764512968063356
Global Precision: 0.33395063649248935
Global Recall: 0.2364742965530424
Global f1score: 0.2597390749424207
50
50
number of selected users 50
Global Trainning Accurancy: 0.26327818126700475
Global Trainning Loss: 2.129217491149902
Global test accurancy: 0.23785344351807675
Global test_loss: 2.1768581295013427
Global Precision: 0.33560572041129505
Global Recall: 0.23785344351807675
Global f1score: 0.2609591806991662
50
50
number of selected users 50
Global Trainning Accurancy: 0.2645249337816527
Global Trainning Loss: 2.1282944965362547
Global test accurancy: 0.2360325045709072
Global test_loss: 2.1769528198242187
Global Precision: 0.33326146875881457
Global Recall: 0.2360325045709072
Global f1score: 0.2593699258454584
50
50
number of selected users 50
Global Trainning Accurancy: 0.2646223866101144
Global Trainning Loss: 2.127661581039429
Global test accurancy: 0.23615528324147336
Global test_loss: 2.1773828411102296
Global Precision: 0.33328994580449917
Global Recall: 0.23615528324147336
Global f1score: 0.25907190727919904
50
50
number of selected users 50
Global Trainning Accurancy: 0.26445268330360966
Global Trainning Loss: 2.126946792602539
Global test accurancy: 0.23585365242028117
Global test_loss: 2.1778345584869383
Global Precision: 0.3367425287444676
Global Recall: 0.23585365242028117
Global f1score: 0.25926915245639254
50
50
number of selected users 50
Global Trainning Accurancy: 0.26486887396910147
Global Trainning Loss: 2.1260051679611207
Global test accurancy: 0.23627346947283065
Global test_loss: 2.1778781318664553
Global Precision: 0.33912551603197244
Global Recall: 0.23627346947283065
Global f1score: 0.2602042602964031
50
50
number of selected users 50
Global Trainning Accurancy: 0.26603190873624033
Global Trainning Loss: 2.1251967573165893
Global test accurancy: 0.23548031333210076
Global test_loss: 2.178129777908325
Global Precision: 0.3376085601912398
Global Recall: 0.23548031333210076
Global f1score: 0.25915394288551374
50
50
number of selected users 50
Global Trainning Accurancy: 0.26616275396625966
Global Trainning Loss: 2.1243431472778322
Global test accurancy: 0.23507594895133074
Global test_loss: 2.1782012033462523
Global Precision: 0.33693114406969893
Global Recall: 0.23507594895133074
Global f1score: 0.25831381815831783
exp_no  0
0_dataset_CIFAR10_algorithm_MOON_KL_model_CNN_3_50_0.4_31_07_2024
100%|██████████| 200/200 [1:07:36<00:00, 19.23s/it]100%|██████████| 200/200 [1:07:36<00:00, 20.28s/it]
50
50
number of selected users 50
Global Trainning Accurancy: 0.10111646519415564
Global Trainning Loss: 2.3036317825317383
Global test accurancy: 0.09976566423236043
Global test_loss: 2.3036698722839355
Global Precision: 0.0196944641697668
Global Recall: 0.09976566423236043
Global f1score: 0.031088996192530183
50
50
number of selected users 50
Global Trainning Accurancy: 0.10111646519415564
Global Trainning Loss: 2.3030344963073732
Global test accurancy: 0.09976566423236043
Global test_loss: 2.303105993270874
Global Precision: 0.0196944641697668
Global Recall: 0.09976566423236043
Global f1score: 0.031088996192530183
50
50
number of selected users 50
Global Trainning Accurancy: 0.10610702038548804
Global Trainning Loss: 2.3024632835388186
Global test accurancy: 0.10558161001187112
Global test_loss: 2.3025619173049927
Global Precision: 0.04394496462780924
Global Recall: 0.10558161001187112
Global f1score: 0.043819360382422574
50
50
number of selected users 50
Global Trainning Accurancy: 0.11642745604443297
Global Trainning Loss: 2.3018925952911378
Global test accurancy: 0.11229471526200636
Global test_loss: 2.3020234060287477
Global Precision: 0.042239973406322254
Global Recall: 0.11229471526200636
Global f1score: 0.05359653233331422
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.3013076829910277
Global test accurancy: 0.10385790459408586
Global test_loss: 2.30147198677063
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.300731506347656
Global test accurancy: 0.10385790459408586
Global test_loss: 2.3009498357772826
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.300134162902832
Global test accurancy: 0.10385790459408586
Global test_loss: 2.3004188346862793
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10896463978137869
Global Trainning Loss: 2.2995554494857786
Global test accurancy: 0.10869540960313576
Global test_loss: 2.2999228048324585
Global Precision: 0.061347668029072384
Global Recall: 0.10869540960313576
Global f1score: 0.04472307367782851
50
50
number of selected users 50
Global Trainning Accurancy: 0.12812567426524135
Global Trainning Loss: 2.2990649557113647
Global test accurancy: 0.12909211630756054
Global test_loss: 2.2995197057723997
Global Precision: 0.055734219992628946
Global Recall: 0.12909211630756054
Global f1score: 0.0724390593533739
50
50
number of selected users 50
Global Trainning Accurancy: 0.11248069093137691
Global Trainning Loss: 2.2988475227355956
Global test accurancy: 0.11289049938128629
Global test_loss: 2.299388117790222
Global Precision: 0.05701985519094416
Global Recall: 0.11289049938128629
Global f1score: 0.044261097386880784
50
50
number of selected users 50
Global Trainning Accurancy: 0.11025140956303454
Global Trainning Loss: 2.2988217163085936
Global test accurancy: 0.1104427360643162
Global test_loss: 2.2994516611099245
Global Precision: 0.03267497364330619
Global Recall: 0.1104427360643162
Global f1score: 0.0383042675077101
50
50
number of selected users 50
Global Trainning Accurancy: 0.11016131947294446
Global Trainning Loss: 2.298567042350769
Global test accurancy: 0.1106674551654398
Global test_loss: 2.299241347312927
Global Precision: 0.038751503602487226
Global Recall: 0.1106674551654398
Global f1score: 0.03874944089960176
50
50
number of selected users 50
Global Trainning Accurancy: 0.11053458647571414
Global Trainning Loss: 2.2978637647628783
Global test accurancy: 0.1110308803960377
Global test_loss: 2.298534893989563
Global Precision: 0.04774562446133545
Global Recall: 0.1110308803960377
Global f1score: 0.03943926656571915
50
50
number of selected users 50
Global Trainning Accurancy: 0.14740052404484755
Global Trainning Loss: 2.296701040267944
Global test accurancy: 0.14847628476519975
Global test_loss: 2.297297730445862
Global Precision: 0.07801716604525678
Global Recall: 0.14847628476519975
Global f1score: 0.07966872232525099
50
50
number of selected users 50
Global Trainning Accurancy: 0.14183049359649189
Global Trainning Loss: 2.295657444000244
Global test accurancy: 0.1439201945668023
Global test_loss: 2.2961108016967775
Global Precision: 0.06001812656251273
Global Recall: 0.1439201945668023
Global f1score: 0.07593339245276287
50
50
number of selected users 50
Global Trainning Accurancy: 0.13310722223272842
Global Trainning Loss: 2.294973568916321
Global test accurancy: 0.1360545927336744
Global test_loss: 2.29530478477478
Global Precision: 0.06267859953840933
Global Recall: 0.1360545927336744
Global f1score: 0.07017120572692646
50
50
number of selected users 50
Global Trainning Accurancy: 0.13135203386967198
Global Trainning Loss: 2.2943341636657717
Global test accurancy: 0.13470077249714715
Global test_loss: 2.2946048736572267
Global Precision: 0.06334261943735744
Global Recall: 0.13470077249714715
Global f1score: 0.06932508978240567
50
50
number of selected users 50
Global Trainning Accurancy: 0.13344520514602098
Global Trainning Loss: 2.2935613059997557
Global test accurancy: 0.13628768729365262
Global test_loss: 2.2938089036941527
Global Precision: 0.06327592272194792
Global Recall: 0.13628768729365262
Global f1score: 0.07073571167649592
50
50
number of selected users 50
Global Trainning Accurancy: 0.13604960204594355
Global Trainning Loss: 2.292726225852966
Global test accurancy: 0.1384478662289373
Global test_loss: 2.2929634380340578
Global Precision: 0.06307669266429357
Global Recall: 0.1384478662289373
Global f1score: 0.0728077116122601
50
50
number of selected users 50
Global Trainning Accurancy: 0.13754653581685253
Global Trainning Loss: 2.2918313694000245
Global test accurancy: 0.14012815633246717
Global test_loss: 2.2920610809326174
Global Precision: 0.06286257169315704
Global Recall: 0.14012815633246717
Global f1score: 0.0741158302638651
50
50
number of selected users 50
Global Trainning Accurancy: 0.13947255580286705
Global Trainning Loss: 2.2908914566040037
Global test accurancy: 0.14145595455645482
Global test_loss: 2.2911107540130615
Global Precision: 0.07309120666218812
Global Recall: 0.14145595455645482
Global f1score: 0.07513826638920171
50
50
number of selected users 50
Global Trainning Accurancy: 0.1413786177976222
Global Trainning Loss: 2.2899010229110717
Global test accurancy: 0.14387489543621199
Global test_loss: 2.2900998258590697
Global Precision: 0.07287613317946676
Global Recall: 0.14387489543621199
Global f1score: 0.07641557438535827
50
50
number of selected users 50
Global Trainning Accurancy: 0.14333183579521194
Global Trainning Loss: 2.288851761817932
Global test accurancy: 0.14624794374189834
Global test_loss: 2.2890352535247804
Global Precision: 0.08751156341276223
Global Recall: 0.14624794374189834
Global f1score: 0.07824186061293412
50
50
number of selected users 50
Global Trainning Accurancy: 0.1444634938665332
Global Trainning Loss: 2.287757225036621
Global test accurancy: 0.14708072183524112
Global test_loss: 2.287923717498779
Global Precision: 0.08756015033480796
Global Recall: 0.14708072183524112
Global f1score: 0.07848127094257894
50
50
number of selected users 50
Global Trainning Accurancy: 0.14517565705139016
Global Trainning Loss: 2.2866321706771853
Global test accurancy: 0.14859231849004356
Global test_loss: 2.286783146858215
Global Precision: 0.09909730156101242
Global Recall: 0.14859231849004356
Global f1score: 0.08041665659066471
50
50
number of selected users 50
Global Trainning Accurancy: 0.14620561535283144
Global Trainning Loss: 2.2854604482650758
Global test accurancy: 0.1481468681183926
Global test_loss: 2.2855948543548585
Global Precision: 0.09437270490207243
Global Recall: 0.1481468681183926
Global f1score: 0.07998562232834498
50
50
number of selected users 50
Global Trainning Accurancy: 0.1471040769788171
Global Trainning Loss: 2.2842379713058474
Global test accurancy: 0.14833418406212867
Global test_loss: 2.2843553495407103
Global Precision: 0.08906356446959476
Global Recall: 0.14833418406212867
Global f1score: 0.080831737472987
50
50
number of selected users 50
Global Trainning Accurancy: 0.14786592236952087
Global Trainning Loss: 2.2829558849334717
Global test accurancy: 0.14880982955709368
Global test_loss: 2.283054885864258
Global Precision: 0.08153043034105173
Global Recall: 0.14880982955709368
Global f1score: 0.08100392103301793
50
50
number of selected users 50
Global Trainning Accurancy: 0.1481503870025795
Global Trainning Loss: 2.281616544723511
Global test accurancy: 0.151066361409738
Global test_loss: 2.2816963481903074
Global Precision: 0.09193579146965491
Global Recall: 0.151066361409738
Global f1score: 0.08337245869243687
50
50
number of selected users 50
Global Trainning Accurancy: 0.14932294782360167
Global Trainning Loss: 2.280222520828247
Global test accurancy: 0.15125480545382192
Global test_loss: 2.280280976295471
Global Precision: 0.09319005530650307
Global Recall: 0.15125480545382192
Global f1score: 0.08407062384400249
50
50
number of selected users 50
Global Trainning Accurancy: 0.14934004845527077
Global Trainning Loss: 2.278770089149475
Global test accurancy: 0.151208949163173
Global test_loss: 2.278801155090332
Global Precision: 0.09223444780668426
Global Recall: 0.151208949163173
Global f1score: 0.08427399826648416
50
50
number of selected users 50
Global Trainning Accurancy: 0.15015248093021674
Global Trainning Loss: 2.2772564935684203
Global test accurancy: 0.15276954858515387
Global test_loss: 2.2772544479370116
Global Precision: 0.08895324978875888
Global Recall: 0.15276954858515387
Global f1score: 0.08642403353313316
50
50
number of selected users 50
Global Trainning Accurancy: 0.1518021098171966
Global Trainning Loss: 2.275688772201538
Global test accurancy: 0.1535758913698804
Global test_loss: 2.2756548261642457
Global Precision: 0.09146423498191984
Global Recall: 0.1535758913698804
Global f1score: 0.088143530564098
50
50
number of selected users 50
Global Trainning Accurancy: 0.15277202686742658
Global Trainning Loss: 2.274078059196472
Global test accurancy: 0.15515720945037054
Global test_loss: 2.274009461402893
Global Precision: 0.09709333569468451
Global Recall: 0.15515720945037054
Global f1score: 0.09108894047269152
50
50
number of selected users 50
Global Trainning Accurancy: 0.15346945692087183
Global Trainning Loss: 2.2724258852005006
Global test accurancy: 0.15669955607966699
Global test_loss: 2.272320237159729
Global Precision: 0.11190651710164115
Global Recall: 0.15669955607966699
Global f1score: 0.09476835850747145
50
50
number of selected users 50
Global Trainning Accurancy: 0.15588203596284933
Global Trainning Loss: 2.2707302570343018
Global test accurancy: 0.15901325085393467
Global test_loss: 2.270589466094971
Global Precision: 0.118305936420001
Global Recall: 0.15901325085393467
Global f1score: 0.09841198244661323
50
50
number of selected users 50
Global Trainning Accurancy: 0.15711640756874093
Global Trainning Loss: 2.2690001726150513
Global test accurancy: 0.16043522614821523
Global test_loss: 2.268820037841797
Global Precision: 0.13829571068881946
Global Recall: 0.16043522614821523
Global f1score: 0.10109345811943686
50
50
number of selected users 50
Global Trainning Accurancy: 0.15923614351461923
Global Trainning Loss: 2.2672561693191526
Global test accurancy: 0.16195843187736397
Global test_loss: 2.267031922340393
Global Precision: 0.14927292868344808
Global Recall: 0.16195843187736397
Global f1score: 0.10435024454690985
50
50
number of selected users 50
Global Trainning Accurancy: 0.16051822673194424
Global Trainning Loss: 2.26550678730011
Global test accurancy: 0.16338693771431978
Global test_loss: 2.2652349042892457
Global Precision: 0.15071841787876206
Global Recall: 0.16338693771431978
Global f1score: 0.10733794570017897
50
50
number of selected users 50
Global Trainning Accurancy: 0.1617115834868027
Global Trainning Loss: 2.2637569665908814
Global test accurancy: 0.16526818395913528
Global test_loss: 2.263437395095825
Global Precision: 0.15607648765419949
Global Recall: 0.16526818395913528
Global f1score: 0.11015665685602682
50
50
number of selected users 50
Global Trainning Accurancy: 0.16424048437616945
Global Trainning Loss: 2.26201352596283
Global test accurancy: 0.16644615767329488
Global test_loss: 2.261643896102905
Global Precision: 0.16448797720028238
Global Recall: 0.16644615767329488
Global f1score: 0.11211241121221809
50
50
number of selected users 50
Global Trainning Accurancy: 0.16584659179204314
Global Trainning Loss: 2.26029287815094
Global test accurancy: 0.1677871579698022
Global test_loss: 2.259866304397583
Global Precision: 0.18276129977098216
Global Recall: 0.1677871579698022
Global f1score: 0.11607856985838097
50
50
number of selected users 50
Global Trainning Accurancy: 0.16753971261657405
Global Trainning Loss: 2.258587431907654
Global test accurancy: 0.1706827870651427
Global test_loss: 2.2580989599227905
Global Precision: 0.202406185646224
Global Recall: 0.1706827870651427
Global f1score: 0.12171746639123925
50
50
number of selected users 50
Global Trainning Accurancy: 0.169399122801689
Global Trainning Loss: 2.256928577423096
Global test accurancy: 0.16903206134498575
Global test_loss: 2.2563710498809812
Global Precision: 0.19768867862637166
Global Recall: 0.16903206134498575
Global f1score: 0.12096871264040338
50
50
number of selected users 50
Global Trainning Accurancy: 0.17023718241281696
Global Trainning Loss: 2.255296845436096
Global test accurancy: 0.17105090819904303
Global test_loss: 2.2546698331832884
Global Precision: 0.19667923544000318
Global Recall: 0.17105090819904303
Global f1score: 0.1255626980797486
50
50
number of selected users 50
Global Trainning Accurancy: 0.17173947117196134
Global Trainning Loss: 2.2537218713760376
Global test accurancy: 0.17138301306935758
Global test_loss: 2.2530329275131225
Global Precision: 0.19766345027847104
Global Recall: 0.17138301306935758
Global f1score: 0.1269503146643871
50
50
number of selected users 50
Global Trainning Accurancy: 0.17301458483498958
Global Trainning Loss: 2.2521830892562864
Global test accurancy: 0.17402729745879156
Global test_loss: 2.2514370489120483
Global Precision: 0.20639460912563873
Global Recall: 0.17402729745879156
Global f1score: 0.1312566533291494
50
50
number of selected users 50
Global Trainning Accurancy: 0.17433364637318066
Global Trainning Loss: 2.250684313774109
Global test accurancy: 0.17507086360953053
Global test_loss: 2.2498791456222533
Global Precision: 0.21154242682360136
Global Recall: 0.17507086360953053
Global f1score: 0.1342898083456879
50
50
number of selected users 50
Global Trainning Accurancy: 0.174857735869863
Global Trainning Loss: 2.2492317485809328
Global test accurancy: 0.1762834110506349
Global test_loss: 2.2483719110488893
Global Precision: 0.20862095294923752
Global Recall: 0.1762834110506349
Global f1score: 0.13635827799012676
50
50
number of selected users 50
Global Trainning Accurancy: 0.17677473154503362
Global Trainning Loss: 2.247801651954651
Global test accurancy: 0.17766368097889457
Global test_loss: 2.2468913984298706
Global Precision: 0.21878155528625254
Global Recall: 0.17766368097889457
Global f1score: 0.13917226667408827
50
50
number of selected users 50
Global Trainning Accurancy: 0.17778761927194
Global Trainning Loss: 2.2464094161987305
Global test accurancy: 0.1793086205556811
Global test_loss: 2.2454525518417356
Global Precision: 0.2220764230342066
Global Recall: 0.1793086205556811
Global f1score: 0.14190030415640767
50
50
number of selected users 50
Global Trainning Accurancy: 0.1785133920581419
Global Trainning Loss: 2.2450617599487304
Global test accurancy: 0.18262178549627003
Global test_loss: 2.2440589809417726
Global Precision: 0.22723715733733205
Global Recall: 0.18262178549627003
Global f1score: 0.14723413924332565
50
50
number of selected users 50
Global Trainning Accurancy: 0.1808972410733401
Global Trainning Loss: 2.2437447261810304
Global test accurancy: 0.18316577303928683
Global test_loss: 2.242692379951477
Global Precision: 0.22415675401934357
Global Recall: 0.18316577303928683
Global f1score: 0.14861887619381003
50
50
number of selected users 50
Global Trainning Accurancy: 0.18351043539867026
Global Trainning Loss: 2.242447028160095
Global test accurancy: 0.18409063907796067
Global test_loss: 2.24134840965271
Global Precision: 0.2225863219179029
Global Recall: 0.18409063907796067
Global f1score: 0.1505693922951034
50
50
number of selected users 50
Global Trainning Accurancy: 0.18444364749220704
Global Trainning Loss: 2.2411602926254273
Global test accurancy: 0.18753730587453252
Global test_loss: 2.2400287532806398
Global Precision: 0.22759099358311016
Global Recall: 0.18753730587453252
Global f1score: 0.15488033125772402
50
50
number of selected users 50
Global Trainning Accurancy: 0.185556904125907
Global Trainning Loss: 2.2399267578125
Global test accurancy: 0.18976956398809003
Global test_loss: 2.2387469959259034
Global Precision: 0.23368816035473153
Global Recall: 0.18976956398809003
Global f1score: 0.1581873430934621
50
50
number of selected users 50
Global Trainning Accurancy: 0.18649527493032572
Global Trainning Loss: 2.2387416219711302
Global test accurancy: 0.1898411176095386
Global test_loss: 2.2375007581710817
Global Precision: 0.22889617863235825
Global Recall: 0.1898411176095386
Global f1score: 0.15857966942955962
50
50
number of selected users 50
Global Trainning Accurancy: 0.18668498988926258
Global Trainning Loss: 2.2375918006896973
Global test accurancy: 0.19034338756259905
Global test_loss: 2.2363011550903322
Global Precision: 0.22551752750953466
Global Recall: 0.19034338756259905
Global f1score: 0.1595633201230215
50
50
number of selected users 50
Global Trainning Accurancy: 0.187740485135577
Global Trainning Loss: 2.2364377975463867
Global test accurancy: 0.191901283919563
Global test_loss: 2.2351004219055177
Global Precision: 0.23309482279548835
Global Recall: 0.191901283919563
Global f1score: 0.162282702995838
50
50
number of selected users 50
Global Trainning Accurancy: 0.1884926177321471
Global Trainning Loss: 2.2352996587753298
Global test accurancy: 0.19342307973576062
Global test_loss: 2.2339204359054565
Global Precision: 0.23260902171336434
Global Recall: 0.19342307973576062
Global f1score: 0.16449589141412765
50
50
number of selected users 50
Global Trainning Accurancy: 0.18978600221602224
Global Trainning Loss: 2.234245705604553
Global test accurancy: 0.1930226491946286
Global test_loss: 2.2328195714950563
Global Precision: 0.22880127879570797
Global Recall: 0.1930226491946286
Global f1score: 0.16453743182393046
50
50
number of selected users 50
Global Trainning Accurancy: 0.19100277916388006
Global Trainning Loss: 2.233158688545227
Global test accurancy: 0.19503616934735657
Global test_loss: 2.2316831159591675
Global Precision: 0.23997734017605854
Global Recall: 0.19503616934735657
Global f1score: 0.16758310863470555
50
50
number of selected users 50
Global Trainning Accurancy: 0.1906757607110036
Global Trainning Loss: 2.2321222734451296
Global test accurancy: 0.19776156174858012
Global test_loss: 2.2306006860733034
Global Precision: 0.24276629524049015
Global Recall: 0.19776156174858012
Global f1score: 0.17064366351047508
50
50
number of selected users 50
Global Trainning Accurancy: 0.19148289037871816
Global Trainning Loss: 2.231079363822937
Global test accurancy: 0.19883531391864884
Global test_loss: 2.2295102167129515
Global Precision: 0.2447611688002678
Global Recall: 0.19883531391864884
Global f1score: 0.1722576788101573
50
50
number of selected users 50
Global Trainning Accurancy: 0.19254439855521333
Global Trainning Loss: 2.2300517702102662
Global test accurancy: 0.19946781650708365
Global test_loss: 2.2284443044662474
Global Precision: 0.25007406118293285
Global Recall: 0.19946781650708365
Global f1score: 0.173496329060249
50
50
number of selected users 50
Global Trainning Accurancy: 0.1930020595638164
Global Trainning Loss: 2.2290707206726075
Global test accurancy: 0.2000239352260776
Global test_loss: 2.2274291563034057
Global Precision: 0.2489606961668398
Global Recall: 0.2000239352260776
Global f1score: 0.17437410685613622
50
50
number of selected users 50
Global Trainning Accurancy: 0.19411938408811466
Global Trainning Loss: 2.228095154762268
Global test accurancy: 0.20083888563324814
Global test_loss: 2.2264084577560426
Global Precision: 0.25701758739774677
Global Recall: 0.20083888563324814
Global f1score: 0.17623838591113983
50
50
number of selected users 50
Global Trainning Accurancy: 0.19511440352908244
Global Trainning Loss: 2.2270989418029785
Global test accurancy: 0.202034752255581
Global test_loss: 2.225372862815857
Global Precision: 0.25980431282598837
Global Recall: 0.202034752255581
Global f1score: 0.177853207185968
50
50
number of selected users 50
Global Trainning Accurancy: 0.1956738655783274
Global Trainning Loss: 2.226113305091858
Global test accurancy: 0.20329331725580732
Global test_loss: 2.2243744325637818
Global Precision: 0.27149162495859563
Global Recall: 0.20329331725580732
Global f1score: 0.18017033235695254
50
50
number of selected users 50
Global Trainning Accurancy: 0.19563301415451592
Global Trainning Loss: 2.225137677192688
Global test accurancy: 0.20365973063259374
Global test_loss: 2.223390817642212
Global Precision: 0.2749122275546866
Global Recall: 0.20365973063259374
Global f1score: 0.18093321469465737
50
50
number of selected users 50
Global Trainning Accurancy: 0.19682078118041743
Global Trainning Loss: 2.224195079803467
Global test accurancy: 0.20447944304895488
Global test_loss: 2.222475600242615
Global Precision: 0.2746844672486307
Global Recall: 0.20447944304895488
Global f1score: 0.18225854641596814
50
50
number of selected users 50
Global Trainning Accurancy: 0.19768691145177875
Global Trainning Loss: 2.223210349082947
Global test accurancy: 0.2059826915096773
Global test_loss: 2.2215092039108275
Global Precision: 0.2769734509239966
Global Recall: 0.2059826915096773
Global f1score: 0.18434667205008137
50
50
number of selected users 50
Global Trainning Accurancy: 0.1998575888773763
Global Trainning Loss: 2.222206506729126
Global test accurancy: 0.20648237049881055
Global test_loss: 2.2205126667022705
Global Precision: 0.2746821224043832
Global Recall: 0.20648237049881055
Global f1score: 0.1849088956503646
50
50
number of selected users 50
Global Trainning Accurancy: 0.19999436328078823
Global Trainning Loss: 2.221247224807739
Global test accurancy: 0.2073423136581651
Global test_loss: 2.2195637893676756
Global Precision: 0.28330624106780516
Global Recall: 0.2073423136581651
Global f1score: 0.18657055948522255
50
50
number of selected users 50
Global Trainning Accurancy: 0.20068998832248228
Global Trainning Loss: 2.2203055477142333
Global test accurancy: 0.20737468712193965
Global test_loss: 2.2186435747146604
Global Precision: 0.28292164663894925
Global Recall: 0.20737468712193965
Global f1score: 0.18796699291394345
50
50
number of selected users 50
Global Trainning Accurancy: 0.20044935545631373
Global Trainning Loss: 2.219295344352722
Global test accurancy: 0.2088531384793447
Global test_loss: 2.217679147720337
Global Precision: 0.28617450707206277
Global Recall: 0.2088531384793447
Global f1score: 0.1902276251889243
50
50
number of selected users 50
Global Trainning Accurancy: 0.20157818089974877
Global Trainning Loss: 2.2183237361907957
Global test accurancy: 0.20881040149799632
Global test_loss: 2.2167403984069822
Global Precision: 0.29186905495856147
Global Recall: 0.20881040149799632
Global f1score: 0.19090480180739558
50
50
number of selected users 50
Global Trainning Accurancy: 0.2025697661962858
Global Trainning Loss: 2.217375650405884
Global test accurancy: 0.20817642380091847
Global test_loss: 2.215829601287842
Global Precision: 0.2885048942738122
Global Recall: 0.20817642380091847
Global f1score: 0.19060491534220333
50
50
number of selected users 50
Global Trainning Accurancy: 0.20334617446376996
Global Trainning Loss: 2.2164239406585695
Global test accurancy: 0.2080296197985457
Global test_loss: 2.2149225425720216
Global Precision: 0.2909551973058709
Global Recall: 0.2080296197985457
Global f1score: 0.19132530153040558
50
50
number of selected users 50
Global Trainning Accurancy: 0.20426554587280435
Global Trainning Loss: 2.215528688430786
Global test accurancy: 0.2088097399123286
Global test_loss: 2.214083724021912
Global Precision: 0.2872549849204989
Global Recall: 0.2088097399123286
Global f1score: 0.19248768213202067
50
50
number of selected users 50
Global Trainning Accurancy: 0.20480072967925234
Global Trainning Loss: 2.2146006870269774
Global test accurancy: 0.208706715596462
Global test_loss: 2.213207278251648
Global Precision: 0.288742635969493
Global Recall: 0.208706715596462
Global f1score: 0.19302403066533358
50
50
number of selected users 50
Global Trainning Accurancy: 0.20451986456634583
Global Trainning Loss: 2.213653645515442
Global test accurancy: 0.20941487527515096
Global test_loss: 2.2123103952407837
Global Precision: 0.29099945318573817
Global Recall: 0.20941487527515096
Global f1score: 0.1944719364832157
50
50
number of selected users 50
Global Trainning Accurancy: 0.20530822276017738
Global Trainning Loss: 2.212736086845398
Global test accurancy: 0.20946644017056384
Global test_loss: 2.21146324634552
Global Precision: 0.29044837478272645
Global Recall: 0.20946644017056384
Global f1score: 0.19534075689372474
50
50
number of selected users 50
Global Trainning Accurancy: 0.20542821654045232
Global Trainning Loss: 2.2118441390991213
Global test accurancy: 0.21133662197663242
Global test_loss: 2.210641236305237
Global Precision: 0.30036504023443394
Global Recall: 0.21133662197663242
Global f1score: 0.19842095449852346
50
50
number of selected users 50
Global Trainning Accurancy: 0.20712414147551725
Global Trainning Loss: 2.2109575176239016
Global test accurancy: 0.21274341500711944
Global test_loss: 2.2098442459106447
Global Precision: 0.30218953063182463
Global Recall: 0.21274341500711944
Global f1score: 0.20096089750432256
50
50
number of selected users 50
Global Trainning Accurancy: 0.20771702531299274
Global Trainning Loss: 2.2100995683670046
Global test accurancy: 0.21362663682435495
Global test_loss: 2.209079174995422
Global Precision: 0.302155146055309
Global Recall: 0.21362663682435495
Global f1score: 0.20230110931833542
50
50
number of selected users 50
Global Trainning Accurancy: 0.20742955960240828
Global Trainning Loss: 2.209216065406799
Global test accurancy: 0.2163799397913062
Global test_loss: 2.2082958602905274
Global Precision: 0.3118001552126205
Global Recall: 0.2163799397913062
Global f1score: 0.20638740214468443
50
50
number of selected users 50
Global Trainning Accurancy: 0.2086621123582939
Global Trainning Loss: 2.20833348274231
Global test accurancy: 0.2167670209287067
Global test_loss: 2.2075204420089722
Global Precision: 0.3088572982856674
Global Recall: 0.2167670209287067
Global f1score: 0.20732203267409688
50
50
number of selected users 50
Global Trainning Accurancy: 0.20947536134119873
Global Trainning Loss: 2.2074756717681883
Global test accurancy: 0.21807965900564574
Global test_loss: 2.206774659156799
Global Precision: 0.31441193013136426
Global Recall: 0.21807965900564574
Global f1score: 0.2094128923537605
50
50
number of selected users 50
Global Trainning Accurancy: 0.21033492350204966
Global Trainning Loss: 2.2065562534332277
Global test accurancy: 0.2179230724170288
Global test_loss: 2.2059704303741454
Global Precision: 0.31310393860225294
Global Recall: 0.2179230724170288
Global f1score: 0.2107689288153863
50
50
number of selected users 50
Global Trainning Accurancy: 0.21130434438210025
Global Trainning Loss: 2.2056366491317747
Global test accurancy: 0.21824846705035597
Global test_loss: 2.205167212486267
Global Precision: 0.31223011900665915
Global Recall: 0.21824846705035597
Global f1score: 0.21272119079095866
50
50
number of selected users 50
Global Trainning Accurancy: 0.21334527535370063
Global Trainning Loss: 2.2047244834899904
Global test accurancy: 0.2180682710578112
Global test_loss: 2.2043792009353638
Global Precision: 0.3123324732272243
Global Recall: 0.2180682710578112
Global f1score: 0.21302072666223135
50
50
number of selected users 50
Global Trainning Accurancy: 0.21423189643653281
Global Trainning Loss: 2.2038305759429933
Global test accurancy: 0.2188707088681268
Global test_loss: 2.2036147785186766
Global Precision: 0.31303943353430336
Global Recall: 0.2188707088681268
Global f1score: 0.21438231749531786
50
50
number of selected users 50
Global Trainning Accurancy: 0.21558776939466967
Global Trainning Loss: 2.202847414016724
Global test accurancy: 0.21926344068940362
Global test_loss: 2.202777037620544
Global Precision: 0.3148799559453341
Global Recall: 0.21926344068940362
Global f1score: 0.21542767330094212
50
50
number of selected users 50
Global Trainning Accurancy: 0.21610156851453777
Global Trainning Loss: 2.2019354248046876
Global test accurancy: 0.21845377532520907
Global test_loss: 2.202007145881653
Global Precision: 0.31183659842026507
Global Recall: 0.21845377532520907
Global f1score: 0.21484817880488988
50
50
number of selected users 50
Global Trainning Accurancy: 0.21655609705142964
Global Trainning Loss: 2.2009710025787355
Global test accurancy: 0.21880873295442024
Global test_loss: 2.201193528175354
Global Precision: 0.3182127127055912
Global Recall: 0.21880873295442024
Global f1score: 0.21640865158392975
50
50
number of selected users 50
Global Trainning Accurancy: 0.2173009241201601
Global Trainning Loss: 2.199987545013428
Global test accurancy: 0.21971627946728448
Global test_loss: 2.2003754568099976
Global Precision: 0.31718305787307227
Global Recall: 0.21971627946728448
Global f1score: 0.21796580794810969
50
50
number of selected users 50
Global Trainning Accurancy: 0.21879001989994304
Global Trainning Loss: 2.199084062576294
Global test accurancy: 0.22000676705166203
Global test_loss: 2.1996443319320678
Global Precision: 0.3177570258090257
Global Recall: 0.22000676705166203
Global f1score: 0.2194934354986638
50
50
number of selected users 50
Global Trainning Accurancy: 0.21991078309565407
Global Trainning Loss: 2.19810489654541
Global test accurancy: 0.22075141906111515
Global test_loss: 2.1988316345214844
Global Precision: 0.31842615623258524
Global Recall: 0.22075141906111515
Global f1score: 0.22059599954096176
50
50
number of selected users 50
Global Trainning Accurancy: 0.2212846551389995
Global Trainning Loss: 2.1971934747695925
Global test accurancy: 0.22142394254465694
Global test_loss: 2.1980988454818724
Global Precision: 0.31724092014592187
Global Recall: 0.22142394254465694
Global f1score: 0.22210208402202666
50
50
number of selected users 50
Global Trainning Accurancy: 0.22230764146552454
Global Trainning Loss: 2.196252660751343
Global test accurancy: 0.22415767803397324
Global test_loss: 2.1973520946502685
Global Precision: 0.3171340202181942
Global Recall: 0.22415767803397324
Global f1score: 0.22532259795320905
50
50
number of selected users 50
Global Trainning Accurancy: 0.2228082705607577
Global Trainning Loss: 2.195337677001953
Global test accurancy: 0.22458373748489802
Global test_loss: 2.1966313314437866
Global Precision: 0.31841393745296886
Global Recall: 0.22458373748489802
Global f1score: 0.22669925626691456
50
50
number of selected users 50
Global Trainning Accurancy: 0.2233392733676269
Global Trainning Loss: 2.194403676986694
Global test accurancy: 0.2259744916190825
Global test_loss: 2.1959046602249144
Global Precision: 0.32284531513507236
Global Recall: 0.2259744916190825
Global f1score: 0.22943491780535225
50
50
number of selected users 50
Global Trainning Accurancy: 0.22427207814610028
Global Trainning Loss: 2.193522391319275
Global test accurancy: 0.22560937722549898
Global test_loss: 2.195252709388733
Global Precision: 0.3244601311296683
Global Recall: 0.22560937722549898
Global f1score: 0.22987313285200722
50
50
number of selected users 50
Global Trainning Accurancy: 0.22624645882970776
Global Trainning Loss: 2.1925815868377687
Global test accurancy: 0.22587183787834395
Global test_loss: 2.194517526626587
Global Precision: 0.32437793429002704
Global Recall: 0.22587183787834395
Global f1score: 0.23066786105541331
50
50
number of selected users 50
Global Trainning Accurancy: 0.22757781807871086
Global Trainning Loss: 2.191706700325012
Global test accurancy: 0.2269279143131375
Global test_loss: 2.1938744497299196
Global Precision: 0.32407580267280806
Global Recall: 0.2269279143131375
Global f1score: 0.23289576690274885
50
50
number of selected users 50
Global Trainning Accurancy: 0.22885029366957044
Global Trainning Loss: 2.1908197736740114
Global test accurancy: 0.22678873382229758
Global test_loss: 2.193232703208923
Global Precision: 0.3261299815036526
Global Recall: 0.22678873382229758
Global f1score: 0.23425108352831522
50
50
number of selected users 50
Global Trainning Accurancy: 0.23011883879576983
Global Trainning Loss: 2.1899096393585205
Global test accurancy: 0.22819241633231768
Global test_loss: 2.1925824213027956
Global Precision: 0.3286933912566363
Global Recall: 0.22819241633231768
Global f1score: 0.23693820035096724
50
50
number of selected users 50
Global Trainning Accurancy: 0.2305914191570326
Global Trainning Loss: 2.189039497375488
Global test accurancy: 0.22884927678167108
Global test_loss: 2.1919900131225587
Global Precision: 0.3277318466243701
Global Recall: 0.22884927678167108
Global f1score: 0.2378671753823295
50
50
number of selected users 50
Global Trainning Accurancy: 0.23141486631124278
Global Trainning Loss: 2.188116779327393
Global test accurancy: 0.2287112550446908
Global test_loss: 2.1913501405715943
Global Precision: 0.32551905929313846
Global Recall: 0.2287112550446908
Global f1score: 0.23774341780315342
50
50
number of selected users 50
Global Trainning Accurancy: 0.23245043006839272
Global Trainning Loss: 2.1872359561920165
Global test accurancy: 0.229432466710522
Global test_loss: 2.1907591342926027
Global Precision: 0.3274119362360511
Global Recall: 0.229432466710522
Global f1score: 0.23889229535424628
50
50
number of selected users 50
Global Trainning Accurancy: 0.23319918162706754
Global Trainning Loss: 2.186403207778931
Global test accurancy: 0.23201011661125376
Global test_loss: 2.190232448577881
Global Precision: 0.32889276692021163
Global Recall: 0.23201011661125376
Global f1score: 0.24148755141673484
50
50
number of selected users 50
Global Trainning Accurancy: 0.23377445197582036
Global Trainning Loss: 2.1855341529846193
Global test accurancy: 0.2338256453128729
Global test_loss: 2.189667959213257
Global Precision: 0.3320485925733677
Global Recall: 0.2338256453128729
Global f1score: 0.243247386035962
50
50
number of selected users 50
Global Trainning Accurancy: 0.23420860838334695
Global Trainning Loss: 2.184739098548889
Global test accurancy: 0.23350074656272443
Global test_loss: 2.189170789718628
Global Precision: 0.33181548063677063
Global Recall: 0.23350074656272443
Global f1score: 0.2430722506872843
50
50
number of selected users 50
Global Trainning Accurancy: 0.23512036853654134
Global Trainning Loss: 2.183917369842529
Global test accurancy: 0.23358564507261287
Global test_loss: 2.1886542415618897
Global Precision: 0.33194283195307384
Global Recall: 0.23358564507261287
Global f1score: 0.24342807247225964
50
50
number of selected users 50
Global Trainning Accurancy: 0.23571184470242476
Global Trainning Loss: 2.1831648683547975
Global test accurancy: 0.23391778821503767
Global test_loss: 2.1882027769088745
Global Precision: 0.3402253363566017
Global Recall: 0.23391778821503767
Global f1score: 0.24469122763649595
50
50
number of selected users 50
Global Trainning Accurancy: 0.23547027072802146
Global Trainning Loss: 2.1824232149124145
Global test accurancy: 0.2351366164370956
Global test_loss: 2.1877917766571047
Global Precision: 0.3421418946849094
Global Recall: 0.2351366164370956
Global f1score: 0.24711274423003257
50
50
number of selected users 50
Global Trainning Accurancy: 0.23586390914447208
Global Trainning Loss: 2.1816919946670534
Global test accurancy: 0.23421289903046733
Global test_loss: 2.1874083852767945
Global Precision: 0.34028873724042863
Global Recall: 0.23421289903046733
Global f1score: 0.2467493836422251
50
50
number of selected users 50
Global Trainning Accurancy: 0.2361123561462132
Global Trainning Loss: 2.180845742225647
Global test accurancy: 0.23545278620281568
Global test_loss: 2.1869073820114138
Global Precision: 0.34165602651853066
Global Recall: 0.23545278620281568
Global f1score: 0.24815690405770932
50
50
number of selected users 50
Global Trainning Accurancy: 0.23674358880319044
Global Trainning Loss: 2.1800111818313597
Global test accurancy: 0.23709268396170247
Global test_loss: 2.1864138412475587
Global Precision: 0.3406595418997663
Global Recall: 0.23709268396170247
Global f1score: 0.25029922969619545
50
50
number of selected users 50
Global Trainning Accurancy: 0.23780402443617787
Global Trainning Loss: 2.1792114973068237
Global test accurancy: 0.2379087989670786
Global test_loss: 2.1859789991378786
Global Precision: 0.34346319322313573
Global Recall: 0.2379087989670786
Global f1score: 0.25164513974126834
50
50
number of selected users 50
Global Trainning Accurancy: 0.2389311780639817
Global Trainning Loss: 2.17840003490448
Global test accurancy: 0.23890406176956733
Global test_loss: 2.1855362844467163
Global Precision: 0.3410575359819882
Global Recall: 0.23890406176956733
Global f1score: 0.2530022850275754
50
50
number of selected users 50
Global Trainning Accurancy: 0.23979521561085065
Global Trainning Loss: 2.177680950164795
Global test accurancy: 0.23840714632412116
Global test_loss: 2.1851954078674316
Global Precision: 0.3411463148086516
Global Recall: 0.23840714632412116
Global f1score: 0.2527080176534396
50
50
number of selected users 50
Global Trainning Accurancy: 0.24046009943478097
Global Trainning Loss: 2.176836123466492
Global test accurancy: 0.2394002718529589
Global test_loss: 2.184706935882568
Global Precision: 0.34021877251490706
Global Recall: 0.2394002718529589
Global f1score: 0.25462362115738774
50
50
number of selected users 50
Global Trainning Accurancy: 0.24105942596344063
Global Trainning Loss: 2.17607017993927
Global test accurancy: 0.23889920549034133
Global test_loss: 2.1843479537963866
Global Precision: 0.33723145944153826
Global Recall: 0.23889920549034133
Global f1score: 0.2541026741758874
50
50
number of selected users 50
Global Trainning Accurancy: 0.24131984235861084
Global Trainning Loss: 2.1753402090072633
Global test accurancy: 0.23805639061432585
Global test_loss: 2.1840154361724853
Global Precision: 0.33724980270868554
Global Recall: 0.23805639061432585
Global f1score: 0.2537438978103118
50
50
number of selected users 50
Global Trainning Accurancy: 0.24265612182203633
Global Trainning Loss: 2.1745449542999267
Global test accurancy: 0.2393532303707996
Global test_loss: 2.183610143661499
Global Precision: 0.33763868125399765
Global Recall: 0.2393532303707996
Global f1score: 0.2555903264815012
50
50
number of selected users 50
Global Trainning Accurancy: 0.24227222435034165
Global Trainning Loss: 2.173879232406616
Global test accurancy: 0.23789498980989837
Global test_loss: 2.1833683013916017
Global Precision: 0.3393930920902963
Global Recall: 0.23789498980989837
Global f1score: 0.2541354918899429
50
50
number of selected users 50
Global Trainning Accurancy: 0.2424924912819265
Global Trainning Loss: 2.173065686225891
Global test accurancy: 0.23852415227101148
Global test_loss: 2.1829551792144777
Global Precision: 0.3413004133812072
Global Recall: 0.23852415227101148
Global f1score: 0.254440764066971
50
50
number of selected users 50
Global Trainning Accurancy: 0.24298447211586557
Global Trainning Loss: 2.1723581552505493
Global test accurancy: 0.2376429816057669
Global test_loss: 2.1826552534103394
Global Precision: 0.34020171702403385
Global Recall: 0.2376429816057669
Global f1score: 0.25294007265279905
50
50
number of selected users 50
Global Trainning Accurancy: 0.24370101532674768
Global Trainning Loss: 2.171547431945801
Global test accurancy: 0.2374765374517179
Global test_loss: 2.18226243019104
Global Precision: 0.339343433222852
Global Recall: 0.2374765374517179
Global f1score: 0.25299935384320715
50
50
number of selected users 50
Global Trainning Accurancy: 0.24483080434770865
Global Trainning Loss: 2.1708649349212648
Global test accurancy: 0.23627787711447829
Global test_loss: 2.1819989252090455
Global Precision: 0.3347510618436537
Global Recall: 0.23627787711447829
Global f1score: 0.25183090406372677
50
50
number of selected users 50
Global Trainning Accurancy: 0.24534177223157255
Global Trainning Loss: 2.1700426578521728
Global test accurancy: 0.23522721421065343
Global test_loss: 2.181579523086548
Global Precision: 0.3339231788337007
Global Recall: 0.23522721421065343
Global f1score: 0.25129946765768263
50
50
number of selected users 50
Global Trainning Accurancy: 0.24603926223249103
Global Trainning Loss: 2.1693327760696413
Global test accurancy: 0.23616837499348264
Global test_loss: 2.181273889541626
Global Precision: 0.3356935981315127
Global Recall: 0.23616837499348264
Global f1score: 0.2525948130924946
50
50
number of selected users 50
Global Trainning Accurancy: 0.2463696469998643
Global Trainning Loss: 2.1686380910873413
Global test accurancy: 0.23683600262023766
Global test_loss: 2.1809958267211913
Global Precision: 0.3364243768215685
Global Recall: 0.23683600262023766
Global f1score: 0.2533429040914748
50
50
number of selected users 50
Global Trainning Accurancy: 0.24615703720584267
Global Trainning Loss: 2.167876987457275
Global test accurancy: 0.23782198705357777
Global test_loss: 2.180634059906006
Global Precision: 0.3371762079537729
Global Recall: 0.23782198705357777
Global f1score: 0.2545490093587056
50
50
number of selected users 50
Global Trainning Accurancy: 0.24634215095285072
Global Trainning Loss: 2.167174024581909
Global test accurancy: 0.23886273688087853
Global test_loss: 2.180335102081299
Global Precision: 0.3395171675756152
Global Recall: 0.23886273688087853
Global f1score: 0.25612443739355195
50
50
number of selected users 50
Global Trainning Accurancy: 0.24672446637884338
Global Trainning Loss: 2.1664701795578
Global test accurancy: 0.23864808431646406
Global test_loss: 2.1800253772735596
Global Precision: 0.339893791967806
Global Recall: 0.23864808431646406
Global f1score: 0.2560551340445565
50
50
number of selected users 50
Global Trainning Accurancy: 0.2469040208142986
Global Trainning Loss: 2.1656973266601565
Global test accurancy: 0.23909650865080886
Global test_loss: 2.1796412086486816
Global Precision: 0.338334348559856
Global Recall: 0.23909650865080886
Global f1score: 0.2569962412900147
50
50
number of selected users 50
Global Trainning Accurancy: 0.24784296953198723
Global Trainning Loss: 2.16493706703186
Global test accurancy: 0.23996817886914076
Global test_loss: 2.1792928743362427
Global Precision: 0.3394236504618116
Global Recall: 0.23996817886914076
Global f1score: 0.25814756350896245
50
50
number of selected users 50
Global Trainning Accurancy: 0.24875529404162358
Global Trainning Loss: 2.164261474609375
Global test accurancy: 0.2401370889991899
Global test_loss: 2.179080772399902
Global Precision: 0.3377801496812099
Global Recall: 0.2401370889991899
Global f1score: 0.2582687157259639
50
50
number of selected users 50
Global Trainning Accurancy: 0.2489066961797269
Global Trainning Loss: 2.1635461330413817
Global test accurancy: 0.2385582735638232
Global test_loss: 2.1788323497772217
Global Precision: 0.33590303783755004
Global Recall: 0.2385582735638232
Global f1score: 0.2568006785684268
50
50
number of selected users 50
Global Trainning Accurancy: 0.24905628494359766
Global Trainning Loss: 2.162941298484802
Global test accurancy: 0.23953160611242824
Global test_loss: 2.178676118850708
Global Precision: 0.3371750024571784
Global Recall: 0.23953160611242824
Global f1score: 0.2578951256027251
50
50
number of selected users 50
Global Trainning Accurancy: 0.24858781136339378
Global Trainning Loss: 2.162278079986572
Global test accurancy: 0.24072595257675272
Global test_loss: 2.1783958101272582
Global Precision: 0.3392960466905012
Global Recall: 0.24072595257675272
Global f1score: 0.26036857519016926
50
50
number of selected users 50
Global Trainning Accurancy: 0.2496334889226587
Global Trainning Loss: 2.1615747308731077
Global test accurancy: 0.2402392493664457
Global test_loss: 2.1780741834640502
Global Precision: 0.33865319118997983
Global Recall: 0.2402392493664457
Global f1score: 0.25948856045980034
50
50
number of selected users 50
Global Trainning Accurancy: 0.24921032536613658
Global Trainning Loss: 2.161046652793884
Global test accurancy: 0.2388362232697915
Global test_loss: 2.17795569896698
Global Precision: 0.33777408421529725
Global Recall: 0.2388362232697915
Global f1score: 0.25854636836372674
50
50
number of selected users 50
Global Trainning Accurancy: 0.24979501575882454
Global Trainning Loss: 2.1604225206375123
Global test accurancy: 0.2401484867592916
Global test_loss: 2.1777517986297608
Global Precision: 0.3375266763340241
Global Recall: 0.2401484867592916
Global f1score: 0.2599297295758843
50
50
number of selected users 50
Global Trainning Accurancy: 0.25061704014285546
Global Trainning Loss: 2.1597503137588503
Global test accurancy: 0.24052895623018197
Global test_loss: 2.1775226545333863
Global Precision: 0.33845012397844365
Global Recall: 0.24052895623018197
Global f1score: 0.26062683531095887
50
50
number of selected users 50
Global Trainning Accurancy: 0.24994243574527786
Global Trainning Loss: 2.1588337659835815
Global test accurancy: 0.24162229962797235
Global test_loss: 2.177046022415161
Global Precision: 0.3385010443506755
Global Recall: 0.24162229962797235
Global f1score: 0.26148329447378454
50
50
number of selected users 50
Global Trainning Accurancy: 0.2510673361679642
Global Trainning Loss: 2.1583277225494384
Global test accurancy: 0.23988526567266538
Global test_loss: 2.176989321708679
Global Precision: 0.33725329890640676
Global Recall: 0.23988526567266538
Global f1score: 0.26031254436814877
50
50
number of selected users 50
Global Trainning Accurancy: 0.25064093130912973
Global Trainning Loss: 2.1574462842941284
Global test accurancy: 0.24180526028297702
Global test_loss: 2.176566271781921
Global Precision: 0.33866949627296455
Global Recall: 0.24180526028297702
Global f1score: 0.26218421774732864
50
50
number of selected users 50
Global Trainning Accurancy: 0.2508493584979667
Global Trainning Loss: 2.1570271348953245
Global test accurancy: 0.24103660277362324
Global test_loss: 2.1766409730911254
Global Precision: 0.33846492440242765
Global Recall: 0.24103660277362324
Global f1score: 0.2618802870913575
50
50
number of selected users 50
Global Trainning Accurancy: 0.25230282337229126
Global Trainning Loss: 2.1563281774520875
Global test accurancy: 0.2427747876142389
Global test_loss: 2.176411581039429
Global Precision: 0.3399197170014817
Global Recall: 0.2427747876142389
Global f1score: 0.2633985672211676
50
50
number of selected users 50
Global Trainning Accurancy: 0.2532515895400733
Global Trainning Loss: 2.155569624900818
Global test accurancy: 0.24209879432885026
Global test_loss: 2.176138277053833
Global Precision: 0.3385040149432245
Global Recall: 0.24209879432885026
Global f1score: 0.26319622243679525
50
50
number of selected users 50
Global Trainning Accurancy: 0.25294037148725984
Global Trainning Loss: 2.1548467206954958
Global test accurancy: 0.2413448451644757
Global test_loss: 2.1759331607818604
Global Precision: 0.3367508958630411
Global Recall: 0.2413448451644757
Global f1score: 0.26204302158021275
50
50
number of selected users 50
Global Trainning Accurancy: 0.25365870076131675
Global Trainning Loss: 2.154197459220886
Global test accurancy: 0.2419998658519412
Global test_loss: 2.1758324813842775
Global Precision: 0.3357033898933755
Global Recall: 0.2419998658519412
Global f1score: 0.2627186478190421
50
50
number of selected users 50
Global Trainning Accurancy: 0.2543124391236814
Global Trainning Loss: 2.153580455780029
Global test accurancy: 0.24155209562308094
Global test_loss: 2.1757050228118895
Global Precision: 0.3347740895961173
Global Recall: 0.24155209562308094
Global f1score: 0.2624260832216681
50
50
number of selected users 50
Global Trainning Accurancy: 0.2550944387439309
Global Trainning Loss: 2.152991700172424
Global test accurancy: 0.2406760080826041
Global test_loss: 2.1756080055236815
Global Precision: 0.33539555276078764
Global Recall: 0.2406760080826041
Global f1score: 0.2617606616871281
50
50
number of selected users 50
Global Trainning Accurancy: 0.2550371196605328
Global Trainning Loss: 2.152371816635132
Global test accurancy: 0.239763170312704
Global test_loss: 2.1754922389984133
Global Precision: 0.33542782313451275
Global Recall: 0.239763170312704
Global f1score: 0.2613475964256768
50
50
number of selected users 50
Global Trainning Accurancy: 0.25588977315567507
Global Trainning Loss: 2.1517306756973267
Global test accurancy: 0.2392364398196422
Global test_loss: 2.1753554582595824
Global Precision: 0.3349044963807162
Global Recall: 0.2392364398196422
Global f1score: 0.2607831623200588
50
50
number of selected users 50
Global Trainning Accurancy: 0.2552987978500341
Global Trainning Loss: 2.1506730365753173
Global test accurancy: 0.24127331673965047
Global test_loss: 2.1747948122024536
Global Precision: 0.3369553434674217
Global Recall: 0.24127331673965047
Global f1score: 0.2627480383653162
50
50
number of selected users 50
Global Trainning Accurancy: 0.255482620145424
Global Trainning Loss: 2.1500697803497313
Global test accurancy: 0.24138554130447368
Global test_loss: 2.1747083568573
Global Precision: 0.3380549586833891
Global Recall: 0.24138554130447368
Global f1score: 0.26337863680022194
50
50
number of selected users 50
Global Trainning Accurancy: 0.2563771739612484
Global Trainning Loss: 2.1494057846069334
Global test accurancy: 0.2418864750204412
Global test_loss: 2.1746425437927246
Global Precision: 0.3388542287648398
Global Recall: 0.2418864750204412
Global f1score: 0.2638237663380819
50
50
number of selected users 50
Global Trainning Accurancy: 0.2569662897141774
Global Trainning Loss: 2.148728404045105
Global test accurancy: 0.2398315750102395
Global test_loss: 2.1745550632476807
Global Precision: 0.33520113826778963
Global Recall: 0.2398315750102395
Global f1score: 0.2617649644965338
50
50
number of selected users 50
Global Trainning Accurancy: 0.2570149546794837
Global Trainning Loss: 2.14808566570282
Global test accurancy: 0.23894471370452575
Global test_loss: 2.1744904136657714
Global Precision: 0.3352411800089907
Global Recall: 0.23894471370452575
Global f1score: 0.26138422005825634
50
50
number of selected users 50
Global Trainning Accurancy: 0.2568348720341237
Global Trainning Loss: 2.147095069885254
Global test accurancy: 0.2407421228098046
Global test_loss: 2.174150938987732
Global Precision: 0.33658449756921005
Global Recall: 0.2407421228098046
Global f1score: 0.26297566370930736
50
50
number of selected users 50
Global Trainning Accurancy: 0.2579231872383991
Global Trainning Loss: 2.146541323661804
Global test accurancy: 0.24126613321627913
Global test_loss: 2.174207916259766
Global Precision: 0.33851779332479626
Global Recall: 0.24126613321627913
Global f1score: 0.2640996332016953
50
50
number of selected users 50
Global Trainning Accurancy: 0.25738854332417366
Global Trainning Loss: 2.1458580923080444
Global test accurancy: 0.24099750277395726
Global test_loss: 2.174210720062256
Global Precision: 0.3341422515755943
Global Recall: 0.24099750277395726
Global f1score: 0.263261888113923
50
50
number of selected users 50
Global Trainning Accurancy: 0.2580155668492627
Global Trainning Loss: 2.1451425981521606
Global test accurancy: 0.24139944362955273
Global test_loss: 2.1741421175003053
Global Precision: 0.3343335360765793
Global Recall: 0.24139944362955273
Global f1score: 0.2632373974246669
50
50
number of selected users 50
Global Trainning Accurancy: 0.2585115780102202
Global Trainning Loss: 2.14446475982666
Global test accurancy: 0.2405417946454331
Global test_loss: 2.174123010635376
Global Precision: 0.3333594811351004
Global Recall: 0.2405417946454331
Global f1score: 0.26242279373569793
50
50
number of selected users 50
Global Trainning Accurancy: 0.2577717857367108
Global Trainning Loss: 2.1438104724884033
Global test accurancy: 0.24074105301842702
Global test_loss: 2.1741379165649413
Global Precision: 0.33488046207909
Global Recall: 0.24074105301842702
Global f1score: 0.26343006413364556
50
50
number of selected users 50
Global Trainning Accurancy: 0.2584340434250276
Global Trainning Loss: 2.1431650590896605
Global test accurancy: 0.24153727486999713
Global test_loss: 2.1741410207748415
Global Precision: 0.33674802825617606
Global Recall: 0.24153727486999713
Global f1score: 0.2640945828475138
50
50
number of selected users 50
Global Trainning Accurancy: 0.25845779989946277
Global Trainning Loss: 2.1426218366622924
Global test accurancy: 0.23955595433771168
Global test_loss: 2.174156913757324
Global Precision: 0.33459437789917285
Global Recall: 0.23955595433771168
Global f1score: 0.26231063063956955
50
50
number of selected users 50
Global Trainning Accurancy: 0.25788429166121846
Global Trainning Loss: 2.1421235752105714
Global test accurancy: 0.23967519049009062
Global test_loss: 2.1743022537231447
Global Precision: 0.3359198750783622
Global Recall: 0.23967519049009062
Global f1score: 0.2623081063446942
50
50
number of selected users 50
Global Trainning Accurancy: 0.2583897884476194
Global Trainning Loss: 2.1414457416534423
Global test accurancy: 0.23981859595897034
Global test_loss: 2.1742122173309326
Global Precision: 0.3357884554014678
Global Recall: 0.23981859595897034
Global f1score: 0.26249987631917204
50
50
number of selected users 50
Global Trainning Accurancy: 0.25912836072706597
Global Trainning Loss: 2.140731520652771
Global test accurancy: 0.24014480896570808
Global test_loss: 2.17414879322052
Global Precision: 0.3367115065198463
Global Recall: 0.24014480896570808
Global f1score: 0.26274373562097986
50
50
number of selected users 50
Global Trainning Accurancy: 0.260241347880082
Global Trainning Loss: 2.140088424682617
Global test accurancy: 0.23853686039819447
Global test_loss: 2.174165759086609
Global Precision: 0.3362519839443193
Global Recall: 0.23853686039819447
Global f1score: 0.26127812869294165
50
50
number of selected users 50
Global Trainning Accurancy: 0.2598760918367772
Global Trainning Loss: 2.1393897914886475
Global test accurancy: 0.23874380510917287
Global test_loss: 2.1741014528274536
Global Precision: 0.33784008945420996
Global Recall: 0.23874380510917287
Global f1score: 0.26163177703015367
50
50
number of selected users 50
Global Trainning Accurancy: 0.2595857369337905
Global Trainning Loss: 2.1387610244750976
Global test accurancy: 0.237251455705691
Global test_loss: 2.1741409206390383
Global Precision: 0.3374503358118731
Global Recall: 0.237251455705691
Global f1score: 0.2602108293117853
50
50
number of selected users 50
Global Trainning Accurancy: 0.26033227646433643
Global Trainning Loss: 2.138216161727905
Global test accurancy: 0.23744951176528634
Global test_loss: 2.1742750883102415
Global Precision: 0.33642653327777566
Global Recall: 0.23744951176528634
Global f1score: 0.26060958000103845
50
50
number of selected users 50
Global Trainning Accurancy: 0.2598439735566789
Global Trainning Loss: 2.137545952796936
Global test accurancy: 0.23732840878141748
Global test_loss: 2.174297904968262
Global Precision: 0.3362944109070762
Global Recall: 0.23732840878141748
Global f1score: 0.2603800152786165
50
50
number of selected users 50
Global Trainning Accurancy: 0.26008112029583497
Global Trainning Loss: 2.1368967866897584
Global test accurancy: 0.23706336218857726
Global test_loss: 2.174376616477966
Global Precision: 0.3362391283815218
Global Recall: 0.23706336218857726
Global f1score: 0.26023441133406044
50
50
number of selected users 50
Global Trainning Accurancy: 0.259904824055993
Global Trainning Loss: 2.136310381889343
Global test accurancy: 0.2379387119358719
Global test_loss: 2.1745701217651368
Global Precision: 0.3384890502381446
Global Recall: 0.2379387119358719
Global f1score: 0.2614453000459383
50
50
number of selected users 50
Global Trainning Accurancy: 0.259525976020937
Global Trainning Loss: 2.135386199951172
Global test accurancy: 0.23699999405108774
Global test_loss: 2.1743840312957765
Global Precision: 0.3366291027325942
Global Recall: 0.23699999405108774
Global f1score: 0.2602187377385932
50
50
number of selected users 50
Global Trainning Accurancy: 0.2603641219184242
Global Trainning Loss: 2.1347376489639283
Global test accurancy: 0.23685825380881517
Global test_loss: 2.1745406007766723
Global Precision: 0.33488736949133013
Global Recall: 0.23685825380881517
Global f1score: 0.2595469781750709
50
50
number of selected users 50
Global Trainning Accurancy: 0.2609405518909539
Global Trainning Loss: 2.1343140697479246
Global test accurancy: 0.237810269368903
Global test_loss: 2.174899139404297
Global Precision: 0.33617473249236907
Global Recall: 0.237810269368903
Global f1score: 0.2607362773603288
50
50
number of selected users 50
Global Trainning Accurancy: 0.26086354673537077
Global Trainning Loss: 2.1335603046417235
Global test accurancy: 0.23740666730298135
Global test_loss: 2.174918737411499
Global Precision: 0.336150058782326
Global Recall: 0.23740666730298135
Global f1score: 0.26100414675221345
50
50
number of selected users 50
Global Trainning Accurancy: 0.260664386650996
Global Trainning Loss: 2.132948832511902
Global test accurancy: 0.2392064235733822
Global test_loss: 2.175214619636536
Global Precision: 0.3372112100617321
Global Recall: 0.2392064235733822
Global f1score: 0.2626636565496455
50
50
number of selected users 50
Global Trainning Accurancy: 0.26176737715822984
Global Trainning Loss: 2.132103929519653
Global test accurancy: 0.23926990094471026
Global test_loss: 2.1752190494537356
Global Precision: 0.3373060659579258
Global Recall: 0.23926990094471026
Global f1score: 0.26264508592678454
50
50
number of selected users 50
Global Trainning Accurancy: 0.2615120536568129
Global Trainning Loss: 2.1312039375305174
Global test accurancy: 0.23976820644769825
Global test_loss: 2.175132112503052
Global Precision: 0.3361889727810228
Global Recall: 0.23976820644769825
Global f1score: 0.2627016036065452
50
50
number of selected users 50
Global Trainning Accurancy: 0.2622482142580789
Global Trainning Loss: 2.130526466369629
Global test accurancy: 0.23946293828220963
Global test_loss: 2.1754026985168458
Global Precision: 0.33598918768938596
Global Recall: 0.23946293828220963
Global f1score: 0.26241822184123753
50
50
number of selected users 50
Global Trainning Accurancy: 0.2618619021935947
Global Trainning Loss: 2.1299972248077395
Global test accurancy: 0.23842456138986332
Global test_loss: 2.1758505392074583
Global Precision: 0.3355465100309023
Global Recall: 0.23842456138986332
Global f1score: 0.2616078902162488
50
50
number of selected users 50
Global Trainning Accurancy: 0.2616100154846863
Global Trainning Loss: 2.129545545578003
Global test accurancy: 0.23630327439749566
Global test_loss: 2.1763669204711915
Global Precision: 0.33313643050718755
Global Recall: 0.23630327439749566
Global f1score: 0.2589817221782759
50
50
number of selected users 50
Global Trainning Accurancy: 0.26255047333095716
Global Trainning Loss: 2.129114031791687
Global test accurancy: 0.23759815229814707
Global test_loss: 2.1770236682891846
Global Precision: 0.3377314607166195
Global Recall: 0.23759815229814707
Global f1score: 0.2607838902747816
50
50
number of selected users 50
Global Trainning Accurancy: 0.26365589840700665
Global Trainning Loss: 2.1282460069656373
Global test accurancy: 0.23681252146268622
Global test_loss: 2.177156004905701
Global Precision: 0.339161546998309
Global Recall: 0.23681252146268622
Global f1score: 0.26048607802955914
50
50
number of selected users 50
Global Trainning Accurancy: 0.2652167822121226
Global Trainning Loss: 2.1274858713150024
Global test accurancy: 0.233841288109228
Global test_loss: 2.17739625453949
Global Precision: 0.33172192891547475
Global Recall: 0.233841288109228
Global f1score: 0.2563078145970342
50
50
number of selected users 50
Global Trainning Accurancy: 0.2662517969694053
Global Trainning Loss: 2.1265855884552
Global test accurancy: 0.23386757994322033
Global test_loss: 2.1774359941482544
Global Precision: 0.33689911845911674
Global Recall: 0.23386757994322033
Global f1score: 0.2576426868213833
50
50
number of selected users 50
Global Trainning Accurancy: 0.26576152650289303
Global Trainning Loss: 2.126025915145874
Global test accurancy: 0.23484012504484178
Global test_loss: 2.1777892971038817
Global Precision: 0.33684142955898166
Global Recall: 0.23484012504484178
Global f1score: 0.2583733350007771
50
50
number of selected users 50
Global Trainning Accurancy: 0.26715271353436176
Global Trainning Loss: 2.124928188323975
Global test accurancy: 0.23356472410184168
Global test_loss: 2.1777052879333496
Global Precision: 0.33717444608318353
Global Recall: 0.23356472410184168
Global f1score: 0.25711450937340696
50
50
number of selected users 50
Global Trainning Accurancy: 0.26737188719945537
Global Trainning Loss: 2.124035367965698
Global test accurancy: 0.23406954927800477
Global test_loss: 2.177790207862854
Global Precision: 0.3375883865785748
Global Recall: 0.23406954927800477
Global f1score: 0.2577777342276898
exp_no  0
0_dataset_CIFAR10_algorithm_MOON_KL_model_CNN_3_50_0.4_31_07_2024
