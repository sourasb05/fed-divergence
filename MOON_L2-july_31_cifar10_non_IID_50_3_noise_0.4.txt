============================================================
Summary of training process:
FL Algorithm: MOON_L2
model: CNN
optimizer: SGD
Batch size: 124
Global_iters: 200
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
============================================================
/proj/bhuyan24/fed-divergence
CIFAR10
./data/data/noisy/0.4_50_3/train/cifa_train.json
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:22<1:16:09, 22.96s/it]  1%|          | 2/200 [00:39<1:02:46, 19.02s/it]  2%|▏         | 3/200 [00:55<58:26, 17.80s/it]    2%|▏         | 4/200 [01:12<56:28, 17.29s/it]  2%|▎         | 5/200 [01:27<54:32, 16.78s/it]  3%|▎         | 6/200 [01:43<53:15, 16.47s/it]  4%|▎         | 7/200 [01:59<52:28, 16.31s/it]  4%|▍         | 8/200 [02:15<51:59, 16.25s/it]  4%|▍         | 9/200 [02:32<51:46, 16.27s/it]  5%|▌         | 10/200 [02:48<51:41, 16.32s/it]  6%|▌         | 11/200 [03:05<51:54, 16.48s/it]  6%|▌         | 12/200 [03:22<52:22, 16.71s/it]  6%|▋         | 13/200 [03:40<52:41, 16.91s/it]  7%|▋         | 14/200 [03:57<52:54, 17.07s/it]  8%|▊         | 15/200 [04:15<53:07, 17.23s/it]  8%|▊         | 16/200 [04:32<53:05, 17.31s/it]  8%|▊         | 17/200 [04:49<52:44, 17.29s/it]  9%|▉         | 18/200 [05:06<52:03, 17.16s/it] 10%|▉         | 19/200 [05:23<51:06, 16.94s/it] 10%|█         | 20/200 [05:39<50:19, 16.77s/it] 10%|█         | 21/200 [05:55<49:30, 16.59s/it] 11%|█         | 22/200 [06:11<48:50, 16.46s/it] 12%|█▏        | 23/200 [06:27<48:11, 16.34s/it] 12%|█▏        | 24/200 [06:43<47:37, 16.23s/it] 12%|█▎        | 25/200 [06:59<47:08, 16.16s/it] 13%|█▎        | 26/200 [07:15<46:40, 16.09s/it] 14%|█▎        | 27/200 [07:32<46:25, 16.10s/it] 14%|█▍        | 28/200 [07:48<46:14, 16.13s/it] 14%|█▍        | 29/200 [08:04<46:00, 16.14s/it] 15%|█▌        | 30/200 [08:20<45:47, 16.16s/it] 16%|█▌        | 31/200 [08:36<45:29, 16.15s/it] 16%|█▌        | 32/200 [08:52<45:15, 16.16s/it] 16%|█▋        | 33/200 [09:09<44:56, 16.15s/it] 17%|█▋        | 34/200 [09:25<44:44, 16.17s/it] 18%|█▊        | 35/200 [09:41<44:35, 16.21s/it] 18%|█▊        | 36/200 [09:57<44:22, 16.23s/it] 18%|█▊        | 37/200 [10:14<44:08, 16.25s/it] 19%|█▉        | 38/200 [10:30<43:53, 16.26s/it] 20%|█▉        | 39/200 [10:46<43:39, 16.27s/it] 20%|██        | 40/200 [11:03<43:26, 16.29s/it] 20%|██        | 41/200 [11:19<43:16, 16.33s/it] 21%|██        | 42/200 [11:35<43:06, 16.37s/it] 22%|██▏       | 43/200 [11:52<42:49, 16.37s/it] 22%|██▏       | 44/200 [12:08<42:30, 16.35s/it] 22%|██▎       | 45/200 [12:24<42:14, 16.35s/it] 23%|██▎       | 46/200 [12:41<41:55, 16.33s/it] 24%|██▎       | 47/200 [12:57<41:37, 16.32s/it] 24%|██▍       | 48/200 [13:13<41:21, 16.33s/it] 24%|██▍       | 49/200 [13:30<40:59, 16.29s/it] 25%|██▌       | 50/200 [13:46<40:45, 16.30s/it] 26%|██▌       | 51/200 [14:02<40:27, 16.29s/it] 26%|██▌       | 52/200 [14:19<40:14, 16.31s/it] 26%|██▋       | 53/200 [14:35<39:57, 16.31s/it] 27%|██▋       | 54/200 [14:51<39:39, 16.29s/it] 28%|██▊       | 55/200 [15:07<39:22, 16.29s/it] 28%|██▊       | 56/200 [15:24<39:07, 16.30s/it] 28%|██▊       | 57/200 [15:40<38:48, 16.28s/it] 29%|██▉       | 58/200 [15:56<38:35, 16.30s/it] 30%|██▉       | 59/200 [16:13<38:17, 16.29s/it] 30%|███       | 60/200 [16:29<37:59, 16.29s/it] 30%|███       | 61/200 [16:45<37:44, 16.29s/it] 31%|███       | 62/200 [17:01<37:24, 16.26s/it] 32%|███▏      | 63/200 [17:17<37:01, 16.21s/it] 32%|███▏      | 64/200 [17:33<36:38, 16.16s/it] 32%|███▎      | 65/200 [17:50<36:21, 16.16s/it] 33%|███▎      | 66/200 [18:06<36:01, 16.13s/it] 34%|███▎      | 67/200 [18:22<35:45, 16.13s/it] 34%|███▍      | 68/200 [18:38<35:34, 16.17s/it] 34%|███▍      | 69/200 [18:54<35:15, 16.15s/it] 35%|███▌      | 70/200 [19:10<34:59, 16.15s/it] 36%|███▌      | 71/200 [19:27<34:44, 16.16s/it] 36%|███▌      | 72/200 [19:43<34:29, 16.17s/it] 36%|███▋      | 73/200 [19:59<34:14, 16.17s/it] 37%|███▋      | 74/200 [20:15<33:56, 16.17s/it] 38%|███▊      | 75/200 [20:31<33:40, 16.16s/it] 38%|███▊      | 76/200 [20:47<33:20, 16.14s/it] 38%|███▊      | 77/200 [21:03<33:00, 16.10s/it] 39%|███▉      | 78/200 [21:19<32:43, 16.10s/it] 40%|███▉      | 79/200 [21:36<32:29, 16.11s/it] 40%|████      | 80/200 [21:52<32:12, 16.11s/it] 40%|████      | 81/200 [22:08<31:54, 16.09s/it] 41%|████      | 82/200 [22:24<31:41, 16.11s/it] 42%|████▏     | 83/200 [22:40<31:21, 16.09s/it] 42%|████▏     | 84/200 [22:56<31:05, 16.08s/it] 42%|████▎     | 85/200 [23:12<30:51, 16.10s/it] 43%|████▎     | 86/200 [23:28<30:33, 16.08s/it] 44%|████▎     | 87/200 [23:44<30:14, 16.05s/it] 44%|████▍     | 88/200 [24:00<29:58, 16.06s/it] 44%|████▍     | 89/200 [24:16<29:42, 16.05s/it] 45%|████▌     | 90/200 [24:32<29:22, 16.03s/it] 46%|████▌     | 91/200 [24:48<29:07, 16.04s/it] 46%|████▌     | 92/200 [25:04<28:52, 16.04s/it] 46%|████▋     | 93/200 [25:21<28:49, 16.17s/it] 47%|████▋     | 94/200 [25:37<28:42, 16.25s/it] 48%|████▊     | 95/200 [25:53<28:22, 16.21s/it] 48%|████▊     | 96/200 [26:09<27:57, 16.13s/it] 48%|████▊     | 97/200 [26:25<27:40, 16.12s/it] 49%|████▉     | 98/200 [26:41<27:21, 16.09s/it] 50%|████▉     | 99/200 [26:57<27:02, 16.06s/it] 50%|█████     | 100/200 [27:13<26:42, 16.02s/it] 50%|█████     | 101/200 [27:29<26:26, 16.03s/it] 51%|█████     | 102/200 [27:45<26:09, 16.02s/it] 52%|█████▏    | 103/200 [28:01<25:52, 16.00s/it] 52%|█████▏    | 104/200 [28:17<25:34, 15.98s/it] 52%|█████▎    | 105/200 [28:33<25:14, 15.94s/it] 53%|█████▎    | 106/200 [28:49<24:56, 15.92s/it] 54%|█████▎    | 107/200 [29:05<24:38, 15.89s/it] 54%|█████▍    | 108/200 [29:21<24:23, 15.90s/it] 55%|█████▍    | 109/200 [29:37<24:09, 15.93s/it] 55%|█████▌    | 110/200 [29:52<23:49, 15.88s/it] 56%|█████▌    | 111/200 [30:08<23:34, 15.89s/it] 56%|█████▌    | 112/200 [30:24<23:21, 15.93s/it] 56%|█████▋    | 113/200 [30:40<23:06, 15.93s/it] 57%|█████▋    | 114/200 [30:56<22:47, 15.90s/it] 57%|█████▊    | 115/200 [31:12<22:31, 15.90s/it] 58%|█████▊    | 116/200 [31:28<22:13, 15.88s/it] 58%|█████▊    | 117/200 [31:44<21:58, 15.88s/it] 59%|█████▉    | 118/200 [32:00<21:43, 15.90s/it] 60%|█████▉    | 119/200 [32:15<21:23, 15.85s/it] 60%|██████    | 120/200 [32:31<21:07, 15.84s/it] 60%|██████    | 121/200 [32:47<20:51, 15.85s/it] 61%|██████    | 122/200 [33:03<20:34, 15.83s/it] 62%|██████▏   | 123/200 [33:19<20:16, 15.80s/it] 62%|██████▏   | 124/200 [33:34<19:59, 15.78s/it] 62%|██████▎   | 125/200 [33:50<19:41, 15.76s/it] 63%|██████▎   | 126/200 [34:06<19:25, 15.75s/it] 64%|██████▎   | 127/200 [34:21<19:07, 15.72s/it] 64%|██████▍   | 128/200 [34:37<18:51, 15.71s/it] 64%|██████▍   | 129/200 [34:53<18:37, 15.74s/it] 65%|██████▌   | 130/200 [35:09<18:19, 15.71s/it] 66%|██████▌   | 131/200 [35:24<18:02, 15.68s/it] 66%|██████▌   | 132/200 [35:40<17:43, 15.64s/it] 66%|██████▋   | 133/200 [35:55<17:26, 15.62s/it] 67%|██████▋   | 134/200 [36:11<17:09, 15.59s/it] 68%|██████▊   | 135/200 [36:26<16:50, 15.55s/it] 68%|██████▊   | 136/200 [36:42<16:36, 15.57s/it] 68%|██████▊   | 137/200 [36:57<16:20, 15.56s/it] 69%|██████▉   | 138/200 [37:13<16:03, 15.53s/it] 70%|██████▉   | 139/200 [37:28<15:44, 15.48s/it] 70%|███████   | 140/200 [37:44<15:26, 15.44s/it] 70%|███████   | 141/200 [37:59<15:09, 15.41s/it] 71%|███████   | 142/200 [38:14<14:53, 15.40s/it] 72%|███████▏  | 143/200 [38:30<14:37, 15.40s/it] 72%|███████▏  | 144/200 [38:45<14:21, 15.39s/it] 72%|███████▎  | 145/200 [39:01<14:08, 15.43s/it] 73%|███████▎  | 146/200 [39:16<13:52, 15.41s/it] 74%|███████▎  | 147/200 [39:31<13:36, 15.41s/it] 74%|███████▍  | 148/200 [39:47<13:20, 15.40s/it] 74%|███████▍  | 149/200 [40:02<13:04, 15.38s/it] 75%|███████▌  | 150/200 [40:17<12:47, 15.35s/it] 76%|███████▌  | 151/200 [40:33<12:31, 15.34s/it] 76%|███████▌  | 152/200 [40:48<12:16, 15.35s/it] 76%|███████▋  | 153/200 [41:03<12:02, 15.36s/it] 77%|███████▋  | 154/200 [41:19<11:46, 15.37s/it] 78%|███████▊  | 155/200 [41:34<11:30, 15.34s/it] 78%|███████▊  | 156/200 [41:50<11:15, 15.35s/it] 78%|███████▊  | 157/200 [42:05<10:58, 15.32s/it] 79%|███████▉  | 158/200 [42:20<10:44, 15.34s/it] 80%|███████▉  | 159/200 [42:35<10:29, 15.34s/it] 80%|████████  | 160/200 [42:51<10:12, 15.31s/it] 80%|████████  | 161/200 [43:06<09:57, 15.31s/it] 81%|████████  | 162/200 [43:21<09:42, 15.33s/it] 82%|████████▏ | 163/200 [43:37<09:26, 15.31s/it] 82%|████████▏ | 164/200 [43:52<09:11, 15.32s/it] 82%|████████▎ | 165/200 [44:07<08:55, 15.29s/it] 83%|████████▎ | 166/200 [44:22<08:38, 15.26s/it] 84%|████████▎ | 167/200 [44:38<08:24, 15.28s/it] 84%|████████▍ | 168/200 [44:53<08:08, 15.28s/it] 84%|████████▍ | 169/200 [45:08<07:53, 15.28s/it] 85%|████████▌ | 170/200 [45:24<07:38, 15.28s/it] 86%|████████▌ | 171/200 [45:39<07:23, 15.29s/it] 86%|████████▌ | 172/200 [45:54<07:07, 15.26s/it] 86%|████████▋ | 173/200 [46:09<06:51, 15.25s/it] 87%|████████▋ | 174/200 [46:25<06:36, 15.26s/it] 88%|████████▊ | 175/200 [46:40<06:21, 15.26s/it] 88%|████████▊ | 176/200 [46:55<06:06, 15.25s/it] 88%|████████▊ | 177/200 [47:10<05:51, 15.27s/it] 89%|████████▉ | 178/200 [47:26<05:35, 15.24s/it] 90%|████████▉ | 179/200 [47:41<05:19, 15.20s/it] 90%|█████████ | 180/200 [47:56<05:03, 15.19s/it] 90%|█████████ | 181/200 [48:11<04:48, 15.19s/it] 91%|█████████ | 182/200 [48:26<04:33, 15.17s/it] 92%|█████████▏| 183/200 [48:41<04:17, 15.14s/it] 92%|█████████▏| 184/200 [48:56<04:02, 15.13s/it] 92%|█████████▎| 185/200 [49:11<03:46, 15.11s/it] 93%|█████████▎| 186/200 [49:26<03:31, 15.10s/it] 94%|█████████▎| 187/200 [49:42<03:16, 15.08s/it] 94%|█████████▍| 188/200 [49:57<03:00, 15.07s/it] 94%|█████████▍| 189/200 [50:12<02:45, 15.08s/it] 95%|█████████▌| 190/200 [50:27<02:30, 15.07s/it] 96%|█████████▌| 191/200 [50:42<02:15, 15.08s/it] 96%|█████████▌| 192/200 [50:57<02:00, 15.10s/it] 96%|█████████▋| 193/200 [51:12<01:45, 15.10s/it] 97%|█████████▋| 194/200 [51:27<01:30, 15.10s/it] 98%|█████████▊| 195/200 [51:42<01:15, 15.11s/it] 98%|█████████▊| 196/200 [51:57<01:00, 15.11s/it] 98%|█████████▊| 197/200 [52:12<00:45, 15.10s/it] 99%|█████████▉| 198/200 [52:28<00:30, 15.09s/it]100%|█████████▉| 199/200 [52:43<00:15, 15.09s/it]100%|██████████| 200/200 [52:58<00:00, 15.07s/it]100%|██████████| 200/200 [52:58<00:00, 15.89s/it]
50
50
number of selected users 50
Global Trainning Accurancy: 0.10111646519415564
Global Trainning Loss: 2.303696370124817
Global test accurancy: 0.09976566423236043
Global test_loss: 2.303729577064514
Global Precision: 0.0196944641697668
Global Recall: 0.09976566423236043
Global f1score: 0.031088996192530183
50
50
number of selected users 50
Global Trainning Accurancy: 0.10111646519415564
Global Trainning Loss: 2.303154993057251
Global test accurancy: 0.09976566423236043
Global test_loss: 2.303218502998352
Global Precision: 0.0196944641697668
Global Recall: 0.09976566423236043
Global f1score: 0.031088996192530183
50
50
number of selected users 50
Global Trainning Accurancy: 0.10137384332217746
Global Trainning Loss: 2.3026392555236814
Global test accurancy: 0.10041407230176502
Global test_loss: 2.302726635932922
Global Precision: 0.029335283562116562
Global Recall: 0.10041407230176502
Global f1score: 0.032411536809378284
50
50
number of selected users 50
Global Trainning Accurancy: 0.1214861594740296
Global Trainning Loss: 2.3021354150772093
Global test accurancy: 0.11984502039975406
Global test_loss: 2.302248830795288
Global Precision: 0.04468899669000559
Global Recall: 0.11984502039975406
Global f1score: 0.059629242061241795
50
50
number of selected users 50
Global Trainning Accurancy: 0.11034605101112867
Global Trainning Loss: 2.301620855331421
Global test accurancy: 0.11019060691494315
Global test_loss: 2.3017598724365236
Global Precision: 0.04519440816925389
Global Recall: 0.11019060691494315
Global f1score: 0.04949534026020693
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.3011009883880615
Global test accurancy: 0.10385790459408586
Global test_loss: 2.301272692680359
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.300586323738098
Global test accurancy: 0.10385790459408586
Global test_loss: 2.3008072900772096
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.3000458908081054
Global test accurancy: 0.10385790459408586
Global test_loss: 2.3003235721588133
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10731054215905829
Global Trainning Loss: 2.299500560760498
Global test accurancy: 0.10780650138866295
Global test_loss: 2.2998474788665773
Global Precision: 0.060534403812600314
Global Recall: 0.10780650138866295
Global f1score: 0.04175581184275026
50
50
number of selected users 50
Global Trainning Accurancy: 0.12941420138255103
Global Trainning Loss: 2.2990031385421754
Global test accurancy: 0.12983680073587858
Global test_loss: 2.2994385719299317
Global Precision: 0.056892924557716856
Global Recall: 0.12983680073587858
Global f1score: 0.0740718096807587
50
50
number of selected users 50
Global Trainning Accurancy: 0.11635784002260978
Global Trainning Loss: 2.29870258808136
Global test accurancy: 0.11325032987527132
Global test_loss: 2.2992138385772707
Global Precision: 0.051050437952349044
Global Recall: 0.11325032987527132
Global f1score: 0.04807808029069236
50
50
number of selected users 50
Global Trainning Accurancy: 0.11028886274655514
Global Trainning Loss: 2.2986498928070067
Global test accurancy: 0.1104427360643162
Global test_loss: 2.2992434453964234
Global Precision: 0.03267449867417918
Global Recall: 0.1104427360643162
Global f1score: 0.03830339881118923
50
50
number of selected users 50
Global Trainning Accurancy: 0.11014875640319673
Global Trainning Loss: 2.2985381841659547
Global test accurancy: 0.1104427360643162
Global test_loss: 2.2991988039016724
Global Precision: 0.032670005822960126
Global Recall: 0.1104427360643162
Global f1score: 0.038295502235095036
50
50
number of selected users 50
Global Trainning Accurancy: 0.11016131947294446
Global Trainning Loss: 2.2981370496749878
Global test accurancy: 0.1104427360643162
Global test_loss: 2.298814449310303
Global Precision: 0.032672830851969634
Global Recall: 0.1104427360643162
Global f1score: 0.03830038048886346
50
50
number of selected users 50
Global Trainning Accurancy: 0.1302522134862485
Global Trainning Loss: 2.297234926223755
Global test accurancy: 0.13050153919471968
Global test_loss: 2.297891101837158
Global Precision: 0.08185426895740643
Global Recall: 0.13050153919471968
Global f1score: 0.06890749714015082
50
50
number of selected users 50
Global Trainning Accurancy: 0.1471543317451229
Global Trainning Loss: 2.296112995147705
Global test accurancy: 0.14911721556234683
Global test_loss: 2.296649193763733
Global Precision: 0.066098194609862
Global Recall: 0.14911721556234683
Global f1score: 0.07804785078113888
50
50
number of selected users 50
Global Trainning Accurancy: 0.13702700288599887
Global Trainning Loss: 2.295259394645691
Global test accurancy: 0.14106387891398442
Global test_loss: 2.2956550455093385
Global Precision: 0.06272517082336185
Global Recall: 0.14106387891398442
Global f1score: 0.0742185386538003
50
50
number of selected users 50
Global Trainning Accurancy: 0.13103035378848146
Global Trainning Loss: 2.2946266555786132
Global test accurancy: 0.13447960095160585
Global test_loss: 2.2949327278137206
Global Precision: 0.06309382977260848
Global Recall: 0.13447960095160585
Global f1score: 0.06909918827360195
50
50
number of selected users 50
Global Trainning Accurancy: 0.13124101172524086
Global Trainning Loss: 2.293947877883911
Global test accurancy: 0.13501558168836084
Global test_loss: 2.294212164878845
Global Precision: 0.06357914750892125
Global Recall: 0.13501558168836084
Global f1score: 0.06961680754861323
50
50
number of selected users 50
Global Trainning Accurancy: 0.13370234497018801
Global Trainning Loss: 2.293178300857544
Global test accurancy: 0.13645934222323008
Global test_loss: 2.2934243297576904
Global Precision: 0.06305826688332866
Global Recall: 0.13645934222323008
Global f1score: 0.07093012574857104
50
50
number of selected users 50
Global Trainning Accurancy: 0.1366328851722425
Global Trainning Loss: 2.292341933250427
Global test accurancy: 0.1386284026215698
Global test_loss: 2.292579174041748
Global Precision: 0.06289716154181528
Global Recall: 0.1386284026215698
Global f1score: 0.0729026073998851
50
50
number of selected users 50
Global Trainning Accurancy: 0.138554996804875
Global Trainning Loss: 2.2914587259292603
Global test accurancy: 0.14057631686758423
Global test_loss: 2.29169056892395
Global Precision: 0.06642682242776668
Global Recall: 0.14057631686758423
Global f1score: 0.07462880622028166
50
50
number of selected users 50
Global Trainning Accurancy: 0.14009786916879438
Global Trainning Loss: 2.2905490732192995
Global test accurancy: 0.14231669659265397
Global test_loss: 2.2907713985443117
Global Precision: 0.07294035070748349
Global Recall: 0.14231669659265397
Global f1score: 0.07559198209350472
50
50
number of selected users 50
Global Trainning Accurancy: 0.14184893729834852
Global Trainning Loss: 2.2895944833755495
Global test accurancy: 0.14506065848475252
Global test_loss: 2.2898065519332884
Global Precision: 0.07719804814681183
Global Recall: 0.14506065848475252
Global f1score: 0.0776874948644444
50
50
number of selected users 50
Global Trainning Accurancy: 0.14350679996446278
Global Trainning Loss: 2.288610806465149
Global test accurancy: 0.14639301866206536
Global test_loss: 2.2888059759140016
Global Precision: 0.08736532304736849
Global Recall: 0.14639301866206536
Global f1score: 0.07825429487212258
50
50
number of selected users 50
Global Trainning Accurancy: 0.1444326794162323
Global Trainning Loss: 2.2875777816772462
Global test accurancy: 0.14707650566549374
Global test_loss: 2.2877509880065916
Global Precision: 0.09092351613250126
Global Recall: 0.14707650566549374
Global f1score: 0.07907543059944733
50
50
number of selected users 50
Global Trainning Accurancy: 0.14524215845603922
Global Trainning Loss: 2.2865104246139527
Global test accurancy: 0.14799050853529241
Global test_loss: 2.2866657161712647
Global Precision: 0.0950782565450861
Global Recall: 0.14799050853529241
Global f1score: 0.08015910935977388
50
50
number of selected users 50
Global Trainning Accurancy: 0.1463783125889318
Global Trainning Loss: 2.2854053592681884
Global test accurancy: 0.14914910254391453
Global test_loss: 2.2855396699905395
Global Precision: 0.09490635868030924
Global Recall: 0.14914910254391453
Global f1score: 0.08062284242975613
50
50
number of selected users 50
Global Trainning Accurancy: 0.14687906464090048
Global Trainning Loss: 2.2842583513259886
Global test accurancy: 0.14862129598095344
Global test_loss: 2.2843744468688967
Global Precision: 0.08910234714285722
Global Recall: 0.14862129598095344
Global f1score: 0.08078885337199997
50
50
number of selected users 50
Global Trainning Accurancy: 0.147727460514076
Global Trainning Loss: 2.283066501617432
Global test accurancy: 0.14887313295086374
Global test_loss: 2.2831656074523927
Global Precision: 0.08986063883010306
Global Recall: 0.14887313295086374
Global f1score: 0.08116292036380249
50
50
number of selected users 50
Global Trainning Accurancy: 0.14806776825021775
Global Trainning Loss: 2.281824884414673
Global test accurancy: 0.1505723265719515
Global test_loss: 2.281906456947327
Global Precision: 0.09262118616065522
Global Recall: 0.1505723265719515
Global f1score: 0.08323932285226329
50
50
number of selected users 50
Global Trainning Accurancy: 0.14907361554606458
Global Trainning Loss: 2.280535717010498
Global test accurancy: 0.1510439755323101
Global test_loss: 2.2805975532531737
Global Precision: 0.08678533871875296
Global Recall: 0.1510439755323101
Global f1score: 0.08364571542174175
50
50
number of selected users 50
Global Trainning Accurancy: 0.14930314919742066
Global Trainning Loss: 2.2791974258422854
Global test accurancy: 0.151468032228621
Global test_loss: 2.279233684539795
Global Precision: 0.09329854123253553
Global Recall: 0.151468032228621
Global f1score: 0.08469864889289908
50
50
number of selected users 50
Global Trainning Accurancy: 0.1495992329657228
Global Trainning Loss: 2.277804913520813
Global test accurancy: 0.1514185876292424
Global test_loss: 2.277812991142273
Global Precision: 0.09102845306980933
Global Recall: 0.1514185876292424
Global f1score: 0.08492230741374887
50
50
number of selected users 50
Global Trainning Accurancy: 0.1504186216747822
Global Trainning Loss: 2.2763692331314087
Global test accurancy: 0.1532977066929915
Global test_loss: 2.2763469982147218
Global Precision: 0.09477899958087843
Global Recall: 0.1532977066929915
Global f1score: 0.08713960664913588
50
50
number of selected users 50
Global Trainning Accurancy: 0.151771985457418
Global Trainning Loss: 2.2748848390579224
Global test accurancy: 0.1536744166144584
Global test_loss: 2.2748272466659545
Global Precision: 0.09143365808619384
Global Recall: 0.1536744166144584
Global f1score: 0.08856204027905665
50
50
number of selected users 50
Global Trainning Accurancy: 0.15298869890104555
Global Trainning Loss: 2.2733566999435424
Global test accurancy: 0.155919000037011
Global test_loss: 2.273262033462524
Global Precision: 0.09845042889517208
Global Recall: 0.155919000037011
Global f1score: 0.09225974546373389
50
50
number of selected users 50
Global Trainning Accurancy: 0.1550643279980065
Global Trainning Loss: 2.2717964601516725
Global test accurancy: 0.15714386080885792
Global test_loss: 2.2716661500930786
Global Precision: 0.122136801492843
Global Recall: 0.15714386080885792
Global f1score: 0.09575045501059913
50
50
number of selected users 50
Global Trainning Accurancy: 0.15598777765928198
Global Trainning Loss: 2.270206918716431
Global test accurancy: 0.15891018307717006
Global test_loss: 2.270040063858032
Global Precision: 0.13970332955132844
Global Recall: 0.15891018307717006
Global f1score: 0.0991230825727027
50
50
number of selected users 50
Global Trainning Accurancy: 0.1578459129089401
Global Trainning Loss: 2.268588104248047
Global test accurancy: 0.1603432247759508
Global test_loss: 2.2683803129196165
Global Precision: 0.14333367054738702
Global Recall: 0.1603432247759508
Global f1score: 0.10192570725030226
50
50
number of selected users 50
Global Trainning Accurancy: 0.15924983196311124
Global Trainning Loss: 2.2669466161727905
Global test accurancy: 0.16296394462020353
Global test_loss: 2.2666961908340455
Global Precision: 0.149581185962379
Global Recall: 0.16296394462020353
Global f1score: 0.10596776524882077
50
50
number of selected users 50
Global Trainning Accurancy: 0.1608392116743332
Global Trainning Loss: 2.265295844078064
Global test accurancy: 0.16405533603663752
Global test_loss: 2.2650019979476927
Global Precision: 0.15388696209854105
Global Recall: 0.16405533603663752
Global f1score: 0.10819433137309833
50
50
number of selected users 50
Global Trainning Accurancy: 0.1620122720349983
Global Trainning Loss: 2.263647770881653
Global test accurancy: 0.1644546548702043
Global test_loss: 2.2633083057403565
Global Precision: 0.15864488192796655
Global Recall: 0.1644546548702043
Global f1score: 0.11003014298955185
50
50
number of selected users 50
Global Trainning Accurancy: 0.1636557284673952
Global Trainning Loss: 2.2620038843154906
Global test accurancy: 0.16686499101850782
Global test_loss: 2.261616325378418
Global Precision: 0.1736929631056201
Global Recall: 0.16686499101850782
Global f1score: 0.11394080776539207
50
50
number of selected users 50
Global Trainning Accurancy: 0.16581562822115462
Global Trainning Loss: 2.2603837060928345
Global test accurancy: 0.16788012110020942
Global test_loss: 2.259942741394043
Global Precision: 0.18277517704015547
Global Recall: 0.16788012110020942
Global f1score: 0.11622237918681488
50
50
number of selected users 50
Global Trainning Accurancy: 0.16760514560763323
Global Trainning Loss: 2.2587757682800294
Global test accurancy: 0.16911278263415905
Global test_loss: 2.2582810592651366
Global Precision: 0.19701889631149602
Global Recall: 0.16911278263415905
Global f1score: 0.1200860934681011
50
50
number of selected users 50
Global Trainning Accurancy: 0.16886133550504273
Global Trainning Loss: 2.257185697555542
Global test accurancy: 0.1708396205951499
Global test_loss: 2.256633973121643
Global Precision: 0.20228890285701484
Global Recall: 0.1708396205951499
Global f1score: 0.12364068815255191
50
50
number of selected users 50
Global Trainning Accurancy: 0.17003503908654127
Global Trainning Loss: 2.2556312704086303
Global test accurancy: 0.16940869404598016
Global test_loss: 2.255015044212341
Global Precision: 0.20117150272728127
Global Recall: 0.16940869404598016
Global f1score: 0.12328796056042818
50
50
number of selected users 50
Global Trainning Accurancy: 0.171198973658281
Global Trainning Loss: 2.254111256599426
Global test accurancy: 0.17238764603635096
Global test_loss: 2.2534266090393067
Global Precision: 0.1976043408467369
Global Recall: 0.17238764603635096
Global f1score: 0.1280563122058774
50
50
number of selected users 50
Global Trainning Accurancy: 0.17259340090792064
Global Trainning Loss: 2.252640795707703
Global test accurancy: 0.1749926664322021
Global test_loss: 2.2518929958343508
Global Precision: 0.20642070674122584
Global Recall: 0.1749926664322021
Global f1score: 0.13285258500722816
50
50
number of selected users 50
Global Trainning Accurancy: 0.17399302237191586
Global Trainning Loss: 2.251203303337097
Global test accurancy: 0.17588246858485398
Global test_loss: 2.2503962564468383
Global Precision: 0.2153799061460887
Global Recall: 0.175882468584854
Global f1score: 0.13556143729790704
50
50
number of selected users 50
Global Trainning Accurancy: 0.17483141216191406
Global Trainning Loss: 2.2498331594467165
Global test accurancy: 0.17687887899069002
Global test_loss: 2.248971791267395
Global Precision: 0.21752716065065153
Global Recall: 0.17687887899069002
Global f1score: 0.13716764007938126
50
50
number of selected users 50
Global Trainning Accurancy: 0.17651055693755177
Global Trainning Loss: 2.2484731769561765
Global test accurancy: 0.17733708215000255
Global test_loss: 2.247556700706482
Global Precision: 0.2165929765996175
Global Recall: 0.17733708215000255
Global f1score: 0.13856377951181376
50
50
number of selected users 50
Global Trainning Accurancy: 0.1769518190695234
Global Trainning Loss: 2.247168836593628
Global test accurancy: 0.1791369471182555
Global test_loss: 2.2462044954299927
Global Precision: 0.2180853264267978
Global Recall: 0.1791369471182555
Global f1score: 0.14154769226166783
50
50
number of selected users 50
Global Trainning Accurancy: 0.17832330541374014
Global Trainning Loss: 2.2458670425415037
Global test accurancy: 0.1804265491292135
Global test_loss: 2.2448585844039917
Global Precision: 0.21895901469307394
Global Recall: 0.1804265491292135
Global f1score: 0.14401848290495117
50
50
number of selected users 50
Global Trainning Accurancy: 0.1796053022352505
Global Trainning Loss: 2.2446106243133546
Global test accurancy: 0.1832537139429566
Global test_loss: 2.243560400009155
Global Precision: 0.22665101656316156
Global Recall: 0.1832537139429566
Global f1score: 0.14819327133793522
50
50
number of selected users 50
Global Trainning Accurancy: 0.18126119780946437
Global Trainning Loss: 2.2433745002746583
Global test accurancy: 0.18489917277256052
Global test_loss: 2.2422694969177246
Global Precision: 0.2285913872502233
Global Recall: 0.18489917277256052
Global f1score: 0.15075064357652213
50
50
number of selected users 50
Global Trainning Accurancy: 0.18357212723528965
Global Trainning Loss: 2.24216344833374
Global test accurancy: 0.1855421099685916
Global test_loss: 2.241015009880066
Global Precision: 0.228715436574592
Global Recall: 0.1855421099685916
Global f1score: 0.15204912488722291
50
50
number of selected users 50
Global Trainning Accurancy: 0.18473704058590143
Global Trainning Loss: 2.2409864330291747
Global test accurancy: 0.18741566786190755
Global test_loss: 2.239804401397705
Global Precision: 0.23458359124067482
Global Recall: 0.18741566786190755
Global f1score: 0.15582737895067733
50
50
number of selected users 50
Global Trainning Accurancy: 0.185157587886343
Global Trainning Loss: 2.2398362064361574
Global test accurancy: 0.1895452469521791
Global test_loss: 2.2385985803604127
Global Precision: 0.2373277623461117
Global Recall: 0.1895452469521791
Global f1score: 0.15873728643880414
50
50
number of selected users 50
Global Trainning Accurancy: 0.1859402150676607
Global Trainning Loss: 2.238717041015625
Global test accurancy: 0.18917104958232772
Global test_loss: 2.237430295944214
Global Precision: 0.23374606707362855
Global Recall: 0.18917104958232772
Global f1score: 0.15846191309005586
50
50
number of selected users 50
Global Trainning Accurancy: 0.1870402775495208
Global Trainning Loss: 2.2376041555404664
Global test accurancy: 0.18959322504333326
Global test_loss: 2.23626859664917
Global Precision: 0.22975922160701528
Global Recall: 0.18959322504333326
Global f1score: 0.15902906650409548
50
50
number of selected users 50
Global Trainning Accurancy: 0.18740409862542534
Global Trainning Loss: 2.236530933380127
Global test accurancy: 0.19042201095705422
Global test_loss: 2.2351466608047486
Global Precision: 0.22631532971450546
Global Recall: 0.19042201095705422
Global f1score: 0.16057336722406923
50
50
number of selected users 50
Global Trainning Accurancy: 0.1883295127469726
Global Trainning Loss: 2.2354590988159178
Global test accurancy: 0.19224071783539512
Global test_loss: 2.2340347576141357
Global Precision: 0.2329873877544314
Global Recall: 0.19224071783539512
Global f1score: 0.16386476302033717
50
50
number of selected users 50
Global Trainning Accurancy: 0.18960231398262087
Global Trainning Loss: 2.2344086885452272
Global test accurancy: 0.19352675870363895
Global test_loss: 2.232932071685791
Global Precision: 0.23019539738294365
Global Recall: 0.19352675870363895
Global f1score: 0.16572848573608465
50
50
number of selected users 50
Global Trainning Accurancy: 0.1903698523245498
Global Trainning Loss: 2.2333777809143065
Global test accurancy: 0.19423518914195237
Global test_loss: 2.231852469444275
Global Precision: 0.23459715834277975
Global Recall: 0.19423518914195237
Global f1score: 0.16687684898553676
50
50
number of selected users 50
Global Trainning Accurancy: 0.1904600189610981
Global Trainning Loss: 2.2323757219314575
Global test accurancy: 0.19495734850017443
Global test_loss: 2.2308087873458864
Global Precision: 0.24326437023325914
Global Recall: 0.19495734850017443
Global f1score: 0.1677819007812766
50
50
number of selected users 50
Global Trainning Accurancy: 0.19110192177868046
Global Trainning Loss: 2.2313754463195803
Global test accurancy: 0.1971355794120299
Global test_loss: 2.2297634267807007
Global Precision: 0.2480342212057267
Global Recall: 0.1971355794120299
Global f1score: 0.17081330759459493
50
50
number of selected users 50
Global Trainning Accurancy: 0.19184553749615602
Global Trainning Loss: 2.230395584106445
Global test accurancy: 0.19782137028384386
Global test_loss: 2.2287400913238526
Global Precision: 0.23608112988856253
Global Recall: 0.19782137028384386
Global f1score: 0.17147330760915258
50
50
number of selected users 50
Global Trainning Accurancy: 0.1924852126363542
Global Trainning Loss: 2.2294490194320677
Global test accurancy: 0.1983043809320895
Global test_loss: 2.2277630758285523
Global Precision: 0.23912316235653558
Global Recall: 0.1983043809320895
Global f1score: 0.17190444453734746
50
50
number of selected users 50
Global Trainning Accurancy: 0.19317390186447322
Global Trainning Loss: 2.2284889316558836
Global test accurancy: 0.19935192567408572
Global test_loss: 2.226760816574097
Global Precision: 0.25536143768648045
Global Recall: 0.19935192567408572
Global f1score: 0.17360190652078186
50
50
number of selected users 50
Global Trainning Accurancy: 0.1941637558202484
Global Trainning Loss: 2.227587580680847
Global test accurancy: 0.20048595117721907
Global test_loss: 2.2258093404769896
Global Precision: 0.26392013148547305
Global Recall: 0.20048595117721907
Global f1score: 0.17628701319929427
50
50
number of selected users 50
Global Trainning Accurancy: 0.19479309031938516
Global Trainning Loss: 2.2266894149780274
Global test accurancy: 0.20213062744449817
Global test_loss: 2.2248874950408934
Global Precision: 0.269870694148809
Global Recall: 0.20213062744449817
Global f1score: 0.17876018126738066
50
50
number of selected users 50
Global Trainning Accurancy: 0.1953001021887351
Global Trainning Loss: 2.2257947969436644
Global test accurancy: 0.20338011842813483
Global test_loss: 2.223988242149353
Global Precision: 0.2683197305558695
Global Recall: 0.20338011842813483
Global f1score: 0.18015183527627618
50
50
number of selected users 50
Global Trainning Accurancy: 0.19640674523643156
Global Trainning Loss: 2.224890112876892
Global test accurancy: 0.2044591552580672
Global test_loss: 2.223104953765869
Global Precision: 0.2722622672170666
Global Recall: 0.2044591552580672
Global f1score: 0.18166239104295834
50
50
number of selected users 50
Global Trainning Accurancy: 0.19699958612221494
Global Trainning Loss: 2.2239646434783937
Global test accurancy: 0.20578825412190194
Global test_loss: 2.2222056102752688
Global Precision: 0.27515544250074075
Global Recall: 0.20578825412190194
Global f1score: 0.18374223306765738
50
50
number of selected users 50
Global Trainning Accurancy: 0.1977094375119862
Global Trainning Loss: 2.223077754974365
Global test accurancy: 0.2062272704668166
Global test_loss: 2.2213331174850466
Global Precision: 0.2760256870817413
Global Recall: 0.2062272704668166
Global f1score: 0.1848761021494862
50
50
number of selected users 50
Global Trainning Accurancy: 0.1982698548222754
Global Trainning Loss: 2.2221917724609375
Global test accurancy: 0.2059166467347521
Global test_loss: 2.2204546880722047
Global Precision: 0.28016651231923373
Global Recall: 0.2059166467347521
Global f1score: 0.18482608376162077
50
50
number of selected users 50
Global Trainning Accurancy: 0.1997196408443504
Global Trainning Loss: 2.221246585845947
Global test accurancy: 0.20637495471870668
Global test_loss: 2.2195230102539063
Global Precision: 0.2768520198818727
Global Recall: 0.20637495471870668
Global f1score: 0.1863232860599249
50
50
number of selected users 50
Global Trainning Accurancy: 0.2002087239020286
Global Trainning Loss: 2.2203371810913084
Global test accurancy: 0.20606798617551417
Global test_loss: 2.2186307096481324
Global Precision: 0.27714724211771846
Global Recall: 0.20606798617551417
Global f1score: 0.18628192618953054
50
50
number of selected users 50
Global Trainning Accurancy: 0.20096715642002067
Global Trainning Loss: 2.2194488573074342
Global test accurancy: 0.2067452870823084
Global test_loss: 2.2177631759643557
Global Precision: 0.2795182845370423
Global Recall: 0.2067452870823084
Global f1score: 0.1873272010037027
50
50
number of selected users 50
Global Trainning Accurancy: 0.2010338536121947
Global Trainning Loss: 2.2185168504714965
Global test accurancy: 0.20736271412138788
Global test_loss: 2.216843810081482
Global Precision: 0.29150022859832064
Global Recall: 0.20736271412138788
Global f1score: 0.1892139223701664
50
50
number of selected users 50
Global Trainning Accurancy: 0.20178348454782405
Global Trainning Loss: 2.217605013847351
Global test accurancy: 0.20728893464269912
Global test_loss: 2.2159507274627686
Global Precision: 0.28732887248443234
Global Recall: 0.20728893464269912
Global f1score: 0.18936793753973422
50
50
number of selected users 50
Global Trainning Accurancy: 0.20271483002850227
Global Trainning Loss: 2.216675362586975
Global test accurancy: 0.20869058092709059
Global test_loss: 2.2150541973114013
Global Precision: 0.2875738442097386
Global Recall: 0.20869058092709059
Global f1score: 0.19145791469171258
50
50
number of selected users 50
Global Trainning Accurancy: 0.20315791131063335
Global Trainning Loss: 2.215795397758484
Global test accurancy: 0.20991319670664554
Global test_loss: 2.214210019111633
Global Precision: 0.29258677903401475
Global Recall: 0.20991319670664554
Global f1score: 0.1934341626480533
50
50
number of selected users 50
Global Trainning Accurancy: 0.2038053638983187
Global Trainning Loss: 2.21491334438324
Global test accurancy: 0.2090868015439105
Global test_loss: 2.2133697271347046
Global Precision: 0.2918244358226081
Global Recall: 0.2090868015439105
Global f1score: 0.1927003464982394
50
50
number of selected users 50
Global Trainning Accurancy: 0.20474620287102346
Global Trainning Loss: 2.214072709083557
Global test accurancy: 0.20898275755497744
Global test_loss: 2.212584795951843
Global Precision: 0.2905929379965384
Global Recall: 0.20898275755497744
Global f1score: 0.19359010348023115
50
50
number of selected users 50
Global Trainning Accurancy: 0.2060764511153626
Global Trainning Loss: 2.2131827116012572
Global test accurancy: 0.20844657633773445
Global test_loss: 2.2117531871795655
Global Precision: 0.29246514928493506
Global Recall: 0.20844657633773445
Global f1score: 0.1933146869679772
50
50
number of selected users 50
Global Trainning Accurancy: 0.20668438536804978
Global Trainning Loss: 2.2123244285583494
Global test accurancy: 0.2101950335995498
Global test_loss: 2.2109485149383543
Global Precision: 0.29443626229992625
Global Recall: 0.2101950335995498
Global f1score: 0.1958155235660502
50
50
number of selected users 50
Global Trainning Accurancy: 0.20684559814654666
Global Trainning Loss: 2.2114638233184816
Global test accurancy: 0.21262213283621392
Global test_loss: 2.2101513051986696
Global Precision: 0.3019684804598886
Global Recall: 0.21262213283621392
Global f1score: 0.19913998799847724
50
50
number of selected users 50
Global Trainning Accurancy: 0.20769064681508106
Global Trainning Loss: 2.210599455833435
Global test accurancy: 0.21305274338844102
Global test_loss: 2.2093627882003783
Global Precision: 0.2998147743885079
Global Recall: 0.21305274338844102
Global f1score: 0.20044153718156205
50
50
number of selected users 50
Global Trainning Accurancy: 0.20889741519859076
Global Trainning Loss: 2.2098014116287232
Global test accurancy: 0.214826125665349
Global test_loss: 2.2086519765853883
Global Precision: 0.30418713795680624
Global Recall: 0.214826125665349
Global f1score: 0.20334077143258736
50
50
number of selected users 50
Global Trainning Accurancy: 0.20976438455381702
Global Trainning Loss: 2.208924746513367
Global test accurancy: 0.21586623399211122
Global test_loss: 2.2078628396987914
Global Precision: 0.3153788676624648
Global Recall: 0.21586623399211122
Global f1score: 0.20568276871545033
50
50
number of selected users 50
Global Trainning Accurancy: 0.21022840918139174
Global Trainning Loss: 2.208120822906494
Global test accurancy: 0.2168977930162333
Global test_loss: 2.207154726982117
Global Precision: 0.3134465458172575
Global Recall: 0.2168977930162333
Global f1score: 0.20711404552197318
50
50
number of selected users 50
Global Trainning Accurancy: 0.21006989502664478
Global Trainning Loss: 2.2072817277908325
Global test accurancy: 0.21734728362340086
Global test_loss: 2.206413245201111
Global Precision: 0.31987712280327535
Global Recall: 0.21734728362340086
Global f1score: 0.2086751951309188
50
50
number of selected users 50
Global Trainning Accurancy: 0.21102543353407624
Global Trainning Loss: 2.206427226066589
Global test accurancy: 0.21803719284203713
Global test_loss: 2.2056721782684328
Global Precision: 0.31975748988282465
Global Recall: 0.21803719284203713
Global f1score: 0.2107363017761382
50
50
number of selected users 50
Global Trainning Accurancy: 0.21146488280959755
Global Trainning Loss: 2.205583305358887
Global test accurancy: 0.2190050409799989
Global test_loss: 2.2049418783187864
Global Precision: 0.31546652283473253
Global Recall: 0.2190050409799989
Global f1score: 0.21299094850717332
50
50
number of selected users 50
Global Trainning Accurancy: 0.21219975188782392
Global Trainning Loss: 2.2047284841537476
Global test accurancy: 0.21913909575273985
Global test_loss: 2.204217643737793
Global Precision: 0.3174165555596879
Global Recall: 0.21913909575273985
Global f1score: 0.21379534051754698
50
50
number of selected users 50
Global Trainning Accurancy: 0.21347070107840851
Global Trainning Loss: 2.203887801170349
Global test accurancy: 0.2185925996306422
Global test_loss: 2.2034956979751588
Global Precision: 0.3163292556741349
Global Recall: 0.2185925996306422
Global f1score: 0.21367060655007955
50
50
number of selected users 50
Global Trainning Accurancy: 0.2143658957502497
Global Trainning Loss: 2.20301034450531
Global test accurancy: 0.21848723038372544
Global test_loss: 2.2027510929107668
Global Precision: 0.3170155451305449
Global Recall: 0.21848723038372544
Global f1score: 0.21452546866797692
50
50
number of selected users 50
Global Trainning Accurancy: 0.21508285096236207
Global Trainning Loss: 2.2021284103393555
Global test accurancy: 0.2190707453439752
Global test_loss: 2.201984167098999
Global Precision: 0.31542842664031034
Global Recall: 0.2190707453439752
Global f1score: 0.21592693417501937
50
50
number of selected users 50
Global Trainning Accurancy: 0.21596168953849623
Global Trainning Loss: 2.2012746810913084
Global test accurancy: 0.22006509894069032
Global test_loss: 2.201258883476257
Global Precision: 0.313673111532345
Global Recall: 0.22006509894069032
Global f1score: 0.21736748439773182
50
50
number of selected users 50
Global Trainning Accurancy: 0.2169244781757887
Global Trainning Loss: 2.200361819267273
Global test accurancy: 0.22128358216488248
Global test_loss: 2.200471420288086
Global Precision: 0.31562953218439216
Global Recall: 0.22128358216488248
Global f1score: 0.21953265824374013
50
50
number of selected users 50
Global Trainning Accurancy: 0.21827500503451863
Global Trainning Loss: 2.1994580078125
Global test accurancy: 0.22085013353307667
Global test_loss: 2.1997029685974123
Global Precision: 0.3178847186502212
Global Recall: 0.22085013353307667
Global f1score: 0.2200606956535938
50
50
number of selected users 50
Global Trainning Accurancy: 0.21983444546186637
Global Trainning Loss: 2.1985441303253173
Global test accurancy: 0.22197968625309553
Global test_loss: 2.1989463472366335
Global Precision: 0.3212959851948278
Global Recall: 0.22197968625309553
Global f1score: 0.22303898486824278
50
50
number of selected users 50
Global Trainning Accurancy: 0.2203131483677574
Global Trainning Loss: 2.197714376449585
Global test accurancy: 0.22394940036593194
Global test_loss: 2.1982751846313477
Global Precision: 0.32142197104794523
Global Recall: 0.22394940036593194
Global f1score: 0.22525065335322042
50
50
number of selected users 50
Global Trainning Accurancy: 0.22196144372682272
Global Trainning Loss: 2.1968786191940306
Global test accurancy: 0.22516995956748495
Global test_loss: 2.197579236030579
Global Precision: 0.32022168781471605
Global Recall: 0.22516995956748495
Global f1score: 0.22690795698157054
50
50
number of selected users 50
Global Trainning Accurancy: 0.223190693830923
Global Trainning Loss: 2.1960126686096193
Global test accurancy: 0.22624603022672152
Global test_loss: 2.1968750810623168
Global Precision: 0.3201478528506546
Global Recall: 0.22624603022672152
Global f1score: 0.22842578691758325
50
50
number of selected users 50
Global Trainning Accurancy: 0.22350192191830157
Global Trainning Loss: 2.1951531600952148
Global test accurancy: 0.2263220572938013
Global test_loss: 2.1961998033523558
Global Precision: 0.32002087340003194
Global Recall: 0.2263220572938013
Global f1score: 0.2289592127636568
50
50
number of selected users 50
Global Trainning Accurancy: 0.22445065348970708
Global Trainning Loss: 2.194264097213745
Global test accurancy: 0.22597499855308262
Global test_loss: 2.1954969644546507
Global Precision: 0.31938334422095715
Global Recall: 0.22597499855308262
Global f1score: 0.22959224135249015
50
50
number of selected users 50
Global Trainning Accurancy: 0.22563553095880323
Global Trainning Loss: 2.1934868907928466
Global test accurancy: 0.22649733493700186
Global test_loss: 2.1949211502075197
Global Precision: 0.3202127934485616
Global Recall: 0.22649733493700186
Global f1score: 0.23108533690913444
50
50
number of selected users 50
Global Trainning Accurancy: 0.22717452929925258
Global Trainning Loss: 2.1926355934143067
Global test accurancy: 0.22646238680013703
Global test_loss: 2.1942826318740845
Global Precision: 0.3237618381031482
Global Recall: 0.22646238680013703
Global f1score: 0.2325846907622577
50
50
number of selected users 50
Global Trainning Accurancy: 0.22759570251223193
Global Trainning Loss: 2.1918101787567137
Global test accurancy: 0.2267431029321832
Global test_loss: 2.193674898147583
Global Precision: 0.32377244045957093
Global Recall: 0.2267431029321832
Global f1score: 0.23338502484788584
50
50
number of selected users 50
Global Trainning Accurancy: 0.22836625273274774
Global Trainning Loss: 2.190950870513916
Global test accurancy: 0.2273601878609084
Global test_loss: 2.1930509185791016
Global Precision: 0.32875290725371153
Global Recall: 0.2273601878609084
Global f1score: 0.23487063847052733
50
50
number of selected users 50
Global Trainning Accurancy: 0.22903287538979705
Global Trainning Loss: 2.190108742713928
Global test accurancy: 0.22876529031629828
Global test_loss: 2.1924559116363525
Global Precision: 0.3305304689757277
Global Recall: 0.22876529031629828
Global f1score: 0.23734714248862052
50
50
number of selected users 50
Global Trainning Accurancy: 0.2299133536052524
Global Trainning Loss: 2.1892883014678954
Global test accurancy: 0.2278255293529988
Global test_loss: 2.1918997049331663
Global Precision: 0.32925773645038203
Global Recall: 0.2278255293529988
Global f1score: 0.23675540164771766
50
50
number of selected users 50
Global Trainning Accurancy: 0.22984961950651367
Global Trainning Loss: 2.188422231674194
Global test accurancy: 0.22787057954078702
Global test_loss: 2.1912996053695677
Global Precision: 0.3301648412859253
Global Recall: 0.22787057954078702
Global f1score: 0.2367735316700644
50
50
number of selected users 50
Global Trainning Accurancy: 0.23070066075102857
Global Trainning Loss: 2.187512378692627
Global test accurancy: 0.2280850534773071
Global test_loss: 2.190675501823425
Global Precision: 0.3298133367050194
Global Recall: 0.2280850534773071
Global f1score: 0.2368123084495469
50
50
number of selected users 50
Global Trainning Accurancy: 0.23128915186586235
Global Trainning Loss: 2.1867176294326782
Global test accurancy: 0.22921764933493946
Global test_loss: 2.1901450777053832
Global Precision: 0.3308195776096528
Global Recall: 0.22921764933493946
Global f1score: 0.2380136935312641
50
50
number of selected users 50
Global Trainning Accurancy: 0.23297513878465806
Global Trainning Loss: 2.1859367179870604
Global test accurancy: 0.23037437487859286
Global test_loss: 2.18963662147522
Global Precision: 0.33083088002466926
Global Recall: 0.23037437487859286
Global f1score: 0.23931716923130597
50
50
number of selected users 50
Global Trainning Accurancy: 0.2334547647874357
Global Trainning Loss: 2.1850664138793947
Global test accurancy: 0.23254809117465092
Global test_loss: 2.1890413093566896
Global Precision: 0.3337417107458363
Global Recall: 0.23254809117465092
Global f1score: 0.24156611681214263
50
50
number of selected users 50
Global Trainning Accurancy: 0.23464761789467486
Global Trainning Loss: 2.184339017868042
Global test accurancy: 0.23236918172245405
Global test_loss: 2.188604645729065
Global Precision: 0.3337235468941152
Global Recall: 0.23236918172245405
Global f1score: 0.24193365512068168
50
50
number of selected users 50
Global Trainning Accurancy: 0.2347736763823628
Global Trainning Loss: 2.183614330291748
Global test accurancy: 0.2336496628784202
Global test_loss: 2.1881839513778685
Global Precision: 0.33956801945199144
Global Recall: 0.2336496628784202
Global f1score: 0.24390897548275048
50
50
number of selected users 50
Global Trainning Accurancy: 0.23613343976714515
Global Trainning Loss: 2.182831463813782
Global test accurancy: 0.23400347661895482
Global test_loss: 2.1877071475982666
Global Precision: 0.340897715432852
Global Recall: 0.23400347661895482
Global f1score: 0.24537736282547587
50
50
number of selected users 50
Global Trainning Accurancy: 0.23657506376141024
Global Trainning Loss: 2.1820074272155763
Global test accurancy: 0.23368799935394233
Global test_loss: 2.18718017578125
Global Precision: 0.33994612832170895
Global Recall: 0.23368799935394233
Global f1score: 0.24526409024792856
50
50
number of selected users 50
Global Trainning Accurancy: 0.23697896414952063
Global Trainning Loss: 2.181301975250244
Global test accurancy: 0.234213103889714
Global test_loss: 2.1868008947372437
Global Precision: 0.34095308181369455
Global Recall: 0.234213103889714
Global f1score: 0.24630306410026662
50
50
number of selected users 50
Global Trainning Accurancy: 0.23786733620898223
Global Trainning Loss: 2.1805539178848266
Global test accurancy: 0.2350372382691284
Global test_loss: 2.1863999223709105
Global Precision: 0.3421047706543761
Global Recall: 0.2350372382691284
Global f1score: 0.24774284131810037
50
50
number of selected users 50
Global Trainning Accurancy: 0.23885940094367206
Global Trainning Loss: 2.179825811386108
Global test accurancy: 0.23426620820667265
Global test_loss: 2.186017994880676
Global Precision: 0.3410857391245106
Global Recall: 0.23426620820667265
Global f1score: 0.24788588880381396
50
50
number of selected users 50
Global Trainning Accurancy: 0.2393363126142537
Global Trainning Loss: 2.1790421533584596
Global test accurancy: 0.23576509730014006
Global test_loss: 2.1855824613571166
Global Precision: 0.3400099791575484
Global Recall: 0.23576509730014006
Global f1score: 0.2495999508154325
50
50
number of selected users 50
Global Trainning Accurancy: 0.24050509711037268
Global Trainning Loss: 2.178305196762085
Global test accurancy: 0.2361315581873132
Global test_loss: 2.1852018356323244
Global Precision: 0.33831909803135823
Global Recall: 0.2361315581873132
Global f1score: 0.25034923610024873
50
50
number of selected users 50
Global Trainning Accurancy: 0.24159613260026233
Global Trainning Loss: 2.1775515842437745
Global test accurancy: 0.23783212336879836
Global test_loss: 2.184825463294983
Global Precision: 0.33920457125902304
Global Recall: 0.23783212336879836
Global f1score: 0.2527456576985898
50
50
number of selected users 50
Global Trainning Accurancy: 0.24117174161725471
Global Trainning Loss: 2.176843218803406
Global test accurancy: 0.23814917922481454
Global test_loss: 2.184496808052063
Global Precision: 0.3401575847377224
Global Recall: 0.23814917922481454
Global f1score: 0.2532242030072341
50
50
number of selected users 50
Global Trainning Accurancy: 0.24260804776257705
Global Trainning Loss: 2.176082663536072
Global test accurancy: 0.2377550941359425
Global test_loss: 2.1841015911102293
Global Precision: 0.33939916061302217
Global Recall: 0.2377550941359425
Global f1score: 0.25345904639280137
50
50
number of selected users 50
Global Trainning Accurancy: 0.24266853139781644
Global Trainning Loss: 2.175323691368103
Global test accurancy: 0.23924339555941196
Global test_loss: 2.1836959075927735
Global Precision: 0.3406320305211738
Global Recall: 0.23924339555941196
Global f1score: 0.25511929986876847
50
50
number of selected users 50
Global Trainning Accurancy: 0.24251009333409967
Global Trainning Loss: 2.174577054977417
Global test accurancy: 0.24036167305651254
Global test_loss: 2.1833375835418702
Global Precision: 0.3425265695062968
Global Recall: 0.24036167305651254
Global f1score: 0.25657479798835614
50
50
number of selected users 50
Global Trainning Accurancy: 0.24256369419786808
Global Trainning Loss: 2.173872036933899
Global test accurancy: 0.24027070353123575
Global test_loss: 2.183019938468933
Global Precision: 0.34150504631199863
Global Recall: 0.24027070353123575
Global f1score: 0.256539669528464
50
50
number of selected users 50
Global Trainning Accurancy: 0.24274270941468695
Global Trainning Loss: 2.1730730819702146
Global test accurancy: 0.2401431540830032
Global test_loss: 2.1825922775268554
Global Precision: 0.3410528662757909
Global Recall: 0.2401431540830032
Global f1score: 0.25651209775473816
50
50
number of selected users 50
Global Trainning Accurancy: 0.24323940187800522
Global Trainning Loss: 2.1723032188415528
Global test accurancy: 0.2380667259835754
Global test_loss: 2.1822197437286377
Global Precision: 0.3404561608800061
Global Recall: 0.2380667259835754
Global f1score: 0.25471179134981686
50
50
number of selected users 50
Global Trainning Accurancy: 0.24422895731271932
Global Trainning Loss: 2.171509556770325
Global test accurancy: 0.23870112297933238
Global test_loss: 2.1818293046951296
Global Precision: 0.34123276251622026
Global Recall: 0.23870112297933238
Global f1score: 0.25552546815975563
50
50
number of selected users 50
Global Trainning Accurancy: 0.24484008832474188
Global Trainning Loss: 2.170825176239014
Global test accurancy: 0.23724120168520263
Global test_loss: 2.181534504890442
Global Precision: 0.33874941124226604
Global Recall: 0.23724120168520263
Global f1score: 0.2539630122471944
50
50
number of selected users 50
Global Trainning Accurancy: 0.2460274710688155
Global Trainning Loss: 2.170040555000305
Global test accurancy: 0.23803286336028462
Global test_loss: 2.18113242149353
Global Precision: 0.34193652234685074
Global Recall: 0.23803286336028462
Global f1score: 0.25514105272936755
50
50
number of selected users 50
Global Trainning Accurancy: 0.24613552691356028
Global Trainning Loss: 2.1692782735824583
Global test accurancy: 0.2375920401613541
Global test_loss: 2.1807528829574583
Global Precision: 0.3396583917442021
Global Recall: 0.2375920401613541
Global f1score: 0.25466311245615003
50
50
number of selected users 50
Global Trainning Accurancy: 0.24597668068403006
Global Trainning Loss: 2.168627619743347
Global test accurancy: 0.23828153124807328
Global test_loss: 2.180508518218994
Global Precision: 0.3435502678857683
Global Recall: 0.23828153124807328
Global f1score: 0.25525360552881
50
50
number of selected users 50
Global Trainning Accurancy: 0.24627751455130004
Global Trainning Loss: 2.16802031993866
Global test accurancy: 0.23838292652709242
Global test_loss: 2.180292258262634
Global Precision: 0.3391961090494836
Global Recall: 0.23838292652709242
Global f1score: 0.25556057222679585
50
50
number of selected users 50
Global Trainning Accurancy: 0.24625026420501245
Global Trainning Loss: 2.1673058080673218
Global test accurancy: 0.23894991901079687
Global test_loss: 2.1799714946746827
Global Precision: 0.3393251665060306
Global Recall: 0.23894991901079687
Global f1score: 0.2561680910877004
50
50
number of selected users 50
Global Trainning Accurancy: 0.24684284126928702
Global Trainning Loss: 2.1666782188415525
Global test accurancy: 0.2395533818362787
Global test_loss: 2.1797515964508056
Global Precision: 0.34011550773083965
Global Recall: 0.2395533818362787
Global f1score: 0.2567129701260539
50
50
number of selected users 50
Global Trainning Accurancy: 0.2476413368391527
Global Trainning Loss: 2.166048216819763
Global test accurancy: 0.23950697320303196
Global test_loss: 2.1795445489883423
Global Precision: 0.3376279242879188
Global Recall: 0.23950697320303196
Global f1score: 0.2573895188343128
50
50
number of selected users 50
Global Trainning Accurancy: 0.24815269728924297
Global Trainning Loss: 2.1652977895736694
Global test accurancy: 0.23772937131910188
Global test_loss: 2.179198155403137
Global Precision: 0.33615220101700716
Global Recall: 0.23772937131910188
Global f1score: 0.25569596873948436
50
50
number of selected users 50
Global Trainning Accurancy: 0.2481825008810049
Global Trainning Loss: 2.164580841064453
Global test accurancy: 0.2387453116893582
Global test_loss: 2.178887119293213
Global Precision: 0.33766363785607745
Global Recall: 0.2387453116893582
Global f1score: 0.257188730204162
50
50
number of selected users 50
Global Trainning Accurancy: 0.24814990644373053
Global Trainning Loss: 2.1638427686691286
Global test accurancy: 0.2394150776945976
Global test_loss: 2.1785458946228027
Global Precision: 0.33670988875906516
Global Recall: 0.2394150776945976
Global f1score: 0.25756841062122626
50
50
number of selected users 50
Global Trainning Accurancy: 0.24807262967415442
Global Trainning Loss: 2.1632782506942747
Global test accurancy: 0.2390278155357814
Global test_loss: 2.1783990812301637
Global Precision: 0.33544512158348433
Global Recall: 0.2390278155357814
Global f1score: 0.25705544489947335
50
50
number of selected users 50
Global Trainning Accurancy: 0.24817896754893534
Global Trainning Loss: 2.16266037940979
Global test accurancy: 0.2395905139767891
Global test_loss: 2.1781859970092774
Global Precision: 0.3356624801655149
Global Recall: 0.2395905139767891
Global f1score: 0.2579365065840451
50
50
number of selected users 50
Global Trainning Accurancy: 0.24872880814573747
Global Trainning Loss: 2.162117056846619
Global test accurancy: 0.23997988542819787
Global test_loss: 2.1780587673187255
Global Precision: 0.33809818220526466
Global Recall: 0.23997988542819787
Global f1score: 0.2592216734640108
50
50
number of selected users 50
Global Trainning Accurancy: 0.24936337877673104
Global Trainning Loss: 2.1614466381072996
Global test accurancy: 0.23979063146620586
Global test_loss: 2.177835216522217
Global Precision: 0.33863016513351646
Global Recall: 0.23979063146620586
Global f1score: 0.259472588348449
50
50
number of selected users 50
Global Trainning Accurancy: 0.24987998759942787
Global Trainning Loss: 2.160845618247986
Global test accurancy: 0.24098024950964636
Global test_loss: 2.1776573944091795
Global Precision: 0.3400546379308397
Global Recall: 0.24098024950964636
Global f1score: 0.26077961110552883
50
50
number of selected users 50
Global Trainning Accurancy: 0.25033568725592226
Global Trainning Loss: 2.160267629623413
Global test accurancy: 0.24110321284891786
Global test_loss: 2.177504858970642
Global Precision: 0.33972047834772895
Global Recall: 0.24110321284891786
Global f1score: 0.26094425897344686
50
50
number of selected users 50
Global Trainning Accurancy: 0.2513388087189358
Global Trainning Loss: 2.1596747922897337
Global test accurancy: 0.24097580334483779
Global test_loss: 2.1773690748214722
Global Precision: 0.3393412280585729
Global Recall: 0.24097580334483779
Global f1score: 0.260986805439925
50
50
number of selected users 50
Global Trainning Accurancy: 0.2508072616949604
Global Trainning Loss: 2.158980994224548
Global test accurancy: 0.24164752664195407
Global test_loss: 2.177128314971924
Global Precision: 0.3406642634263391
Global Recall: 0.24164752664195407
Global f1score: 0.2619940651216804
50
50
number of selected users 50
Global Trainning Accurancy: 0.2521614057959984
Global Trainning Loss: 2.158420624732971
Global test accurancy: 0.24165708260002314
Global test_loss: 2.17701566696167
Global Precision: 0.34094806811837647
Global Recall: 0.24165708260002314
Global f1score: 0.26219395707329013
50
50
number of selected users 50
Global Trainning Accurancy: 0.25230649923645204
Global Trainning Loss: 2.1576165628433226
Global test accurancy: 0.24145034362285056
Global test_loss: 2.1766782569885255
Global Precision: 0.339737738606264
Global Recall: 0.24145034362285056
Global f1score: 0.2613961352879958
50
50
number of selected users 50
Global Trainning Accurancy: 0.25217380596876726
Global Trainning Loss: 2.1572009420394895
Global test accurancy: 0.2413915164850552
Global test_loss: 2.1767167615890504
Global Precision: 0.33810139488029034
Global Recall: 0.2413915164850552
Global f1score: 0.26137832152459345
50
50
number of selected users 50
Global Trainning Accurancy: 0.2528875781596189
Global Trainning Loss: 2.156320357322693
Global test accurancy: 0.24119226582952347
Global test_loss: 2.176280670166016
Global Precision: 0.338464956146299
Global Recall: 0.24119226582952347
Global f1score: 0.2610565939192088
50
50
number of selected users 50
Global Trainning Accurancy: 0.2535530925560821
Global Trainning Loss: 2.155639147758484
Global test accurancy: 0.24114354835488888
Global test_loss: 2.1760878419876097
Global Precision: 0.3372028188088996
Global Recall: 0.24114354835488888
Global f1score: 0.2613583854463696
50
50
number of selected users 50
Global Trainning Accurancy: 0.2538772228330902
Global Trainning Loss: 2.1550139808654785
Global test accurancy: 0.2414194174334347
Global test_loss: 2.1759294986724855
Global Precision: 0.3339490506705321
Global Recall: 0.2414194174334347
Global f1score: 0.2610381356969975
50
50
number of selected users 50
Global Trainning Accurancy: 0.25428000774391757
Global Trainning Loss: 2.1545179271698
Global test accurancy: 0.2414548219641947
Global test_loss: 2.175891842842102
Global Precision: 0.3333476052620036
Global Recall: 0.2414548219641947
Global f1score: 0.2613645059320632
50
50
number of selected users 50
Global Trainning Accurancy: 0.2545630646748087
Global Trainning Loss: 2.153623456954956
Global test accurancy: 0.24157077353462073
Global test_loss: 2.175480694770813
Global Precision: 0.3338175581588479
Global Recall: 0.24157077353462073
Global f1score: 0.2613089959996483
50
50
number of selected users 50
Global Trainning Accurancy: 0.2551255370604936
Global Trainning Loss: 2.1530683755874636
Global test accurancy: 0.24113383872779476
Global test_loss: 2.1754457998275756
Global Precision: 0.33446162293678194
Global Recall: 0.24113383872779476
Global f1score: 0.26118913553598444
50
50
number of selected users 50
Global Trainning Accurancy: 0.25496308339041435
Global Trainning Loss: 2.15255117893219
Global test accurancy: 0.2416608202317174
Global test_loss: 2.175430235862732
Global Precision: 0.33358970378162156
Global Recall: 0.2416608202317174
Global f1score: 0.26181332464426504
50
50
number of selected users 50
Global Trainning Accurancy: 0.2553103002145272
Global Trainning Loss: 2.151733627319336
Global test accurancy: 0.24227496997725495
Global test_loss: 2.1751323890686036
Global Precision: 0.33591888983462825
Global Recall: 0.24227496997725495
Global f1score: 0.26230232860059977
50
50
number of selected users 50
Global Trainning Accurancy: 0.25554273381518444
Global Trainning Loss: 2.1511542558670045
Global test accurancy: 0.24161229716470023
Global test_loss: 2.175064973831177
Global Precision: 0.33506155868279053
Global Recall: 0.24161229716470023
Global f1score: 0.26194263228513837
50
50
number of selected users 50
Global Trainning Accurancy: 0.2553446093415211
Global Trainning Loss: 2.150733790397644
Global test accurancy: 0.24298742966211953
Global test_loss: 2.1751784467697144
Global Precision: 0.3361377607998522
Global Recall: 0.24298742966211953
Global f1score: 0.26321931489323874
50
50
number of selected users 50
Global Trainning Accurancy: 0.2563991242566069
Global Trainning Loss: 2.149792113304138
Global test accurancy: 0.24152350011072155
Global test_loss: 2.1748207998275757
Global Precision: 0.3360982524243668
Global Recall: 0.24152350011072155
Global f1score: 0.26198482455579974
50
50
number of selected users 50
Global Trainning Accurancy: 0.2561824146189279
Global Trainning Loss: 2.1491612243652343
Global test accurancy: 0.23964481727041992
Global test_loss: 2.1747831010818484
Global Precision: 0.33596168785952285
Global Recall: 0.23964481727041992
Global f1score: 0.2607676103977031
50
50
number of selected users 50
Global Trainning Accurancy: 0.256763204482002
Global Trainning Loss: 2.148607316017151
Global test accurancy: 0.23972619957464897
Global test_loss: 2.1748562097549438
Global Precision: 0.33612459879157114
Global Recall: 0.23972619957464897
Global f1score: 0.26097553850441585
50
50
number of selected users 50
Global Trainning Accurancy: 0.25687036995378165
Global Trainning Loss: 2.1481351137161253
Global test accurancy: 0.2391040546041252
Global test_loss: 2.174949493408203
Global Precision: 0.33739672085702105
Global Recall: 0.2391040546041252
Global f1score: 0.26119984101372185
50
50
number of selected users 50
Global Trainning Accurancy: 0.25716691084872917
Global Trainning Loss: 2.1475781345367433
Global test accurancy: 0.2395403718979452
Global test_loss: 2.1749519538879394
Global Precision: 0.33642502745287195
Global Recall: 0.2395403718979452
Global f1score: 0.2607952509485209
50
50
number of selected users 50
Global Trainning Accurancy: 0.25678296339165874
Global Trainning Loss: 2.1470610094070435
Global test accurancy: 0.2419549809057992
Global test_loss: 2.1749420070648195
Global Precision: 0.3369025101929513
Global Recall: 0.2419549809057992
Global f1score: 0.2633031980364995
50
50
number of selected users 50
Global Trainning Accurancy: 0.2581855895701746
Global Trainning Loss: 2.1461765718460084
Global test accurancy: 0.23963008193817675
Global test_loss: 2.174612216949463
Global Precision: 0.3349536431240614
Global Recall: 0.23963008193817675
Global f1score: 0.2611329955760714
50
50
number of selected users 50
Global Trainning Accurancy: 0.25845173826392276
Global Trainning Loss: 2.1455866622924806
Global test accurancy: 0.24132553899323342
Global test_loss: 2.1745521450042724
Global Precision: 0.33626585390141317
Global Recall: 0.24132553899323342
Global f1score: 0.26291920956075815
50
50
number of selected users 50
Global Trainning Accurancy: 0.257730167983494
Global Trainning Loss: 2.144944987297058
Global test accurancy: 0.24098938795234412
Global test_loss: 2.1744833326339723
Global Precision: 0.336746163768367
Global Recall: 0.24098938795234412
Global f1score: 0.26285606328951644
50
50
number of selected users 50
Global Trainning Accurancy: 0.25765437561550514
Global Trainning Loss: 2.144348726272583
Global test accurancy: 0.23890349531763835
Global test_loss: 2.1744334268569947
Global Precision: 0.33391227045075306
Global Recall: 0.23890349531763835
Global f1score: 0.26105990930005674
50
50
number of selected users 50
Global Trainning Accurancy: 0.25818952752426294
Global Trainning Loss: 2.143672380447388
Global test accurancy: 0.23944030265186952
Global test_loss: 2.1743175411224365
Global Precision: 0.33661745191839376
Global Recall: 0.23944030265186952
Global f1score: 0.2620311137143705
50
50
number of selected users 50
Global Trainning Accurancy: 0.25890118174663596
Global Trainning Loss: 2.1431939506530764
Global test accurancy: 0.24019801095972773
Global test_loss: 2.1744291400909423
Global Precision: 0.33690008739757044
Global Recall: 0.24019801095972773
Global f1score: 0.2629732017942603
50
50
number of selected users 50
Global Trainning Accurancy: 0.25963258730994054
Global Trainning Loss: 2.142690062522888
Global test accurancy: 0.23856475347243009
Global test_loss: 2.174533319473267
Global Precision: 0.33599407423625083
Global Recall: 0.23856475347243009
Global f1score: 0.2613595086779963
50
50
number of selected users 50
Global Trainning Accurancy: 0.2595267448180475
Global Trainning Loss: 2.142051920890808
Global test accurancy: 0.2376863190212487
Global test_loss: 2.1744722175598143
Global Precision: 0.3354593721049245
Global Recall: 0.2376863190212487
Global f1score: 0.2606352772022154
50
50
number of selected users 50
Global Trainning Accurancy: 0.25960974493356875
Global Trainning Loss: 2.141530866622925
Global test accurancy: 0.2383677656194098
Global test_loss: 2.1745972633361816
Global Precision: 0.3387588840844993
Global Recall: 0.2383677656194098
Global f1score: 0.261555555438078
50
50
number of selected users 50
Global Trainning Accurancy: 0.25920204069212405
Global Trainning Loss: 2.1409254837036134
Global test accurancy: 0.23769087331250968
Global test_loss: 2.174633893966675
Global Precision: 0.3382462545056513
Global Recall: 0.23769087331250968
Global f1score: 0.2612129752615753
50
50
number of selected users 50
Global Trainning Accurancy: 0.25903032916761565
Global Trainning Loss: 2.140469946861267
Global test accurancy: 0.2373875168620368
Global test_loss: 2.1748256921768188
Global Precision: 0.33766614741897516
Global Recall: 0.2373875168620368
Global f1score: 0.2604747593034094
50
50
number of selected users 50
Global Trainning Accurancy: 0.2597995517841116
Global Trainning Loss: 2.1398943519592284
Global test accurancy: 0.23822358324988852
Global test_loss: 2.17485641002655
Global Precision: 0.3381808701717569
Global Recall: 0.23822358324988852
Global f1score: 0.261353041142791
50
50
number of selected users 50
Global Trainning Accurancy: 0.2601425717094493
Global Trainning Loss: 2.139261155128479
Global test accurancy: 0.23800166226129063
Global test_loss: 2.1748376846313477
Global Precision: 0.3356578070756375
Global Recall: 0.23800166226129063
Global f1score: 0.2606348443985057
50
50
number of selected users 50
Global Trainning Accurancy: 0.26068225237871956
Global Trainning Loss: 2.138872494697571
Global test accurancy: 0.2377409118271248
Global test_loss: 2.175106725692749
Global Precision: 0.3372744624178315
Global Recall: 0.2377409118271248
Global f1score: 0.26047382602601293
50
50
number of selected users 50
Global Trainning Accurancy: 0.2615731775209926
Global Trainning Loss: 2.1383885526657105
Global test accurancy: 0.23544525965638463
Global test_loss: 2.1752450704574584
Global Precision: 0.3356348703472885
Global Recall: 0.23544525965638463
Global f1score: 0.2583967417092311
50
50
number of selected users 50
Global Trainning Accurancy: 0.2611107476407698
Global Trainning Loss: 2.1376677799224852
Global test accurancy: 0.23677754653088942
Global test_loss: 2.1752023839950563
Global Precision: 0.3373243321651276
Global Recall: 0.23677754653088942
Global f1score: 0.2596134792039471
50
50
number of selected users 50
Global Trainning Accurancy: 0.26060542572496204
Global Trainning Loss: 2.1371762800216674
Global test accurancy: 0.2373642837560626
Global test_loss: 2.1753400182724
Global Precision: 0.33870757221723274
Global Recall: 0.2373642837560626
Global f1score: 0.26055646980920444
50
50
number of selected users 50
Global Trainning Accurancy: 0.2611456699899068
Global Trainning Loss: 2.136384997367859
Global test accurancy: 0.23672365397949816
Global test_loss: 2.1752022886276245
Global Precision: 0.3399520710678968
Global Recall: 0.23672365397949816
Global f1score: 0.2597160938812837
50
50
number of selected users 50
Global Trainning Accurancy: 0.2604127482596462
Global Trainning Loss: 2.1358202075958252
Global test accurancy: 0.2366232252178602
Global test_loss: 2.1753119134902956
Global Precision: 0.33885856020715566
Global Recall: 0.2366232252178602
Global f1score: 0.25970397785050064
50
50
number of selected users 50
Global Trainning Accurancy: 0.2591821389462725
Global Trainning Loss: 2.1352529764175414
Global test accurancy: 0.23667339657349068
Global test_loss: 2.17540105342865
Global Precision: 0.3383502260078844
Global Recall: 0.23667339657349068
Global f1score: 0.2599991343799853
50
50
number of selected users 50
Global Trainning Accurancy: 0.2593432441424416
Global Trainning Loss: 2.134703588485718
Global test accurancy: 0.23684641736436657
Global test_loss: 2.1756108951568605
Global Precision: 0.33760578310452155
Global Recall: 0.23684641736436657
Global f1score: 0.2604392488585369
50
50
number of selected users 50
Global Trainning Accurancy: 0.25918970219681836
Global Trainning Loss: 2.134042081832886
Global test accurancy: 0.23737560930069967
Global test_loss: 2.1756788063049317
Global Precision: 0.3384827234606841
Global Recall: 0.23737560930069967
Global f1score: 0.26097535099268043
50
50
number of selected users 50
Global Trainning Accurancy: 0.25928283531301155
Global Trainning Loss: 2.1336733961105345
Global test accurancy: 0.2354523277666975
Global test_loss: 2.176132946014404
Global Precision: 0.33636775157002546
Global Recall: 0.2354523277666975
Global f1score: 0.2587759454259531
exp_no  0
0_dataset_CIFAR10_algorithm_MOON_L2_model_CNN_3_50_0.4_31_07_2024
