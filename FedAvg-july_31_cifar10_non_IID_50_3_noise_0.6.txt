============================================================
Summary of training process:
FL Algorithm: FedAvg
model: CNN
optimizer: SGD
Batch size: 124
Global_iters: 200
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
============================================================
/proj/bhuyan24/fed-divergence
CIFAR10
./data/data/noisy/0.6_50_3/train/cifa_train.json
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:19<1:03:39, 19.19s/it]  1%|          | 2/200 [00:23<33:36, 10.18s/it]    2%|▏         | 3/200 [00:26<23:56,  7.29s/it]  2%|▏         | 4/200 [00:30<19:24,  5.94s/it]  2%|▎         | 5/200 [00:34<16:56,  5.21s/it]  3%|▎         | 6/200 [00:38<15:18,  4.73s/it]  4%|▎         | 7/200 [00:42<14:21,  4.46s/it]  4%|▍         | 8/200 [00:46<13:39,  4.27s/it]  4%|▍         | 9/200 [00:50<13:09,  4.13s/it]  5%|▌         | 10/200 [00:53<12:44,  4.02s/it]  6%|▌         | 11/200 [00:57<12:26,  3.95s/it]  6%|▌         | 12/200 [01:01<12:13,  3.90s/it]  6%|▋         | 13/200 [01:05<12:03,  3.87s/it]  7%|▋         | 14/200 [01:09<11:56,  3.85s/it]  8%|▊         | 15/200 [01:12<11:52,  3.85s/it]  8%|▊         | 16/200 [01:16<11:45,  3.83s/it]  8%|▊         | 17/200 [01:20<11:41,  3.83s/it]  9%|▉         | 18/200 [01:24<11:38,  3.84s/it] 10%|▉         | 19/200 [01:28<11:35,  3.84s/it] 10%|█         | 20/200 [01:32<11:31,  3.84s/it] 10%|█         | 21/200 [01:35<11:30,  3.86s/it] 11%|█         | 22/200 [01:39<11:26,  3.86s/it] 12%|█▏        | 23/200 [01:43<11:23,  3.86s/it] 12%|█▏        | 24/200 [01:47<11:18,  3.85s/it] 12%|█▎        | 25/200 [01:51<11:14,  3.85s/it] 13%|█▎        | 26/200 [01:55<11:11,  3.86s/it] 14%|█▎        | 27/200 [01:59<11:08,  3.86s/it] 14%|█▍        | 28/200 [02:02<11:03,  3.86s/it] 14%|█▍        | 29/200 [02:06<11:02,  3.87s/it] 15%|█▌        | 30/200 [02:10<11:00,  3.89s/it] 16%|█▌        | 31/200 [02:14<11:01,  3.91s/it] 16%|█▌        | 32/200 [02:18<11:01,  3.94s/it] 16%|█▋        | 33/200 [02:22<11:05,  3.99s/it] 17%|█▋        | 34/200 [02:27<11:10,  4.04s/it] 18%|█▊        | 35/200 [02:31<11:22,  4.13s/it] 18%|█▊        | 36/200 [02:35<11:34,  4.23s/it] 18%|█▊        | 37/200 [02:40<11:41,  4.31s/it] 19%|█▉        | 38/200 [02:44<11:53,  4.41s/it] 20%|█▉        | 39/200 [02:49<11:59,  4.47s/it] 20%|██        | 40/200 [02:54<12:01,  4.51s/it] 20%|██        | 41/200 [02:58<12:00,  4.53s/it] 21%|██        | 42/200 [03:03<11:59,  4.55s/it] 22%|██▏       | 43/200 [03:08<12:01,  4.59s/it] 22%|██▏       | 44/200 [03:12<12:03,  4.64s/it] 22%|██▎       | 45/200 [03:17<12:06,  4.69s/it] 23%|██▎       | 46/200 [03:22<12:04,  4.70s/it] 24%|██▎       | 47/200 [03:26<11:53,  4.66s/it] 24%|██▍       | 48/200 [03:31<11:39,  4.60s/it] 24%|██▍       | 49/200 [03:35<11:27,  4.55s/it] 25%|██▌       | 50/200 [03:40<11:14,  4.49s/it] 26%|██▌       | 51/200 [03:44<10:58,  4.42s/it] 26%|██▌       | 52/200 [03:48<10:38,  4.31s/it] 26%|██▋       | 53/200 [03:52<10:22,  4.23s/it] 27%|██▋       | 54/200 [03:56<10:07,  4.16s/it] 28%|██▊       | 55/200 [04:00<09:51,  4.08s/it] 28%|██▊       | 56/200 [04:04<09:39,  4.02s/it] 28%|██▊       | 57/200 [04:08<09:29,  3.98s/it] 29%|██▉       | 58/200 [04:12<09:23,  3.97s/it] 30%|██▉       | 59/200 [04:16<09:20,  3.97s/it] 30%|███       | 60/200 [04:20<09:16,  3.97s/it] 30%|███       | 61/200 [04:23<09:07,  3.94s/it] 31%|███       | 62/200 [04:27<08:56,  3.89s/it] 32%|███▏      | 63/200 [04:31<08:49,  3.86s/it] 32%|███▏      | 64/200 [04:35<08:41,  3.84s/it] 32%|███▎      | 65/200 [04:39<08:37,  3.83s/it] 33%|███▎      | 66/200 [04:43<08:37,  3.86s/it] 34%|███▎      | 67/200 [04:46<08:33,  3.86s/it] 34%|███▍      | 68/200 [04:50<08:31,  3.88s/it] 34%|███▍      | 69/200 [04:54<08:27,  3.87s/it] 35%|███▌      | 70/200 [04:58<08:18,  3.84s/it] 36%|███▌      | 71/200 [05:02<08:12,  3.82s/it] 36%|███▌      | 72/200 [05:05<08:04,  3.79s/it] 36%|███▋      | 73/200 [05:09<08:01,  3.79s/it] 37%|███▋      | 74/200 [05:13<08:00,  3.81s/it] 38%|███▊      | 75/200 [05:17<07:56,  3.81s/it] 38%|███▊      | 76/200 [05:21<07:52,  3.81s/it] 38%|███▊      | 77/200 [05:25<07:47,  3.80s/it] 39%|███▉      | 78/200 [05:28<07:41,  3.79s/it] 40%|███▉      | 79/200 [05:32<07:39,  3.80s/it] 40%|████      | 80/200 [05:36<07:37,  3.81s/it] 40%|████      | 81/200 [05:40<07:34,  3.82s/it] 41%|████      | 82/200 [05:44<07:30,  3.82s/it] 42%|████▏     | 83/200 [05:47<07:29,  3.84s/it] 42%|████▏     | 84/200 [05:51<07:25,  3.84s/it] 42%|████▎     | 85/200 [05:55<07:23,  3.85s/it] 43%|████▎     | 86/200 [05:59<07:22,  3.88s/it] 44%|████▎     | 87/200 [06:03<07:19,  3.89s/it] 44%|████▍     | 88/200 [06:07<07:16,  3.90s/it] 44%|████▍     | 89/200 [06:11<07:11,  3.89s/it] 45%|████▌     | 90/200 [06:15<07:05,  3.87s/it] 46%|████▌     | 91/200 [06:18<06:59,  3.85s/it] 46%|████▌     | 92/200 [06:22<06:56,  3.85s/it] 46%|████▋     | 93/200 [06:26<06:51,  3.85s/it] 47%|████▋     | 94/200 [06:30<06:46,  3.83s/it] 48%|████▊     | 95/200 [06:34<06:39,  3.81s/it] 48%|████▊     | 96/200 [06:38<06:37,  3.82s/it] 48%|████▊     | 97/200 [06:41<06:31,  3.81s/it] 49%|████▉     | 98/200 [06:45<06:28,  3.81s/it] 50%|████▉     | 99/200 [06:49<06:26,  3.83s/it] 50%|█████     | 100/200 [06:53<06:24,  3.84s/it] 50%|█████     | 101/200 [06:57<06:20,  3.85s/it] 51%|█████     | 102/200 [07:01<06:18,  3.86s/it] 52%|█████▏    | 103/200 [07:04<06:09,  3.81s/it] 52%|█████▏    | 104/200 [07:08<06:05,  3.81s/it] 52%|█████▎    | 105/200 [07:12<06:01,  3.81s/it] 53%|█████▎    | 106/200 [07:16<05:57,  3.81s/it] 54%|█████▎    | 107/200 [07:20<05:53,  3.80s/it] 54%|█████▍    | 108/200 [07:23<05:47,  3.78s/it] 55%|█████▍    | 109/200 [07:27<05:43,  3.78s/it] 55%|█████▌    | 110/200 [07:31<05:36,  3.74s/it] 56%|█████▌    | 111/200 [07:34<05:32,  3.74s/it] 56%|█████▌    | 112/200 [07:38<05:29,  3.75s/it] 56%|█████▋    | 113/200 [07:42<05:25,  3.74s/it] 57%|█████▋    | 114/200 [07:46<05:19,  3.71s/it] 57%|█████▊    | 115/200 [07:49<05:13,  3.69s/it] 58%|█████▊    | 116/200 [07:53<05:11,  3.71s/it] 58%|█████▊    | 117/200 [07:57<05:06,  3.69s/it] 59%|█████▉    | 118/200 [08:00<05:01,  3.68s/it] 60%|█████▉    | 119/200 [08:04<04:55,  3.65s/it] 60%|██████    | 120/200 [08:07<04:51,  3.65s/it] 60%|██████    | 121/200 [08:11<04:47,  3.64s/it] 61%|██████    | 122/200 [08:15<04:42,  3.62s/it] 62%|██████▏   | 123/200 [08:18<04:39,  3.63s/it] 62%|██████▏   | 124/200 [08:22<04:37,  3.65s/it] 62%|██████▎   | 125/200 [08:26<04:32,  3.63s/it] 63%|██████▎   | 126/200 [08:29<04:29,  3.65s/it] 64%|██████▎   | 127/200 [08:33<04:26,  3.65s/it] 64%|██████▍   | 128/200 [08:37<04:22,  3.64s/it] 64%|██████▍   | 129/200 [08:40<04:17,  3.62s/it] 65%|██████▌   | 130/200 [08:44<04:12,  3.61s/it] 66%|██████▌   | 131/200 [08:47<04:10,  3.63s/it] 66%|██████▌   | 132/200 [08:51<04:06,  3.63s/it] 66%|██████▋   | 133/200 [08:55<04:03,  3.64s/it] 67%|██████▋   | 134/200 [08:58<04:01,  3.66s/it] 68%|██████▊   | 135/200 [09:02<03:56,  3.64s/it] 68%|██████▊   | 136/200 [09:06<03:53,  3.65s/it] 68%|██████▊   | 137/200 [09:09<03:49,  3.64s/it] 69%|██████▉   | 138/200 [09:13<03:45,  3.64s/it] 70%|██████▉   | 139/200 [09:17<03:42,  3.64s/it] 70%|███████   | 140/200 [09:20<03:37,  3.63s/it] 70%|███████   | 141/200 [09:24<03:33,  3.62s/it] 71%|███████   | 142/200 [09:27<03:29,  3.61s/it] 72%|███████▏  | 143/200 [09:31<03:24,  3.59s/it] 72%|███████▏  | 144/200 [09:34<03:19,  3.57s/it] 72%|███████▎  | 145/200 [09:38<03:14,  3.54s/it] 73%|███████▎  | 146/200 [09:41<03:11,  3.55s/it] 74%|███████▎  | 147/200 [09:45<03:08,  3.56s/it] 74%|███████▍  | 148/200 [09:49<03:05,  3.58s/it] 74%|███████▍  | 149/200 [09:52<03:02,  3.58s/it] 75%|███████▌  | 150/200 [09:56<02:57,  3.55s/it] 76%|███████▌  | 151/200 [09:59<02:53,  3.54s/it] 76%|███████▌  | 152/200 [10:03<02:49,  3.54s/it] 76%|███████▋  | 153/200 [10:06<02:47,  3.55s/it] 77%|███████▋  | 154/200 [10:10<02:42,  3.54s/it] 78%|███████▊  | 155/200 [10:13<02:38,  3.53s/it] 78%|███████▊  | 156/200 [10:17<02:35,  3.53s/it] 78%|███████▊  | 157/200 [10:20<02:31,  3.52s/it] 79%|███████▉  | 158/200 [10:24<02:27,  3.52s/it] 80%|███████▉  | 159/200 [10:27<02:24,  3.52s/it] 80%|████████  | 160/200 [10:31<02:20,  3.51s/it] 80%|████████  | 161/200 [10:34<02:16,  3.51s/it] 81%|████████  | 162/200 [10:38<02:12,  3.50s/it] 82%|████████▏ | 163/200 [10:41<02:08,  3.48s/it] 82%|████████▏ | 164/200 [10:45<02:05,  3.48s/it] 82%|████████▎ | 165/200 [10:48<02:01,  3.46s/it] 83%|████████▎ | 166/200 [10:52<01:57,  3.45s/it] 84%|████████▎ | 167/200 [10:55<01:53,  3.44s/it] 84%|████████▍ | 168/200 [10:58<01:49,  3.42s/it] 84%|████████▍ | 169/200 [11:02<01:45,  3.42s/it] 85%|████████▌ | 170/200 [11:05<01:42,  3.42s/it] 86%|████████▌ | 171/200 [11:09<01:39,  3.41s/it] 86%|████████▌ | 172/200 [11:12<01:35,  3.41s/it] 86%|████████▋ | 173/200 [11:15<01:31,  3.40s/it] 87%|████████▋ | 174/200 [11:19<01:28,  3.39s/it] 88%|████████▊ | 175/200 [11:22<01:24,  3.40s/it] 88%|████████▊ | 176/200 [11:26<01:21,  3.39s/it] 88%|████████▊ | 177/200 [11:29<01:17,  3.38s/it] 89%|████████▉ | 178/200 [11:32<01:14,  3.40s/it] 90%|████████▉ | 179/200 [11:36<01:11,  3.39s/it] 90%|█████████ | 180/200 [11:39<01:07,  3.38s/it] 90%|█████████ | 181/200 [11:43<01:04,  3.40s/it] 91%|█████████ | 182/200 [11:46<01:01,  3.40s/it] 92%|█████████▏| 183/200 [11:49<00:57,  3.39s/it] 92%|█████████▏| 184/200 [11:53<00:54,  3.38s/it] 92%|█████████▎| 185/200 [11:56<00:50,  3.38s/it] 93%|█████████▎| 186/200 [11:59<00:47,  3.39s/it] 94%|█████████▎| 187/200 [12:03<00:43,  3.38s/it] 94%|█████████▍| 188/200 [12:06<00:40,  3.38s/it] 94%|█████████▍| 189/200 [12:10<00:37,  3.36s/it] 95%|█████████▌| 190/200 [12:13<00:33,  3.36s/it] 96%|█████████▌| 191/200 [12:16<00:30,  3.36s/it] 96%|█████████▌| 192/200 [12:20<00:26,  3.36s/it] 96%|█████████▋| 193/200 [12:23<00:23,  3.35s/it] 97%|█████████▋| 194/200 [12:26<00:20,  3.36s/it] 98%|█████████▊| 195/200 [12:30<00:16,  3.35s/it] 98%|█████████▊| 196/200 [12:33<00:13,  3.34s/it] 98%|█████████▊| 197/200 [12:36<00:10,  3.35s/it] 99%|█████████▉| 198/200 [12:40<00:06,  3.34s/it]100%|█████████▉| 199/200 [12:43<00:03,  3.35s/it]100%|██████████| 200/200 [12:46<00:00,  3.36s/it]100%|██████████| 200/200 [12:46<00:00,  3.83s/it]
50
50
number of selected users 50
Global Trainning Accurancy: 0.09935829097383882
Global Trainning Loss: 2.303054480552673
Global test accurancy: 0.10033084192049224
Global test_loss: 2.302888107299805
Global Precision: 0.013493352200919096
Global Recall: 0.10033084192049224
Global f1score: 0.023175638335185055
50
50
number of selected users 50
Global Trainning Accurancy: 0.09935829097383882
Global Trainning Loss: 2.302922248840332
Global test accurancy: 0.10033084192049224
Global test_loss: 2.302768406867981
Global Precision: 0.013493352200919096
Global Recall: 0.10033084192049224
Global f1score: 0.023175638335185055
50
50
number of selected users 50
Global Trainning Accurancy: 0.09935829097383882
Global Trainning Loss: 2.3028009939193725
Global test accurancy: 0.10033084192049224
Global test_loss: 2.302659010887146
Global Precision: 0.013493352200919096
Global Recall: 0.10033084192049224
Global f1score: 0.023175638335185055
50
50
number of selected users 50
Global Trainning Accurancy: 0.09975197890276094
Global Trainning Loss: 2.3026898288726807
Global test accurancy: 0.10061655620620653
Global test_loss: 2.3025583124160764
Global Precision: 0.014944090638115188
Global Recall: 0.10061655620620653
Global f1score: 0.02368498577061788
50
50
number of selected users 50
Global Trainning Accurancy: 0.10245007995979877
Global Trainning Loss: 2.3025886392593384
Global test accurancy: 0.10442415038290467
Global test_loss: 2.3024680614471436
Global Precision: 0.03137176320793201
Global Recall: 0.10442415038290467
Global f1score: 0.0398605061262904
50
50
number of selected users 50
Global Trainning Accurancy: 0.10867488760050996
Global Trainning Loss: 2.3024957370758057
Global test accurancy: 0.10847054090772221
Global test_loss: 2.3023865699768065
Global Precision: 0.03783581039200879
Global Recall: 0.10847054090772221
Global f1score: 0.04809831603767599
50
50
number of selected users 50
Global Trainning Accurancy: 0.10368169447887637
Global Trainning Loss: 2.3024108171463014
Global test accurancy: 0.1022450258535941
Global test_loss: 2.3023117446899413
Global Precision: 0.04697395444492611
Global Recall: 0.1022450258535941
Global f1score: 0.038296102044234746
50
50
number of selected users 50
Global Trainning Accurancy: 0.10322859260621703
Global Trainning Loss: 2.302332754135132
Global test accurancy: 0.10077234644107713
Global test_loss: 2.3022442722320555
Global Precision: 0.029701558703222566
Global Recall: 0.10077234644107713
Global f1score: 0.03017623258925089
50
50
number of selected users 50
Global Trainning Accurancy: 0.10210861089878213
Global Trainning Loss: 2.3022595119476317
Global test accurancy: 0.10091250856460904
Global test_loss: 2.3021812534332273
Global Precision: 0.023961548233202186
Global Recall: 0.10091250856460904
Global f1score: 0.026553522241964674
50
50
number of selected users 50
Global Trainning Accurancy: 0.10174718714428996
Global Trainning Loss: 2.302190999984741
Global test accurancy: 0.100616008580725
Global test_loss: 2.3021219158172608
Global Precision: 0.01688363243256255
Global Recall: 0.100616008580725
Global f1score: 0.024462019485518006
50
50
number of selected users 50
Global Trainning Accurancy: 0.10140130082076468
Global Trainning Loss: 2.3021249961853028
Global test accurancy: 0.10026513138774254
Global test_loss: 2.302065386772156
Global Precision: 0.01545016519096547
Global Recall: 0.10026513138774254
Global f1score: 0.023852696824325337
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.302061939239502
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3020121097564696
Global Precision: 0.013786696743074945
Global Recall: 0.10026513138774254
Global f1score: 0.023516583042865014
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3020003700256346
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301960964202881
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301940541267395
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301910705566406
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301882309913635
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3018620824813842
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3018262004852295
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301816463470459
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3017721366882324
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3017714500427244
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301718039512634
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3017266464233397
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3016637182235717
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3016805982589723
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3016080331802367
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301637568473816
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301550521850586
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3015949821472166
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301493625640869
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3015527057647707
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3014383697509766
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3015129232406615
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301385660171509
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3014747858047486
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3013342094421385
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3014375257492063
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3012833070755003
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3014008235931396
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.30123309135437
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301365113258362
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3011820459365846
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3013276481628417
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3011300420761107
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3012881660461426
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3010759258270266
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3012478351593018
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3010162258148195
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3012031936645507
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3009541273117065
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3011558055877686
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3008925819396975
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3011137294769286
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3008347511291505
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301072082519531
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3007781648635866
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301026453971863
Global Precision: 0.013779073753683747
Global Recall: 0.10026513138774254
Global f1score: 0.02350786744743869
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012778449136928
Global Trainning Loss: 2.3007162952423097
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3009848690032957
Global Precision: 0.013779073753683747
Global Recall: 0.10026513138774254
Global f1score: 0.02350786744743869
50
50
number of selected users 50
Global Trainning Accurancy: 0.10152198628531464
Global Trainning Loss: 2.3006443309783937
Global test accurancy: 0.10049768952727743
Global test_loss: 2.3009401893615724
Global Precision: 0.017274831616145032
Global Recall: 0.10049768952727743
Global f1score: 0.02395553662528183
50
50
number of selected users 50
Global Trainning Accurancy: 0.10242249742947528
Global Trainning Loss: 2.300565447807312
Global test accurancy: 0.10181538383102968
Global test_loss: 2.300874376296997
Global Precision: 0.02902199698720079
Global Recall: 0.10181538383102968
Global f1score: 0.02628334698127311
50
50
number of selected users 50
Global Trainning Accurancy: 0.10319098286424783
Global Trainning Loss: 2.30046094417572
Global test accurancy: 0.10203682623624967
Global test_loss: 2.300788025856018
Global Precision: 0.037928493277043576
Global Recall: 0.10203682623624967
Global f1score: 0.02824400663540102
50
50
number of selected users 50
Global Trainning Accurancy: 0.10460712151113002
Global Trainning Loss: 2.30033408164978
Global test accurancy: 0.10253475426390778
Global test_loss: 2.300681118965149
Global Precision: 0.03946507732912418
Global Recall: 0.10253475426390778
Global f1score: 0.030569431929453193
50
50
number of selected users 50
Global Trainning Accurancy: 0.10666720902678155
Global Trainning Loss: 2.3001407861709593
Global test accurancy: 0.10288604874383532
Global test_loss: 2.300510220527649
Global Precision: 0.03756199298169321
Global Recall: 0.10288604874383532
Global f1score: 0.03250424536518638
50
50
number of selected users 50
Global Trainning Accurancy: 0.10971119327105378
Global Trainning Loss: 2.299850540161133
Global test accurancy: 0.10314745930386168
Global test_loss: 2.3002363204956056
Global Precision: 0.043236022005865925
Global Recall: 0.10314745930386168
Global f1score: 0.03937118714176471
50
50
number of selected users 50
Global Trainning Accurancy: 0.11328785333665477
Global Trainning Loss: 2.2994486618041994
Global test accurancy: 0.10628612160705442
Global test_loss: 2.2998521518707276
Global Precision: 0.059728704996262094
Global Recall: 0.10628612160705442
Global f1score: 0.048154209574889625
50
50
number of selected users 50
Global Trainning Accurancy: 0.1156264404876686
Global Trainning Loss: 2.2989249515533445
Global test accurancy: 0.11296903545904255
Global test_loss: 2.2993605852127077
Global Precision: 0.0618737191415142
Global Recall: 0.11296903545904255
Global f1score: 0.05727562475622496
50
50
number of selected users 50
Global Trainning Accurancy: 0.12171683651506844
Global Trainning Loss: 2.2982937717437744
Global test accurancy: 0.11853744979447624
Global test_loss: 2.298788194656372
Global Precision: 0.06403541046074084
Global Recall: 0.11853744979447624
Global f1score: 0.06385740956908496
50
50
number of selected users 50
Global Trainning Accurancy: 0.12319251813722625
Global Trainning Loss: 2.2976961660385133
Global test accurancy: 0.1246882947809974
Global test_loss: 2.298257060050964
Global Precision: 0.07842899959816843
Global Recall: 0.1246882947809974
Global f1score: 0.0754441940552598
50
50
number of selected users 50
Global Trainning Accurancy: 0.12576786830916895
Global Trainning Loss: 2.2971860790252685
Global test accurancy: 0.12379065461066655
Global test_loss: 2.297828617095947
Global Precision: 0.0818465466907671
Global Recall: 0.12379065461066655
Global f1score: 0.08444303820694739
50
50
number of selected users 50
Global Trainning Accurancy: 0.1311034388637092
Global Trainning Loss: 2.2967551279067995
Global test accurancy: 0.13257828168063526
Global test_loss: 2.2974867677688597
Global Precision: 0.08909117805906736
Global Recall: 0.13257828168063526
Global f1score: 0.09494785311228514
50
50
number of selected users 50
Global Trainning Accurancy: 0.13457179967233715
Global Trainning Loss: 2.296374807357788
Global test accurancy: 0.1339326453843612
Global test_loss: 2.2971914482116698
Global Precision: 0.10108718983730325
Global Recall: 0.1339326453843612
Global f1score: 0.09515133071928576
50
50
number of selected users 50
Global Trainning Accurancy: 0.13433838685930138
Global Trainning Loss: 2.296017484664917
Global test accurancy: 0.13438335635208343
Global test_loss: 2.2969189739227294
Global Precision: 0.10469579995745693
Global Recall: 0.13438335635208343
Global f1score: 0.09555606551577082
50
50
number of selected users 50
Global Trainning Accurancy: 0.13523443085530298
Global Trainning Loss: 2.2956649971008303
Global test accurancy: 0.1367648335241818
Global test_loss: 2.2966489219665527
Global Precision: 0.10323094793678807
Global Recall: 0.1367648335241818
Global f1score: 0.09551922241866581
50
50
number of selected users 50
Global Trainning Accurancy: 0.1372063107496235
Global Trainning Loss: 2.295309863090515
Global test accurancy: 0.13985679906144793
Global test_loss: 2.296372580528259
Global Precision: 0.11643210773214871
Global Recall: 0.13985679906144793
Global f1score: 0.09876216354364739
50
50
number of selected users 50
Global Trainning Accurancy: 0.13767691530418194
Global Trainning Loss: 2.294951844215393
Global test accurancy: 0.1417095450937007
Global test_loss: 2.296094365119934
Global Precision: 0.12198541298253691
Global Recall: 0.1417095450937007
Global f1score: 0.10034050657697378
50
50
number of selected users 50
Global Trainning Accurancy: 0.13803841028013508
Global Trainning Loss: 2.2945866870880125
Global test accurancy: 0.14401368736973164
Global test_loss: 2.2958095407485963
Global Precision: 0.12378315927475629
Global Recall: 0.14401368736973164
Global f1score: 0.10189193501659702
50
50
number of selected users 50
Global Trainning Accurancy: 0.13910828884963286
Global Trainning Loss: 2.2942197275161744
Global test accurancy: 0.14383879731711355
Global test_loss: 2.2955280494689942
Global Precision: 0.12368103473900466
Global Recall: 0.14383879731711355
Global f1score: 0.10146437332130281
50
50
number of selected users 50
Global Trainning Accurancy: 0.1399183360902758
Global Trainning Loss: 2.293847050666809
Global test accurancy: 0.14410370822017948
Global test_loss: 2.2952409172058106
Global Precision: 0.12275980988241914
Global Recall: 0.14410370822017948
Global f1score: 0.10198274427920008
50
50
number of selected users 50
Global Trainning Accurancy: 0.14059134615284727
Global Trainning Loss: 2.293467712402344
Global test accurancy: 0.1440658478580172
Global test_loss: 2.2949491119384766
Global Precision: 0.1231579953094172
Global Recall: 0.1440658478580172
Global f1score: 0.1023867714647645
50
50
number of selected users 50
Global Trainning Accurancy: 0.14166265840378792
Global Trainning Loss: 2.2930850315093996
Global test accurancy: 0.1446123461764043
Global test_loss: 2.2946548461914062
Global Precision: 0.12047438445439511
Global Recall: 0.1446123461764043
Global f1score: 0.10347468138324133
50
50
number of selected users 50
Global Trainning Accurancy: 0.14267676593879955
Global Trainning Loss: 2.2926990222930907
Global test accurancy: 0.1452634181132668
Global test_loss: 2.2943555688858033
Global Precision: 0.11902382576913398
Global Recall: 0.1452634181132668
Global f1score: 0.10378741850441751
50
50
number of selected users 50
Global Trainning Accurancy: 0.14248225550114466
Global Trainning Loss: 2.2923120498657226
Global test accurancy: 0.1457939350029675
Global test_loss: 2.2940554237365722
Global Precision: 0.11887905177145204
Global Recall: 0.1457939350029675
Global f1score: 0.10382278259398511
50
50
number of selected users 50
Global Trainning Accurancy: 0.14337782706765498
Global Trainning Loss: 2.291918883323669
Global test accurancy: 0.14498781242549538
Global test_loss: 2.2937518644332884
Global Precision: 0.11213393159495516
Global Recall: 0.14498781242549538
Global f1score: 0.10315344155388065
50
50
number of selected users 50
Global Trainning Accurancy: 0.14357166922263187
Global Trainning Loss: 2.291525092124939
Global test accurancy: 0.14464711080745987
Global test_loss: 2.293449349403381
Global Precision: 0.11062429364184732
Global Recall: 0.14464711080745987
Global f1score: 0.10323776278663876
50
50
number of selected users 50
Global Trainning Accurancy: 0.14378673122154084
Global Trainning Loss: 2.291126947402954
Global test accurancy: 0.1436074793379344
Global test_loss: 2.2931408166885374
Global Precision: 0.10681013570201318
Global Recall: 0.1436074793379344
Global f1score: 0.1025416211551709
50
50
number of selected users 50
Global Trainning Accurancy: 0.14329489864026407
Global Trainning Loss: 2.290729212760925
Global test accurancy: 0.1435595997589459
Global test_loss: 2.292833366394043
Global Precision: 0.10690611832935441
Global Recall: 0.1435595997589459
Global f1score: 0.10269106823322291
50
50
number of selected users 50
Global Trainning Accurancy: 0.14455071612596251
Global Trainning Loss: 2.290326189994812
Global test accurancy: 0.14388713826357583
Global test_loss: 2.292524299621582
Global Precision: 0.09841936693257104
Global Recall: 0.14388713826357583
Global f1score: 0.10272348982225524
50
50
number of selected users 50
Global Trainning Accurancy: 0.14547655813388888
Global Trainning Loss: 2.2899251699447634
Global test accurancy: 0.14384380827007973
Global test_loss: 2.2922168159484864
Global Precision: 0.098337260343651
Global Recall: 0.14384380827007973
Global f1score: 0.10243623498817718
50
50
number of selected users 50
Global Trainning Accurancy: 0.14614235007624807
Global Trainning Loss: 2.2895198678970337
Global test accurancy: 0.1444771423277742
Global test_loss: 2.291905360221863
Global Precision: 0.09773154773060799
Global Recall: 0.1444771423277742
Global f1score: 0.1030809707157374
50
50
number of selected users 50
Global Trainning Accurancy: 0.14634052141963108
Global Trainning Loss: 2.289113187789917
Global test accurancy: 0.144129443154237
Global test_loss: 2.2915935707092285
Global Precision: 0.09738798294579955
Global Recall: 0.144129443154237
Global f1score: 0.10264976455158031
50
50
number of selected users 50
Global Trainning Accurancy: 0.1485382164854314
Global Trainning Loss: 2.2887046909332276
Global test accurancy: 0.1450778516356409
Global test_loss: 2.2912777185440065
Global Precision: 0.09869995691410156
Global Recall: 0.1450778516356409
Global f1score: 0.1037266414956423
50
50
number of selected users 50
Global Trainning Accurancy: 0.14903112957419584
Global Trainning Loss: 2.2882969760894776
Global test accurancy: 0.14593399421428987
Global test_loss: 2.2909658575057983
Global Precision: 0.09853588058329311
Global Recall: 0.14593399421428987
Global f1score: 0.1043129230245174
50
50
number of selected users 50
Global Trainning Accurancy: 0.15037969542027235
Global Trainning Loss: 2.287891221046448
Global test accurancy: 0.14540843678707227
Global test_loss: 2.2906562948226927
Global Precision: 0.09842453094647079
Global Recall: 0.14540843678707227
Global f1score: 0.10362963496031084
50
50
number of selected users 50
Global Trainning Accurancy: 0.15081378588437772
Global Trainning Loss: 2.2874850940704348
Global test accurancy: 0.14689409653386631
Global test_loss: 2.2903498220443725
Global Precision: 0.100067375056725
Global Recall: 0.14689409653386631
Global f1score: 0.10519985468424085
50
50
number of selected users 50
Global Trainning Accurancy: 0.1507686413612339
Global Trainning Loss: 2.287080726623535
Global test accurancy: 0.1481784179064688
Global test_loss: 2.2900465869903566
Global Precision: 0.1004235477438416
Global Recall: 0.1481784179064688
Global f1score: 0.10594223125610025
50
50
number of selected users 50
Global Trainning Accurancy: 0.15081111223926638
Global Trainning Loss: 2.2866790533065795
Global test accurancy: 0.14850575185826065
Global test_loss: 2.289748649597168
Global Precision: 0.10072290933846932
Global Recall: 0.14850575185826065
Global f1score: 0.10612417556852798
50
50
number of selected users 50
Global Trainning Accurancy: 0.15027986663576007
Global Trainning Loss: 2.286280460357666
Global test accurancy: 0.14819681867822468
Global test_loss: 2.289455189704895
Global Precision: 0.10230019960537258
Global Recall: 0.14819681867822468
Global f1score: 0.10640919153926559
50
50
number of selected users 50
Global Trainning Accurancy: 0.15094288975086018
Global Trainning Loss: 2.285884680747986
Global test accurancy: 0.14891634747752222
Global test_loss: 2.2891684436798094
Global Precision: 0.10176712128873386
Global Recall: 0.14891634747752222
Global f1score: 0.10654149404372194
50
50
number of selected users 50
Global Trainning Accurancy: 0.1517643302981334
Global Trainning Loss: 2.2854902744293213
Global test accurancy: 0.14827637009890543
Global test_loss: 2.2888851261138914
Global Precision: 0.10123134581756092
Global Recall: 0.14827637009890543
Global f1score: 0.1062176078095461
50
50
number of selected users 50
Global Trainning Accurancy: 0.1523318706407763
Global Trainning Loss: 2.2851045608520506
Global test accurancy: 0.14810125506925567
Global test_loss: 2.2886099004745484
Global Precision: 0.10120581720376134
Global Recall: 0.14810125506925567
Global f1score: 0.10618712971254449
50
50
number of selected users 50
Global Trainning Accurancy: 0.15277116701764457
Global Trainning Loss: 2.284722938537598
Global test accurancy: 0.1485757002666235
Global test_loss: 2.2883417129516603
Global Precision: 0.10152695769813984
Global Recall: 0.1485757002666235
Global f1score: 0.10646716705470519
50
50
number of selected users 50
Global Trainning Accurancy: 0.1525607715843879
Global Trainning Loss: 2.284346032142639
Global test accurancy: 0.14891440590819977
Global test_loss: 2.2880785179138186
Global Precision: 0.10455496482769712
Global Recall: 0.14891440590819977
Global f1score: 0.10686830579511583
50
50
number of selected users 50
Global Trainning Accurancy: 0.15235605254836423
Global Trainning Loss: 2.28397497177124
Global test accurancy: 0.14896438555491837
Global test_loss: 2.2878249502182006
Global Precision: 0.10595890323162857
Global Recall: 0.14896438555491837
Global f1score: 0.10710550276515032
50
50
number of selected users 50
Global Trainning Accurancy: 0.15279931141387593
Global Trainning Loss: 2.2836065673828125
Global test accurancy: 0.1486783286450763
Global test_loss: 2.287574443817139
Global Precision: 0.10599310068794157
Global Recall: 0.1486783286450763
Global f1score: 0.10716618687390002
50
50
number of selected users 50
Global Trainning Accurancy: 0.15301870849686747
Global Trainning Loss: 2.283246636390686
Global test accurancy: 0.14822751477794435
Global test_loss: 2.2873300409317014
Global Precision: 0.10740668205126368
Global Recall: 0.14822751477794435
Global f1score: 0.10696688875210637
50
50
number of selected users 50
Global Trainning Accurancy: 0.15338390188980178
Global Trainning Loss: 2.28289297580719
Global test accurancy: 0.14794703260173295
Global test_loss: 2.287094898223877
Global Precision: 0.10792268916457695
Global Recall: 0.14794703260173295
Global f1score: 0.10717630814537896
50
50
number of selected users 50
Global Trainning Accurancy: 0.15372935117311318
Global Trainning Loss: 2.282546944618225
Global test accurancy: 0.14845234669211357
Global test_loss: 2.2868683195114134
Global Precision: 0.10903410244323848
Global Recall: 0.14845234669211357
Global f1score: 0.10787768915924653
50
50
number of selected users 50
Global Trainning Accurancy: 0.15366087301458295
Global Trainning Loss: 2.2822032833099364
Global test accurancy: 0.14992158306641265
Global test_loss: 2.28664345741272
Global Precision: 0.11495288345456299
Global Recall: 0.14992158306641265
Global f1score: 0.1103678568784058
50
50
number of selected users 50
Global Trainning Accurancy: 0.15454599614340964
Global Trainning Loss: 2.281869945526123
Global test accurancy: 0.14901016290446137
Global test_loss: 2.2864322757720945
Global Precision: 0.11512615668263561
Global Recall: 0.14901016290446137
Global f1score: 0.1100474274823208
50
50
number of selected users 50
Global Trainning Accurancy: 0.15517805023465805
Global Trainning Loss: 2.2815459728240968
Global test accurancy: 0.149672427431789
Global test_loss: 2.286227054595947
Global Precision: 0.12495752471670002
Global Recall: 0.149672427431789
Global f1score: 0.11183666600190907
50
50
number of selected users 50
Global Trainning Accurancy: 0.15508413536216678
Global Trainning Loss: 2.2812166166305543
Global test accurancy: 0.14982206236334575
Global test_loss: 2.286015429496765
Global Precision: 0.12763217517309425
Global Recall: 0.14982206236334575
Global f1score: 0.11213287187401955
50
50
number of selected users 50
Global Trainning Accurancy: 0.15461874339437073
Global Trainning Loss: 2.280883593559265
Global test accurancy: 0.15007323170954823
Global test_loss: 2.2858150100708006
Global Precision: 0.1307880742330596
Global Recall: 0.15007323170954823
Global f1score: 0.11262516051029464
50
50
number of selected users 50
Global Trainning Accurancy: 0.15389551511085028
Global Trainning Loss: 2.280549850463867
Global test accurancy: 0.1504540723887143
Global test_loss: 2.285610237121582
Global Precision: 0.13067803486287152
Global Recall: 0.1504540723887143
Global f1score: 0.11294252671300795
50
50
number of selected users 50
Global Trainning Accurancy: 0.15428739843836922
Global Trainning Loss: 2.2802220392227173
Global test accurancy: 0.1498205121764214
Global test_loss: 2.2854156160354613
Global Precision: 0.1266544517753077
Global Recall: 0.1498205121764214
Global f1score: 0.11266814091918367
50
50
number of selected users 50
Global Trainning Accurancy: 0.15534624141619993
Global Trainning Loss: 2.279901132583618
Global test accurancy: 0.1505173874464663
Global test_loss: 2.2852322673797607
Global Precision: 0.13178902529201084
Global Recall: 0.1505173874464663
Global f1score: 0.1138504029551783
50
50
number of selected users 50
Global Trainning Accurancy: 0.15460627407601008
Global Trainning Loss: 2.2795988607406614
Global test accurancy: 0.15134851796741472
Global test_loss: 2.285067706108093
Global Precision: 0.13873787144354566
Global Recall: 0.15134851796741472
Global f1score: 0.11592881129213616
50
50
number of selected users 50
Global Trainning Accurancy: 0.15484916532902188
Global Trainning Loss: 2.279298448562622
Global test accurancy: 0.1510986061523851
Global test_loss: 2.284907703399658
Global Precision: 0.13947572305171793
Global Recall: 0.1510986061523851
Global f1score: 0.1165162600575159
50
50
number of selected users 50
Global Trainning Accurancy: 0.15478343506962514
Global Trainning Loss: 2.2790024280548096
Global test accurancy: 0.1513863970482896
Global test_loss: 2.2847503328323366
Global Precision: 0.13745089321366358
Global Recall: 0.1513863970482896
Global f1score: 0.11695774292162699
50
50
number of selected users 50
Global Trainning Accurancy: 0.15527267770806893
Global Trainning Loss: 2.2787139892578123
Global test accurancy: 0.15218477632672983
Global test_loss: 2.2846022129058836
Global Precision: 0.1396107608836184
Global Recall: 0.15218477632672983
Global f1score: 0.11865129293001062
50
50
number of selected users 50
Global Trainning Accurancy: 0.15518483865398003
Global Trainning Loss: 2.2784287118911744
Global test accurancy: 0.15351738057454775
Global test_loss: 2.284455714225769
Global Precision: 0.1426086255629852
Global Recall: 0.15351738057454775
Global f1score: 0.12067504613818938
50
50
number of selected users 50
Global Trainning Accurancy: 0.15616992542697652
Global Trainning Loss: 2.278152732849121
Global test accurancy: 0.15300927271937062
Global test_loss: 2.2843262338638306
Global Precision: 0.13911970966244597
Global Recall: 0.15300927271937062
Global f1score: 0.12076163906155038
50
50
number of selected users 50
Global Trainning Accurancy: 0.15545768438873936
Global Trainning Loss: 2.277885413169861
Global test accurancy: 0.15354046074675687
Global test_loss: 2.284202537536621
Global Precision: 0.13842410153319978
Global Recall: 0.15354046074675687
Global f1score: 0.12117588645922919
50
50
number of selected users 50
Global Trainning Accurancy: 0.1554755072768464
Global Trainning Loss: 2.2776154899597167
Global test accurancy: 0.15410358699676943
Global test_loss: 2.2840695190429687
Global Precision: 0.13529123865073603
Global Recall: 0.15410358699676943
Global f1score: 0.12142291768818195
50
50
number of selected users 50
Global Trainning Accurancy: 0.155835141827932
Global Trainning Loss: 2.2773530197143557
Global test accurancy: 0.1537767170136585
Global test_loss: 2.2839439344406127
Global Precision: 0.13364293282494197
Global Recall: 0.1537767170136585
Global f1score: 0.12131061845335861
50
50
number of selected users 50
Global Trainning Accurancy: 0.15570291808399161
Global Trainning Loss: 2.277082862854004
Global test accurancy: 0.1541386042519353
Global test_loss: 2.2838017988204955
Global Precision: 0.13320406732628287
Global Recall: 0.1541386042519353
Global f1score: 0.12214917525499355
50
50
number of selected users 50
Global Trainning Accurancy: 0.155584731821239
Global Trainning Loss: 2.276808567047119
Global test accurancy: 0.1549306148856535
Global test_loss: 2.2836617708206175
Global Precision: 0.1329403233346898
Global Recall: 0.1549306148856535
Global f1score: 0.12306135428062494
50
50
number of selected users 50
Global Trainning Accurancy: 0.15624303676607898
Global Trainning Loss: 2.276535062789917
Global test accurancy: 0.15613109867916977
Global test_loss: 2.283537254333496
Global Precision: 0.13329345252863445
Global Recall: 0.15613109867916977
Global f1score: 0.12470845257947849
50
50
number of selected users 50
Global Trainning Accurancy: 0.15693462275552994
Global Trainning Loss: 2.276269407272339
Global test accurancy: 0.15546958223456298
Global test_loss: 2.283414249420166
Global Precision: 0.1320272861518729
Global Recall: 0.15546958223456298
Global f1score: 0.12431883452673406
50
50
number of selected users 50
Global Trainning Accurancy: 0.15665891848014102
Global Trainning Loss: 2.2760010719299317
Global test accurancy: 0.15568255557190436
Global test_loss: 2.2833011150360107
Global Precision: 0.1310325221397127
Global Recall: 0.15568255557190436
Global f1score: 0.12480305383420617
50
50
number of selected users 50
Global Trainning Accurancy: 0.15658428185598586
Global Trainning Loss: 2.275727243423462
Global test accurancy: 0.15590469713270289
Global test_loss: 2.283186354637146
Global Precision: 0.12858818855736898
Global Recall: 0.15590469713270289
Global f1score: 0.12520325039245384
50
50
number of selected users 50
Global Trainning Accurancy: 0.15704085758291547
Global Trainning Loss: 2.2754622411727907
Global test accurancy: 0.154833301191164
Global test_loss: 2.2830669260025025
Global Precision: 0.12829596293782244
Global Recall: 0.154833301191164
Global f1score: 0.12465115812897033
50
50
number of selected users 50
Global Trainning Accurancy: 0.1583059511182078
Global Trainning Loss: 2.27519672870636
Global test accurancy: 0.15377705909170053
Global test_loss: 2.282958812713623
Global Precision: 0.12789703210851527
Global Recall: 0.15377705909170053
Global f1score: 0.12432681314805737
50
50
number of selected users 50
Global Trainning Accurancy: 0.15743701722511622
Global Trainning Loss: 2.2749342775344847
Global test accurancy: 0.15446691081676361
Global test_loss: 2.2828434610366823
Global Precision: 0.12796345084118227
Global Recall: 0.15446691081676361
Global f1score: 0.12484168460603451
50
50
number of selected users 50
Global Trainning Accurancy: 0.15835773887964646
Global Trainning Loss: 2.274664907455444
Global test accurancy: 0.15469688951617008
Global test_loss: 2.282727918624878
Global Precision: 0.1262093790496234
Global Recall: 0.15469688951617008
Global f1score: 0.12468083443269513
50
50
number of selected users 50
Global Trainning Accurancy: 0.15780607629281976
Global Trainning Loss: 2.2744004011154173
Global test accurancy: 0.1545175313235755
Global test_loss: 2.2826057624816896
Global Precision: 0.12521687237448137
Global Recall: 0.1545175313235755
Global f1score: 0.12478260466842228
50
50
number of selected users 50
Global Trainning Accurancy: 0.15826557654861242
Global Trainning Loss: 2.274132137298584
Global test accurancy: 0.1535659590773786
Global test_loss: 2.282485089302063
Global Precision: 0.12528091012197984
Global Recall: 0.1535659590773786
Global f1score: 0.12409934048749634
50
50
number of selected users 50
Global Trainning Accurancy: 0.15855634725961545
Global Trainning Loss: 2.273868942260742
Global test accurancy: 0.15362505570336296
Global test_loss: 2.2823749446868895
Global Precision: 0.1246253804723654
Global Recall: 0.15362505570336296
Global f1score: 0.12440422250229854
50
50
number of selected users 50
Global Trainning Accurancy: 0.15802206437526026
Global Trainning Loss: 2.273607277870178
Global test accurancy: 0.15290796089919007
Global test_loss: 2.2822663021087646
Global Precision: 0.12449089283136576
Global Recall: 0.15290796089919007
Global f1score: 0.12424942820564275
50
50
number of selected users 50
Global Trainning Accurancy: 0.1584340108855349
Global Trainning Loss: 2.2733499574661256
Global test accurancy: 0.1527672408149686
Global test_loss: 2.282153525352478
Global Precision: 0.12446293866315607
Global Recall: 0.1527672408149686
Global f1score: 0.12454778102830592
50
50
number of selected users 50
Global Trainning Accurancy: 0.1586462494522046
Global Trainning Loss: 2.2730919027328493
Global test accurancy: 0.1524556323005951
Global test_loss: 2.2820472383499144
Global Precision: 0.12485290276965522
Global Recall: 0.1524556323005951
Global f1score: 0.12464450371989826
50
50
number of selected users 50
Global Trainning Accurancy: 0.1591920093224038
Global Trainning Loss: 2.2728338623046875
Global test accurancy: 0.15222307416106023
Global test_loss: 2.2819397163391115
Global Precision: 0.12471562599112827
Global Recall: 0.15222307416106023
Global f1score: 0.12454506746846879
50
50
number of selected users 50
Global Trainning Accurancy: 0.15954640466167044
Global Trainning Loss: 2.2725629568099976
Global test accurancy: 0.15267128315287976
Global test_loss: 2.281829481124878
Global Precision: 0.12505025234158895
Global Recall: 0.15267128315287976
Global f1score: 0.12509991868127346
50
50
number of selected users 50
Global Trainning Accurancy: 0.15979533964760437
Global Trainning Loss: 2.272298264503479
Global test accurancy: 0.15398591475245998
Global test_loss: 2.2817240858078005
Global Precision: 0.12684506807798387
Global Recall: 0.15398591475245998
Global f1score: 0.12668956032617887
50
50
number of selected users 50
Global Trainning Accurancy: 0.16013000796474683
Global Trainning Loss: 2.2720304918289185
Global test accurancy: 0.15396241027093452
Global test_loss: 2.2816184663772585
Global Precision: 0.12606794737328808
Global Recall: 0.15396241027093452
Global f1score: 0.12674157026459876
50
50
number of selected users 50
Global Trainning Accurancy: 0.15970578327839915
Global Trainning Loss: 2.27177761554718
Global test accurancy: 0.15323789680165953
Global test_loss: 2.281526231765747
Global Precision: 0.12998104838377036
Global Recall: 0.15323789680165953
Global f1score: 0.12690348976344967
50
50
number of selected users 50
Global Trainning Accurancy: 0.1602420484099144
Global Trainning Loss: 2.2715132331848142
Global test accurancy: 0.15193024500277305
Global test_loss: 2.2814216899871824
Global Precision: 0.12914889837727428
Global Recall: 0.15193024500277305
Global f1score: 0.12608045321729489
50
50
number of selected users 50
Global Trainning Accurancy: 0.16070874069944266
Global Trainning Loss: 2.271252360343933
Global test accurancy: 0.15177603836161946
Global test_loss: 2.2813259506225587
Global Precision: 0.12967323324428315
Global Recall: 0.15177603836161946
Global f1score: 0.12631902696399933
50
50
number of selected users 50
Global Trainning Accurancy: 0.16058816494058678
Global Trainning Loss: 2.270988173484802
Global test accurancy: 0.15184293060092546
Global test_loss: 2.281231288909912
Global Precision: 0.12911287347582653
Global Recall: 0.15184293060092546
Global f1score: 0.12618273611908917
50
50
number of selected users 50
Global Trainning Accurancy: 0.16065020559007206
Global Trainning Loss: 2.2707302951812744
Global test accurancy: 0.153878795273744
Global test_loss: 2.2811433839797974
Global Precision: 0.13417538951749747
Global Recall: 0.153878795273744
Global f1score: 0.12876407525886333
50
50
number of selected users 50
Global Trainning Accurancy: 0.16141503908540863
Global Trainning Loss: 2.270467896461487
Global test accurancy: 0.15322523003635516
Global test_loss: 2.2810513019561767
Global Precision: 0.13436680753086874
Global Recall: 0.15322523003635516
Global f1score: 0.128879446415339
50
50
number of selected users 50
Global Trainning Accurancy: 0.16180811249919422
Global Trainning Loss: 2.2702040576934817
Global test accurancy: 0.15378469209251008
Global test_loss: 2.280955080986023
Global Precision: 0.1355624770787216
Global Recall: 0.15378469209251008
Global f1score: 0.1297775494122388
50
50
number of selected users 50
Global Trainning Accurancy: 0.16199065916743452
Global Trainning Loss: 2.2699424409866333
Global test accurancy: 0.15377218844134422
Global test_loss: 2.2808567762374876
Global Precision: 0.134016396917316
Global Recall: 0.15377218844134422
Global f1score: 0.12961438520628085
50
50
number of selected users 50
Global Trainning Accurancy: 0.16265372193055283
Global Trainning Loss: 2.2696707487106322
Global test accurancy: 0.15321811774332042
Global test_loss: 2.280741934776306
Global Precision: 0.13296711218416413
Global Recall: 0.15321811774332042
Global f1score: 0.1291944665569649
50
50
number of selected users 50
Global Trainning Accurancy: 0.1637356548745595
Global Trainning Loss: 2.269399847984314
Global test accurancy: 0.15370398330288018
Global test_loss: 2.280642857551575
Global Precision: 0.13387650583153404
Global Recall: 0.15370398330288018
Global f1score: 0.129861623785458
50
50
number of selected users 50
Global Trainning Accurancy: 0.16386334460643728
Global Trainning Loss: 2.269135384559631
Global test accurancy: 0.15413750284038222
Global test_loss: 2.280552840232849
Global Precision: 0.13310553810350464
Global Recall: 0.15413750284038222
Global f1score: 0.13007299600359562
50
50
number of selected users 50
Global Trainning Accurancy: 0.16400876833463723
Global Trainning Loss: 2.268861141204834
Global test accurancy: 0.15407410656439616
Global test_loss: 2.2804443883895873
Global Precision: 0.13416545519281162
Global Recall: 0.15407410656439616
Global f1score: 0.13067165182725737
50
50
number of selected users 50
Global Trainning Accurancy: 0.16398345607053602
Global Trainning Loss: 2.2685847091674805
Global test accurancy: 0.15402921807163308
Global test_loss: 2.2803300285339354
Global Precision: 0.13443037088643173
Global Recall: 0.15402921807163308
Global f1score: 0.13063614700452486
50
50
number of selected users 50
Global Trainning Accurancy: 0.16491725706514535
Global Trainning Loss: 2.268306350708008
Global test accurancy: 0.15439252986600263
Global test_loss: 2.2802177619934083
Global Precision: 0.13552261586015227
Global Recall: 0.15439252986600263
Global f1score: 0.13132458827108617
50
50
number of selected users 50
Global Trainning Accurancy: 0.16443025703192551
Global Trainning Loss: 2.2680118036270143
Global test accurancy: 0.15477427205907482
Global test_loss: 2.280094542503357
Global Precision: 0.13634981209387428
Global Recall: 0.15477427205907482
Global f1score: 0.13185215451633575
50
50
number of selected users 50
Global Trainning Accurancy: 0.16484485813806254
Global Trainning Loss: 2.2677223014831545
Global test accurancy: 0.15547733956661727
Global test_loss: 2.2799776697158816
Global Precision: 0.1371651484084998
Global Recall: 0.15547733956661727
Global f1score: 0.13285674028947753
50
50
number of selected users 50
Global Trainning Accurancy: 0.16538114068787893
Global Trainning Loss: 2.2674351596832274
Global test accurancy: 0.15436528272357442
Global test_loss: 2.2798603057861326
Global Precision: 0.13613640086950726
Global Recall: 0.15436528272357442
Global f1score: 0.13208514781616548
50
50
number of selected users 50
Global Trainning Accurancy: 0.16615455931533982
Global Trainning Loss: 2.2671559190750123
Global test accurancy: 0.15500420032029694
Global test_loss: 2.2797570276260375
Global Precision: 0.1369683133070222
Global Recall: 0.15500420032029694
Global f1score: 0.1328012297619837
50
50
number of selected users 50
Global Trainning Accurancy: 0.16706550503559123
Global Trainning Loss: 2.2668702459335326
Global test accurancy: 0.15553087610689065
Global test_loss: 2.2796416234970094
Global Precision: 0.1390757231352956
Global Recall: 0.15553087610689065
Global f1score: 0.13327477163149082
50
50
number of selected users 50
Global Trainning Accurancy: 0.16666595431046105
Global Trainning Loss: 2.2665883111953735
Global test accurancy: 0.15493098144366235
Global test_loss: 2.2795405435562133
Global Precision: 0.13912807475101363
Global Recall: 0.15493098144366235
Global f1score: 0.133242483359993
50
50
number of selected users 50
Global Trainning Accurancy: 0.16690527199735497
Global Trainning Loss: 2.266301894187927
Global test accurancy: 0.15366467661421188
Global test_loss: 2.2794296741485596
Global Precision: 0.13779218286230524
Global Recall: 0.15366467661421188
Global f1score: 0.13205741158638487
50
50
number of selected users 50
Global Trainning Accurancy: 0.1679464564912993
Global Trainning Loss: 2.266019821166992
Global test accurancy: 0.15382436801642296
Global test_loss: 2.2793328619003295
Global Precision: 0.13727533255108895
Global Recall: 0.15382436801642296
Global f1score: 0.13222796961092095
50
50
number of selected users 50
Global Trainning Accurancy: 0.16789433272918577
Global Trainning Loss: 2.2657412338256835
Global test accurancy: 0.15431663357895445
Global test_loss: 2.279253029823303
Global Precision: 0.13856313282463217
Global Recall: 0.15431663357895445
Global f1score: 0.13294612156724286
50
50
number of selected users 50
Global Trainning Accurancy: 0.1678972937742819
Global Trainning Loss: 2.265446972846985
Global test accurancy: 0.15381707271590653
Global test_loss: 2.2791695594787598
Global Precision: 0.13842552983607054
Global Recall: 0.15381707271590653
Global f1score: 0.1324504358413442
50
50
number of selected users 50
Global Trainning Accurancy: 0.16882602533995195
Global Trainning Loss: 2.26514732837677
Global test accurancy: 0.15425413693614184
Global test_loss: 2.2790696001052857
Global Precision: 0.13858423170848222
Global Recall: 0.15425413693614184
Global f1score: 0.13299515159532826
50
50
number of selected users 50
Global Trainning Accurancy: 0.16865992829143683
Global Trainning Loss: 2.264867315292358
Global test accurancy: 0.15449414192095215
Global test_loss: 2.278980312347412
Global Precision: 0.14268949734419936
Global Recall: 0.15449414192095215
Global f1score: 0.13364108191603918
50
50
number of selected users 50
Global Trainning Accurancy: 0.1691741985189416
Global Trainning Loss: 2.264584164619446
Global test accurancy: 0.1554633510968385
Global test_loss: 2.2788828802108765
Global Precision: 0.14307574458026878
Global Recall: 0.1554633510968385
Global f1score: 0.13442070632659925
50
50
number of selected users 50
Global Trainning Accurancy: 0.16886859413204308
Global Trainning Loss: 2.2642472267150877
Global test accurancy: 0.15566799912031623
Global test_loss: 2.2787219285964966
Global Precision: 0.14086034915058832
Global Recall: 0.15566799912031623
Global f1score: 0.13465325185236393
50
50
number of selected users 50
Global Trainning Accurancy: 0.16929536552849928
Global Trainning Loss: 2.2639220571517944
Global test accurancy: 0.15682173321186443
Global test_loss: 2.2785786771774292
Global Precision: 0.143762486058448
Global Recall: 0.15682173321186443
Global f1score: 0.1358012123511062
50
50
number of selected users 50
Global Trainning Accurancy: 0.16880110324394984
Global Trainning Loss: 2.2636054801940917
Global test accurancy: 0.15813734959494075
Global test_loss: 2.278460807800293
Global Precision: 0.14572440247362226
Global Recall: 0.15813734959494075
Global f1score: 0.13731075691160358
50
50
number of selected users 50
Global Trainning Accurancy: 0.16910697014151457
Global Trainning Loss: 2.263271255493164
Global test accurancy: 0.1579573282680662
Global test_loss: 2.278328104019165
Global Precision: 0.14970479273601234
Global Recall: 0.1579573282680662
Global f1score: 0.13758200473442056
50
50
number of selected users 50
Global Trainning Accurancy: 0.1693859142026072
Global Trainning Loss: 2.262951865196228
Global test accurancy: 0.15751947273090303
Global test_loss: 2.278228988647461
Global Precision: 0.15488195958577483
Global Recall: 0.15751947273090303
Global f1score: 0.13818393271423063
50
50
number of selected users 50
Global Trainning Accurancy: 0.1698410308890394
Global Trainning Loss: 2.2626426601409912
Global test accurancy: 0.15895053555390587
Global test_loss: 2.278152813911438
Global Precision: 0.15611804315794958
Global Recall: 0.15895053555390587
Global f1score: 0.13982822164977834
50
50
number of selected users 50
Global Trainning Accurancy: 0.17007418116898265
Global Trainning Loss: 2.2623116874694826
Global test accurancy: 0.15964939079244342
Global test_loss: 2.2780560874938964
Global Precision: 0.1565859818425832
Global Recall: 0.15964939079244342
Global f1score: 0.14023429020304126
50
50
number of selected users 50
Global Trainning Accurancy: 0.16979237132880542
Global Trainning Loss: 2.261933264732361
Global test accurancy: 0.16034614311603249
Global test_loss: 2.277917547225952
Global Precision: 0.15534814460754887
Global Recall: 0.16034614311603249
Global f1score: 0.14142858681871945
50
50
number of selected users 50
Global Trainning Accurancy: 0.17068414413201521
Global Trainning Loss: 2.2615636873245237
Global test accurancy: 0.15964684037999768
Global test_loss: 2.27777973651886
Global Precision: 0.15556620116648173
Global Recall: 0.15964684037999768
Global f1score: 0.14111425724517443
50
50
number of selected users 50
Global Trainning Accurancy: 0.1712567346255689
Global Trainning Loss: 2.261187181472778
Global test accurancy: 0.16157708913628674
Global test_loss: 2.2776427268981934
Global Precision: 0.15696723337872542
Global Recall: 0.16157708913628674
Global f1score: 0.14289635006937135
50
50
number of selected users 50
Global Trainning Accurancy: 0.1710755736312402
Global Trainning Loss: 2.2608147764205935
Global test accurancy: 0.16293342300883748
Global test_loss: 2.277515940666199
Global Precision: 0.16087113504331407
Global Recall: 0.16293342300883748
Global f1score: 0.1446990463163898
50
50
number of selected users 50
Global Trainning Accurancy: 0.1713987274618881
Global Trainning Loss: 2.260458755493164
Global test accurancy: 0.16382229019937622
Global test_loss: 2.277409405708313
Global Precision: 0.16135419017579344
Global Recall: 0.16382229019937622
Global f1score: 0.1456300423950194
50
50
number of selected users 50
Global Trainning Accurancy: 0.17163672987540657
Global Trainning Loss: 2.2600695085525513
Global test accurancy: 0.16192911120836853
Global test_loss: 2.2772733879089357
Global Precision: 0.15859086524857136
Global Recall: 0.16192911120836853
Global f1score: 0.14444509008000225
50
50
number of selected users 50
Global Trainning Accurancy: 0.17130182363066768
Global Trainning Loss: 2.259663257598877
Global test accurancy: 0.16348419439288073
Global test_loss: 2.2771267032623292
Global Precision: 0.16100466876620428
Global Recall: 0.16348419439288073
Global f1score: 0.14623254807214103
50
50
number of selected users 50
Global Trainning Accurancy: 0.17161226589744358
Global Trainning Loss: 2.2592516469955446
Global test accurancy: 0.16327795520368385
Global test_loss: 2.2769777393341064
Global Precision: 0.16117614048882567
Global Recall: 0.16327795520368385
Global f1score: 0.14631655266764862
50
50
number of selected users 50
Global Trainning Accurancy: 0.17186814368636602
Global Trainning Loss: 2.258862853050232
Global test accurancy: 0.16194380967760943
Global test_loss: 2.2768745851516723
Global Precision: 0.1611340063916433
Global Recall: 0.16194380967760943
Global f1score: 0.14537304312977667
50
50
number of selected users 50
Global Trainning Accurancy: 0.17256802212054148
Global Trainning Loss: 2.2584513568878175
Global test accurancy: 0.16239080976736506
Global test_loss: 2.276761255264282
Global Precision: 0.16206192462985602
Global Recall: 0.16239080976736506
Global f1score: 0.1462127492794293
50
50
number of selected users 50
Global Trainning Accurancy: 0.17251413783493513
Global Trainning Loss: 2.2580415630340576
Global test accurancy: 0.16265415611704
Global test_loss: 2.2766603899002074
Global Precision: 0.16193499600437924
Global Recall: 0.16265415611704
Global f1score: 0.14636326050428294
50
50
number of selected users 50
Global Trainning Accurancy: 0.17291199995237141
Global Trainning Loss: 2.2576298761367797
Global test accurancy: 0.16354854325009252
Global test_loss: 2.276567916870117
Global Precision: 0.16020286943387815
Global Recall: 0.16354854325009252
Global f1score: 0.1470237318158441
50
50
number of selected users 50
Global Trainning Accurancy: 0.17458770807492668
Global Trainning Loss: 2.2571995735168455
Global test accurancy: 0.16221711271211584
Global test_loss: 2.2764797019958496
Global Precision: 0.1627736081659712
Global Recall: 0.16221711271211584
Global f1score: 0.14671435657485435
50
50
number of selected users 50
Global Trainning Accurancy: 0.17538123640536687
Global Trainning Loss: 2.2567437219619753
Global test accurancy: 0.16238201080091647
Global test_loss: 2.276374559402466
Global Precision: 0.16531226651059086
Global Recall: 0.16238201080091647
Global f1score: 0.14749707215444127
50
50
number of selected users 50
Global Trainning Accurancy: 0.1753836550218937
Global Trainning Loss: 2.256319670677185
Global test accurancy: 0.16186197207459443
Global test_loss: 2.276297826766968
Global Precision: 0.16346818931262927
Global Recall: 0.16186197207459443
Global f1score: 0.14651986304095876
50
50
number of selected users 50
Global Trainning Accurancy: 0.1750012045265716
Global Trainning Loss: 2.2558688879013062
Global test accurancy: 0.16095073004743723
Global test_loss: 2.276206078529358
Global Precision: 0.16377021695143631
Global Recall: 0.16095073004743723
Global f1score: 0.14617441825637195
50
50
number of selected users 50
Global Trainning Accurancy: 0.1752503194480751
Global Trainning Loss: 2.2554595613479616
Global test accurancy: 0.16054118005762438
Global test_loss: 2.2761562299728393
Global Precision: 0.16260783208704174
Global Recall: 0.16054118005762438
Global f1score: 0.145613487163959
50
50
number of selected users 50
Global Trainning Accurancy: 0.17597313058469924
Global Trainning Loss: 2.2550089883804323
Global test accurancy: 0.1618757453581292
Global test_loss: 2.2761086988449097
Global Precision: 0.16325004534672583
Global Recall: 0.1618757453581292
Global f1score: 0.14694128960919453
50
50
number of selected users 50
Global Trainning Accurancy: 0.17618526646564087
Global Trainning Loss: 2.2545772314071657
Global test accurancy: 0.16203655298875658
Global test_loss: 2.276111660003662
Global Precision: 0.1640912411251229
Global Recall: 0.16203655298875658
Global f1score: 0.1473955768557003
50
50
number of selected users 50
Global Trainning Accurancy: 0.176317495299994
Global Trainning Loss: 2.254097843170166
Global test accurancy: 0.16002080719817555
Global test_loss: 2.2760674715042115
Global Precision: 0.15851784095832017
Global Recall: 0.16002080719817555
Global f1score: 0.14577032450442678
50
50
number of selected users 50
Global Trainning Accurancy: 0.17723233653878576
Global Trainning Loss: 2.2537066745758056
Global test accurancy: 0.15917903922828602
Global test_loss: 2.2761334943771363
Global Precision: 0.15716924375589625
Global Recall: 0.15917903922828602
Global f1score: 0.14511977745341295
50
50
number of selected users 50
Global Trainning Accurancy: 0.17720268566725197
Global Trainning Loss: 2.2532524061203003
Global test accurancy: 0.15892891430942618
Global test_loss: 2.276098756790161
Global Precision: 0.15658088898845848
Global Recall: 0.15892891430942618
Global f1score: 0.14504529186400464
50
50
number of selected users 50
Global Trainning Accurancy: 0.17735727158683345
Global Trainning Loss: 2.2528067684173583
Global test accurancy: 0.15997695880591706
Global test_loss: 2.276134395599365
Global Precision: 0.1612860352523837
Global Recall: 0.15997695880591706
Global f1score: 0.14680306909542087
50
50
number of selected users 50
Global Trainning Accurancy: 0.17704563747469265
Global Trainning Loss: 2.2523707962036132
Global test accurancy: 0.15904724784657318
Global test_loss: 2.276152949333191
Global Precision: 0.1579000691203687
Global Recall: 0.15904724784657318
Global f1score: 0.1458466765016463
50
50
number of selected users 50
Global Trainning Accurancy: 0.17673368330292397
Global Trainning Loss: 2.251952385902405
Global test accurancy: 0.15893170765409909
Global test_loss: 2.276174240112305
Global Precision: 0.1601279419064899
Global Recall: 0.15893170765409909
Global f1score: 0.1462652836675092
50
50
number of selected users 50
Global Trainning Accurancy: 0.17703625532072473
Global Trainning Loss: 2.251570019721985
Global test accurancy: 0.15921845530252687
Global test_loss: 2.276283526420593
Global Precision: 0.16145672360575544
Global Recall: 0.15921845530252687
Global f1score: 0.14703180108612507
50
50
number of selected users 50
Global Trainning Accurancy: 0.17623822335133932
Global Trainning Loss: 2.251175661087036
Global test accurancy: 0.16026889989860532
Global test_loss: 2.2763520240783692
Global Precision: 0.16266752224519027
Global Recall: 0.16026889989860532
Global f1score: 0.14822451096204467
50
50
number of selected users 50
Global Trainning Accurancy: 0.1765349757181238
Global Trainning Loss: 2.2508280658721924
Global test accurancy: 0.15981780327822398
Global test_loss: 2.276481409072876
Global Precision: 0.16083339326629337
Global Recall: 0.15981780327822398
Global f1score: 0.14764657812489382
50
50
number of selected users 50
Global Trainning Accurancy: 0.1768408488555123
Global Trainning Loss: 2.250447654724121
Global test accurancy: 0.15982253302974153
Global test_loss: 2.2766199111938477
Global Precision: 0.16048244113865348
Global Recall: 0.15982253302974153
Global f1score: 0.14846072172461125
50
50
number of selected users 50
Global Trainning Accurancy: 0.17635832720406758
Global Trainning Loss: 2.250062427520752
Global test accurancy: 0.1600164351210103
Global test_loss: 2.276754069328308
Global Precision: 0.1603954169490133
Global Recall: 0.1600164351210103
Global f1score: 0.14888797796904346
50
50
number of selected users 50
Global Trainning Accurancy: 0.1772563702878269
Global Trainning Loss: 2.2496995162963866
Global test accurancy: 0.15984384411981312
Global test_loss: 2.276883239746094
Global Precision: 0.15718833203468716
Global Recall: 0.15984384411981312
Global f1score: 0.14788781530814513
50
50
number of selected users 50
Global Trainning Accurancy: 0.17745957853792263
Global Trainning Loss: 2.249294681549072
Global test accurancy: 0.1594932597908247
Global test_loss: 2.2769802236557006
Global Precision: 0.1579082369674976
Global Recall: 0.1594932597908247
Global f1score: 0.14804373269127175
50
50
number of selected users 50
Global Trainning Accurancy: 0.1778695212600869
Global Trainning Loss: 2.248935189247131
Global test accurancy: 0.1585482752178367
Global test_loss: 2.2771151781082155
Global Precision: 0.15761205053166108
Global Recall: 0.1585482752178367
Global f1score: 0.1477140446903381
50
50
number of selected users 50
Global Trainning Accurancy: 0.1778341074101707
Global Trainning Loss: 2.2485260486602785
Global test accurancy: 0.15829123755153096
Global test_loss: 2.2772493171691894
Global Precision: 0.15967374745852175
Global Recall: 0.15829123755153096
Global f1score: 0.1478024067481458
50
50
number of selected users 50
Global Trainning Accurancy: 0.1791465457130759
Global Trainning Loss: 2.2481679821014406
Global test accurancy: 0.15755735363378243
Global test_loss: 2.277429127693176
Global Precision: 0.1601736802467931
Global Recall: 0.15755735363378243
Global f1score: 0.1477245142034956
50
50
number of selected users 50
Global Trainning Accurancy: 0.1793564190736864
Global Trainning Loss: 2.247914490699768
Global test accurancy: 0.15706125878379476
Global test_loss: 2.277748246192932
Global Precision: 0.15673843327827477
Global Recall: 0.15706125878379476
Global f1score: 0.14592149822003445
50
50
number of selected users 50
Global Trainning Accurancy: 0.18001136962485526
Global Trainning Loss: 2.247458510398865
Global test accurancy: 0.15635013385938942
Global test_loss: 2.277803831100464
Global Precision: 0.16019114179894206
Global Recall: 0.15635013385938942
Global f1score: 0.14716805252596948
50
50
number of selected users 50
Global Trainning Accurancy: 0.18061638606308325
Global Trainning Loss: 2.247065315246582
Global test accurancy: 0.15625808413083433
Global test_loss: 2.2778964376449586
Global Precision: 0.16061458251183092
Global Recall: 0.15625808413083433
Global f1score: 0.1471965078233003
50
50
number of selected users 50
Global Trainning Accurancy: 0.1813937257562778
Global Trainning Loss: 2.2466111087799074
Global test accurancy: 0.1553957560448215
Global test_loss: 2.2779750347137453
Global Precision: 0.16047375322409002
Global Recall: 0.1553957560448215
Global f1score: 0.14709516624310584
50
50
number of selected users 50
Global Trainning Accurancy: 0.18132095749250657
Global Trainning Loss: 2.246245093345642
Global test accurancy: 0.15569373359959826
Global test_loss: 2.2781400632858277
Global Precision: 0.164714931584932
Global Recall: 0.15569373359959826
Global f1score: 0.14798886923263443
50
50
number of selected users 50
Global Trainning Accurancy: 0.18192028029687454
Global Trainning Loss: 2.2458323669433593
Global test accurancy: 0.1558628325044299
Global test_loss: 2.2782845497131348
Global Precision: 0.16418519169495419
Global Recall: 0.1558628325044299
Global f1score: 0.1483446159845007
50
50
number of selected users 50
Global Trainning Accurancy: 0.18141862708762432
Global Trainning Loss: 2.2454270410537718
Global test accurancy: 0.15519415893801072
Global test_loss: 2.2784468126296997
Global Precision: 0.1645993695582306
Global Recall: 0.15519415893801072
Global f1score: 0.14799281819827137
50
50
number of selected users 50
Global Trainning Accurancy: 0.18131868325278094
Global Trainning Loss: 2.244941735267639
Global test accurancy: 0.15583428117097448
Global test_loss: 2.2785768270492555
Global Precision: 0.1655973283830906
Global Recall: 0.15583428117097448
Global f1score: 0.14850778863507286
50
50
number of selected users 50
Global Trainning Accurancy: 0.18107258132955145
Global Trainning Loss: 2.2445220041275022
Global test accurancy: 0.15696740135114987
Global test_loss: 2.278815369606018
Global Precision: 0.1694038538774285
Global Recall: 0.15696740135114987
Global f1score: 0.15021958584134046
exp_no  0
0_dataset_CIFAR10_algorithm_FedAvg_model_CNN_3_50_0.6_31_07_2024
