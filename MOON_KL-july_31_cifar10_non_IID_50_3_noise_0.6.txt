============================================================
Summary of training process:
FL Algorithm: MOON_KL
model: CNN
optimizer: SGD
Batch size: 124
Global_iters: 200
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
============================================================
/proj/bhuyan24/fed-divergence
CIFAR10
./data/data/noisy/0.6_50_3/train/cifa_train.json
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:31<1:43:05, 31.08s/it]  1%|          | 2/200 [00:48<1:16:42, 23.25s/it]  2%|▏         | 3/200 [01:06<1:08:09, 20.76s/it]  2%|▏         | 4/200 [01:24<1:04:24, 19.72s/it]  2%|▎         | 5/200 [01:42<1:02:06, 19.11s/it]  3%|▎         | 6/200 [02:00<1:00:30, 18.72s/it]  4%|▎         | 7/200 [02:18<59:24, 18.47s/it]    4%|▍         | 8/200 [02:36<58:28, 18.27s/it]  4%|▍         | 9/200 [02:54<57:54, 18.19s/it]  5%|▌         | 10/200 [03:12<57:14, 18.08s/it]  6%|▌         | 11/200 [03:30<56:43, 18.01s/it]  6%|▌         | 12/200 [03:48<56:15, 17.96s/it]  6%|▋         | 13/200 [04:05<55:51, 17.92s/it]  7%|▋         | 14/200 [04:23<55:29, 17.90s/it]  8%|▊         | 15/200 [04:41<55:04, 17.86s/it]  8%|▊         | 16/200 [04:59<54:42, 17.84s/it]  8%|▊         | 17/200 [05:17<54:24, 17.84s/it]  9%|▉         | 18/200 [05:35<54:11, 17.87s/it] 10%|▉         | 19/200 [05:52<53:50, 17.85s/it] 10%|█         | 20/200 [06:10<53:32, 17.85s/it] 10%|█         | 21/200 [06:28<53:11, 17.83s/it] 11%|█         | 22/200 [06:46<52:53, 17.83s/it] 12%|█▏        | 23/200 [07:04<52:34, 17.82s/it] 12%|█▏        | 24/200 [07:21<52:16, 17.82s/it] 12%|█▎        | 25/200 [07:39<52:03, 17.85s/it] 13%|█▎        | 26/200 [07:57<51:48, 17.87s/it] 14%|█▎        | 27/200 [08:15<51:31, 17.87s/it] 14%|█▍        | 28/200 [08:33<51:13, 17.87s/it] 14%|█▍        | 29/200 [08:51<50:58, 17.89s/it] 15%|█▌        | 30/200 [09:09<50:45, 17.91s/it] 16%|█▌        | 31/200 [09:27<50:33, 17.95s/it] 16%|█▌        | 32/200 [09:45<50:19, 17.97s/it] 16%|█▋        | 33/200 [10:03<50:08, 18.02s/it] 17%|█▋        | 34/200 [10:21<50:01, 18.08s/it] 18%|█▊        | 35/200 [10:40<49:59, 18.18s/it] 18%|█▊        | 36/200 [10:58<49:55, 18.26s/it] 18%|█▊        | 37/200 [11:17<49:50, 18.35s/it] 19%|█▉        | 38/200 [11:35<49:44, 18.42s/it] 20%|█▉        | 39/200 [11:54<49:37, 18.50s/it] 20%|██        | 40/200 [12:13<49:24, 18.53s/it] 20%|██        | 41/200 [12:31<49:07, 18.54s/it] 21%|██        | 42/200 [12:50<48:52, 18.56s/it] 22%|██▏       | 43/200 [13:08<48:38, 18.59s/it] 22%|██▏       | 44/200 [13:27<48:27, 18.64s/it] 22%|██▎       | 45/200 [13:46<48:11, 18.66s/it] 23%|██▎       | 46/200 [14:05<47:52, 18.65s/it] 24%|██▎       | 47/200 [14:23<47:25, 18.60s/it] 24%|██▍       | 48/200 [14:41<46:54, 18.52s/it] 24%|██▍       | 49/200 [15:00<46:20, 18.41s/it] 25%|██▌       | 50/200 [15:18<45:50, 18.34s/it] 26%|██▌       | 51/200 [15:36<45:20, 18.26s/it] 26%|██▌       | 52/200 [15:54<44:47, 18.16s/it] 26%|██▋       | 53/200 [16:12<44:16, 18.07s/it] 27%|██▋       | 54/200 [16:30<43:53, 18.04s/it] 28%|██▊       | 55/200 [16:48<43:37, 18.05s/it] 28%|██▊       | 56/200 [17:06<43:15, 18.03s/it] 28%|██▊       | 57/200 [17:23<42:51, 17.98s/it] 29%|██▉       | 58/200 [17:41<42:21, 17.90s/it] 30%|██▉       | 59/200 [17:59<41:53, 17.82s/it] 30%|███       | 60/200 [18:16<41:26, 17.76s/it] 30%|███       | 61/200 [18:34<41:03, 17.72s/it] 31%|███       | 62/200 [18:52<40:39, 17.68s/it] 32%|███▏      | 63/200 [19:09<40:19, 17.66s/it] 32%|███▏      | 64/200 [19:27<39:56, 17.62s/it] 32%|███▎      | 65/200 [19:44<39:37, 17.61s/it] 33%|███▎      | 66/200 [20:02<39:17, 17.60s/it] 34%|███▎      | 67/200 [20:20<38:59, 17.59s/it] 34%|███▍      | 68/200 [20:37<38:41, 17.59s/it] 34%|███▍      | 69/200 [20:55<38:23, 17.58s/it] 35%|███▌      | 70/200 [21:12<38:04, 17.58s/it] 36%|███▌      | 71/200 [21:30<37:46, 17.57s/it] 36%|███▌      | 72/200 [21:47<37:28, 17.57s/it] 36%|███▋      | 73/200 [22:05<37:15, 17.60s/it] 37%|███▋      | 74/200 [22:23<37:00, 17.62s/it] 38%|███▊      | 75/200 [22:40<36:44, 17.64s/it] 38%|███▊      | 76/200 [22:58<36:29, 17.66s/it] 38%|███▊      | 77/200 [23:16<36:15, 17.68s/it] 39%|███▉      | 78/200 [23:33<35:54, 17.66s/it] 40%|███▉      | 79/200 [23:51<35:37, 17.66s/it] 40%|████      | 80/200 [24:09<35:19, 17.66s/it] 40%|████      | 81/200 [24:26<35:00, 17.65s/it] 41%|████      | 82/200 [24:44<34:42, 17.64s/it] 42%|████▏     | 83/200 [25:02<34:26, 17.66s/it] 42%|████▏     | 84/200 [25:19<34:10, 17.68s/it] 42%|████▎     | 85/200 [25:37<33:56, 17.71s/it] 43%|████▎     | 86/200 [25:55<33:41, 17.73s/it] 44%|████▎     | 87/200 [26:13<33:22, 17.72s/it] 44%|████▍     | 88/200 [26:30<33:06, 17.74s/it] 44%|████▍     | 89/200 [26:48<32:48, 17.73s/it] 45%|████▌     | 90/200 [27:06<32:29, 17.72s/it] 46%|████▌     | 91/200 [27:23<32:07, 17.69s/it] 46%|████▌     | 92/200 [27:41<31:49, 17.68s/it] 46%|████▋     | 93/200 [27:59<31:31, 17.68s/it] 47%|████▋     | 94/200 [28:17<31:15, 17.70s/it] 48%|████▊     | 95/200 [28:34<30:57, 17.69s/it] 48%|████▊     | 96/200 [28:52<30:40, 17.69s/it] 48%|████▊     | 97/200 [29:09<30:17, 17.65s/it] 49%|████▉     | 98/200 [29:27<29:59, 17.64s/it] 50%|████▉     | 99/200 [29:45<29:43, 17.66s/it] 50%|█████     | 100/200 [30:03<29:27, 17.68s/it] 50%|█████     | 101/200 [30:20<29:11, 17.69s/it] 51%|█████     | 102/200 [30:38<28:53, 17.69s/it] 52%|█████▏    | 103/200 [30:56<28:33, 17.66s/it] 52%|█████▏    | 104/200 [31:13<28:13, 17.64s/it] 52%|█████▎    | 105/200 [31:31<27:56, 17.65s/it] 53%|█████▎    | 106/200 [31:48<27:38, 17.65s/it] 54%|█████▎    | 107/200 [32:06<27:19, 17.63s/it] 54%|█████▍    | 108/200 [32:24<27:00, 17.62s/it] 55%|█████▍    | 109/200 [32:41<26:41, 17.59s/it] 55%|█████▌    | 110/200 [32:59<26:20, 17.56s/it] 56%|█████▌    | 111/200 [33:16<26:02, 17.55s/it] 56%|█████▌    | 112/200 [33:34<25:44, 17.55s/it] 56%|█████▋    | 113/200 [33:51<25:24, 17.52s/it] 57%|█████▋    | 114/200 [34:09<25:05, 17.50s/it] 57%|█████▊    | 115/200 [34:26<24:45, 17.48s/it] 58%|█████▊    | 116/200 [34:43<24:27, 17.47s/it] 58%|█████▊    | 117/200 [35:01<24:06, 17.43s/it] 59%|█████▉    | 118/200 [35:18<23:49, 17.43s/it] 60%|█████▉    | 119/200 [35:36<23:32, 17.44s/it] 60%|██████    | 120/200 [35:53<23:14, 17.43s/it] 60%|██████    | 121/200 [36:10<22:55, 17.41s/it] 61%|██████    | 122/200 [36:28<22:36, 17.39s/it] 62%|██████▏   | 123/200 [36:45<22:19, 17.40s/it] 62%|██████▏   | 124/200 [37:03<22:04, 17.42s/it] 62%|██████▎   | 125/200 [37:20<21:46, 17.42s/it] 63%|██████▎   | 126/200 [37:38<21:30, 17.43s/it] 64%|██████▎   | 127/200 [37:55<21:11, 17.42s/it] 64%|██████▍   | 128/200 [38:12<20:52, 17.40s/it] 64%|██████▍   | 129/200 [38:30<20:35, 17.40s/it] 65%|██████▌   | 130/200 [38:47<20:15, 17.37s/it] 66%|██████▌   | 131/200 [39:04<19:56, 17.35s/it] 66%|██████▌   | 132/200 [39:22<19:39, 17.35s/it] 66%|██████▋   | 133/200 [39:39<19:21, 17.34s/it] 67%|██████▋   | 134/200 [39:56<19:03, 17.33s/it] 68%|██████▊   | 135/200 [40:14<18:45, 17.31s/it] 68%|██████▊   | 136/200 [40:31<18:27, 17.30s/it] 68%|██████▊   | 137/200 [40:48<18:08, 17.28s/it] 69%|██████▉   | 138/200 [41:05<17:51, 17.29s/it] 70%|██████▉   | 139/200 [41:23<17:34, 17.29s/it] 70%|███████   | 140/200 [41:40<17:19, 17.32s/it] 70%|███████   | 141/200 [41:57<17:02, 17.33s/it] 71%|███████   | 142/200 [42:15<16:45, 17.34s/it] 72%|███████▏  | 143/200 [42:32<16:29, 17.36s/it] 72%|███████▏  | 144/200 [42:50<16:12, 17.37s/it] 72%|███████▎  | 145/200 [43:07<15:55, 17.37s/it] 73%|███████▎  | 146/200 [43:24<15:37, 17.36s/it] 74%|███████▎  | 147/200 [43:42<15:19, 17.35s/it] 74%|███████▍  | 148/200 [43:59<15:00, 17.31s/it] 74%|███████▍  | 149/200 [44:16<14:42, 17.31s/it] 75%|███████▌  | 150/200 [44:33<14:25, 17.30s/it] 76%|███████▌  | 151/200 [44:51<14:07, 17.30s/it] 76%|███████▌  | 152/200 [45:08<13:50, 17.29s/it] 76%|███████▋  | 153/200 [45:25<13:32, 17.28s/it] 77%|███████▋  | 154/200 [45:42<13:13, 17.25s/it] 78%|███████▊  | 155/200 [46:00<12:56, 17.26s/it] 78%|███████▊  | 156/200 [46:17<12:39, 17.26s/it] 78%|███████▊  | 157/200 [46:34<12:21, 17.26s/it] 79%|███████▉  | 158/200 [46:52<12:04, 17.26s/it] 80%|███████▉  | 159/200 [47:09<11:47, 17.26s/it] 80%|████████  | 160/200 [47:26<11:30, 17.26s/it] 80%|████████  | 161/200 [47:43<11:12, 17.25s/it] 81%|████████  | 162/200 [48:01<10:56, 17.28s/it] 82%|████████▏ | 163/200 [48:18<10:38, 17.27s/it] 82%|████████▏ | 164/200 [48:35<10:21, 17.26s/it] 82%|████████▎ | 165/200 [48:52<10:03, 17.24s/it] 83%|████████▎ | 166/200 [49:10<09:46, 17.24s/it] 84%|████████▎ | 167/200 [49:27<09:28, 17.22s/it] 84%|████████▍ | 168/200 [49:44<09:09, 17.18s/it] 84%|████████▍ | 169/200 [50:01<08:51, 17.16s/it] 85%|████████▌ | 170/200 [50:18<08:35, 17.18s/it] 86%|████████▌ | 171/200 [50:35<08:18, 17.18s/it] 86%|████████▌ | 172/200 [50:52<08:00, 17.17s/it] 86%|████████▋ | 173/200 [51:10<07:42, 17.15s/it] 87%|████████▋ | 174/200 [51:27<07:25, 17.14s/it] 88%|████████▊ | 175/200 [51:44<07:08, 17.15s/it] 88%|████████▊ | 176/200 [52:01<06:51, 17.14s/it] 88%|████████▊ | 177/200 [52:18<06:34, 17.15s/it] 89%|████████▉ | 178/200 [52:35<06:17, 17.15s/it] 90%|████████▉ | 179/200 [52:52<06:00, 17.16s/it] 90%|█████████ | 180/200 [53:10<05:43, 17.16s/it] 90%|█████████ | 181/200 [53:27<05:26, 17.16s/it] 91%|█████████ | 182/200 [53:44<05:09, 17.17s/it] 92%|█████████▏| 183/200 [54:01<04:51, 17.16s/it] 92%|█████████▏| 184/200 [54:18<04:34, 17.14s/it] 92%|█████████▎| 185/200 [54:35<04:16, 17.13s/it] 93%|█████████▎| 186/200 [54:52<03:59, 17.13s/it] 94%|█████████▎| 187/200 [55:09<03:42, 17.11s/it] 94%|█████████▍| 188/200 [55:27<03:25, 17.11s/it] 94%|█████████▍| 189/200 [55:44<03:08, 17.11s/it] 95%|█████████▌| 190/200 [56:01<02:51, 17.12s/it] 96%|█████████▌| 191/200 [56:18<02:34, 17.15s/it] 96%|█████████▌| 192/200 [56:35<02:17, 17.17s/it] 96%|█████████▋| 193/200 [56:53<02:00, 17.18s/it] 97%|█████████▋| 194/200 [57:10<01:43, 17.20s/it] 98%|█████████▊| 195/200 [57:27<01:25, 17.20s/it] 98%|█████████▊| 196/200 [57:44<01:08, 17.19s/it] 98%|█████████▊| 197/200 [58:01<00:51, 17.19s/it] 99%|█████████▉| 198/200 [58:19<00:34, 17.20s/it]100%|█████████▉| 199/200 [58:36<00:17, 17.19s/it]100%|██████████| 200/200 [58:53<00:00, 17.17s/it]100%|██████████| 200/200 [58:53<00:00, 17.67s/it]
50
50
number of selected users 50
Global Trainning Accurancy: 0.09935829097383882
Global Trainning Loss: 2.303054518699646
Global test accurancy: 0.10033084192049224
Global test_loss: 2.3028881120681763
Global Precision: 0.013493352200919096
Global Recall: 0.10033084192049224
Global f1score: 0.023175638335185055
50
50
number of selected users 50
Global Trainning Accurancy: 0.09935829097383882
Global Trainning Loss: 2.3029222774505613
Global test accurancy: 0.10033084192049224
Global test_loss: 2.3027683448791505
Global Precision: 0.013493352200919096
Global Recall: 0.10033084192049224
Global f1score: 0.023175638335185055
50
50
number of selected users 50
Global Trainning Accurancy: 0.09935829097383882
Global Trainning Loss: 2.302800998687744
Global test accurancy: 0.10033084192049224
Global test_loss: 2.3026590585708617
Global Precision: 0.013493352200919096
Global Recall: 0.10033084192049224
Global f1score: 0.023175638335185055
50
50
number of selected users 50
Global Trainning Accurancy: 0.09975197890276094
Global Trainning Loss: 2.302689862251282
Global test accurancy: 0.10061655620620653
Global test_loss: 2.302558283805847
Global Precision: 0.014944090638115188
Global Recall: 0.10061655620620653
Global f1score: 0.02368498577061788
50
50
number of selected users 50
Global Trainning Accurancy: 0.10245007995979877
Global Trainning Loss: 2.3025885915756223
Global test accurancy: 0.10442415038290467
Global test_loss: 2.3024680995941162
Global Precision: 0.031357266181051086
Global Recall: 0.10442415038290467
Global f1score: 0.03984961123422464
50
50
number of selected users 50
Global Trainning Accurancy: 0.10867488760050996
Global Trainning Loss: 2.302495675086975
Global test accurancy: 0.10847054090772221
Global test_loss: 2.3023865270614623
Global Precision: 0.03783581039200879
Global Recall: 0.10847054090772221
Global f1score: 0.04809831603767599
50
50
number of selected users 50
Global Trainning Accurancy: 0.10368169447887637
Global Trainning Loss: 2.30241081237793
Global test accurancy: 0.1022450258535941
Global test_loss: 2.3023117876052854
Global Precision: 0.04697395444492611
Global Recall: 0.1022450258535941
Global f1score: 0.038296102044234746
50
50
number of selected users 50
Global Trainning Accurancy: 0.10322859260621703
Global Trainning Loss: 2.302332739830017
Global test accurancy: 0.10077234644107713
Global test_loss: 2.302244334220886
Global Precision: 0.029701558703222566
Global Recall: 0.10077234644107713
Global f1score: 0.03017623258925089
50
50
number of selected users 50
Global Trainning Accurancy: 0.10210861089878213
Global Trainning Loss: 2.3022595596313478
Global test accurancy: 0.10091250856460904
Global test_loss: 2.3021812868118285
Global Precision: 0.023961548233202186
Global Recall: 0.10091250856460904
Global f1score: 0.026553522241964674
50
50
number of selected users 50
Global Trainning Accurancy: 0.10174718714428996
Global Trainning Loss: 2.302191109657288
Global test accurancy: 0.100616008580725
Global test_loss: 2.30212197303772
Global Precision: 0.01688363243256255
Global Recall: 0.100616008580725
Global f1score: 0.024462019485518006
50
50
number of selected users 50
Global Trainning Accurancy: 0.10140130082076468
Global Trainning Loss: 2.302125024795532
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3020654249191286
Global Precision: 0.01545016519096547
Global Recall: 0.10026513138774254
Global f1score: 0.023852696824325337
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3020618629455565
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3020120573043825
Global Precision: 0.013786696743074945
Global Recall: 0.10026513138774254
Global f1score: 0.023516583042865014
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3020003604888917
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301960973739624
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.30194052696228
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3019105577468872
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3018823432922364
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3018620872497557
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3018262577056885
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301816382408142
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3017721700668337
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3017714548110964
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3017179012298583
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3017265462875365
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301663751602173
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3016805171966555
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301607942581177
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301637444496155
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3015504026412965
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301595015525818
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3014935636520386
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3015526390075682
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301438179016113
Global test accurancy: 0.10026513138774254
Global test_loss: 2.30151282787323
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301385579109192
Global test accurancy: 0.10026513138774254
Global test_loss: 2.30147469997406
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3013341331481936
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301437439918518
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3012832975387574
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3014007902145384
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301232919692993
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301364917755127
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3011819648742677
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3013275241851807
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3011300468444826
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3012880945205687
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3010759210586547
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301247754096985
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.301016159057617
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301203284263611
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3009542751312257
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3011557722091673
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.3008925819396975
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301113724708557
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.300834913253784
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301072154045105
Global Precision: 0.013778306110688917
Global Recall: 0.10026513138774254
Global f1score: 0.023506471665800974
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012419382709639
Global Trainning Loss: 2.300778212547302
Global test accurancy: 0.10026513138774254
Global test_loss: 2.301026310920715
Global Precision: 0.013779073753683747
Global Recall: 0.10026513138774254
Global f1score: 0.02350786744743869
50
50
number of selected users 50
Global Trainning Accurancy: 0.1012778449136928
Global Trainning Loss: 2.3007163619995117
Global test accurancy: 0.10026513138774254
Global test_loss: 2.3009846448898315
Global Precision: 0.013779073753683747
Global Recall: 0.10026513138774254
Global f1score: 0.02350786744743869
50
50
number of selected users 50
Global Trainning Accurancy: 0.10152198628531464
Global Trainning Loss: 2.300644106864929
Global test accurancy: 0.10049768952727743
Global test_loss: 2.3009398365020752
Global Precision: 0.017274831616145032
Global Recall: 0.10049768952727743
Global f1score: 0.02395553662528183
50
50
number of selected users 50
Global Trainning Accurancy: 0.10242249742947528
Global Trainning Loss: 2.3005647611618043
Global test accurancy: 0.10181538383102968
Global test_loss: 2.300873522758484
Global Precision: 0.02902199698720079
Global Recall: 0.10181538383102968
Global f1score: 0.02628334698127311
50
50
number of selected users 50
Global Trainning Accurancy: 0.10308459988552443
Global Trainning Loss: 2.3004604291915896
Global test accurancy: 0.10203682623624967
Global test_loss: 2.3007876253128052
Global Precision: 0.037928493277043576
Global Recall: 0.10203682623624967
Global f1score: 0.02824400663540102
50
50
number of selected users 50
Global Trainning Accurancy: 0.10460712151113002
Global Trainning Loss: 2.300333685874939
Global test accurancy: 0.10253475426390778
Global test_loss: 2.3006808376312256
Global Precision: 0.03946507732912418
Global Recall: 0.10253475426390778
Global f1score: 0.030569431929453193
50
50
number of selected users 50
Global Trainning Accurancy: 0.10666720902678155
Global Trainning Loss: 2.300139856338501
Global test accurancy: 0.10288604874383532
Global test_loss: 2.300509142875671
Global Precision: 0.03729180664628948
Global Recall: 0.10288604874383532
Global f1score: 0.03248152699464193
50
50
number of selected users 50
Global Trainning Accurancy: 0.10980378586364636
Global Trainning Loss: 2.2998497438430787
Global test accurancy: 0.10314745930386168
Global test_loss: 2.3002359962463377
Global Precision: 0.043236022005865925
Global Recall: 0.10314745930386168
Global f1score: 0.03937118714176471
50
50
number of selected users 50
Global Trainning Accurancy: 0.11320052216948309
Global Trainning Loss: 2.299448323249817
Global test accurancy: 0.10617859472533399
Global test_loss: 2.299852385520935
Global Precision: 0.05951935844374012
Global Recall: 0.10617859472533399
Global f1score: 0.048008937821673654
50
50
number of selected users 50
Global Trainning Accurancy: 0.11580291107590389
Global Trainning Loss: 2.298925156593323
Global test accurancy: 0.11305953319659912
Global test_loss: 2.2993611764907835
Global Precision: 0.06196352684781927
Global Recall: 0.11305953319659912
Global f1score: 0.05731912163919613
50
50
number of selected users 50
Global Trainning Accurancy: 0.12187296495005238
Global Trainning Loss: 2.2982940101623535
Global test accurancy: 0.1187525035579171
Global test_loss: 2.298788824081421
Global Precision: 0.06415679467523522
Global Recall: 0.1187525035579171
Global f1score: 0.06401300208246465
50
50
number of selected users 50
Global Trainning Accurancy: 0.12319251813722625
Global Trainning Loss: 2.2976969051361085
Global test accurancy: 0.12486220782447566
Global test_loss: 2.2982579469680786
Global Precision: 0.0783906125098914
Global Recall: 0.12486220782447566
Global f1score: 0.07552719799764476
50
50
number of selected users 50
Global Trainning Accurancy: 0.12541347203145006
Global Trainning Loss: 2.2971851873397826
Global test accurancy: 0.12406904608353475
Global test_loss: 2.2978289318084717
Global Precision: 0.08170469508077743
Global Recall: 0.12406904608353475
Global f1score: 0.08435525963216275
50
50
number of selected users 50
Global Trainning Accurancy: 0.13116908246343278
Global Trainning Loss: 2.296755485534668
Global test accurancy: 0.1326858085623557
Global test_loss: 2.297487053871155
Global Precision: 0.08914521913317937
Global Recall: 0.1326858085623557
Global f1score: 0.09501770713616693
50
50
number of selected users 50
Global Trainning Accurancy: 0.13460189281741292
Global Trainning Loss: 2.2963746404647827
Global test accurancy: 0.13375873234088295
Global test_loss: 2.297190146446228
Global Precision: 0.10089922759339809
Global Recall: 0.13375873234088295
Global f1score: 0.09496010276018257
50
50
number of selected users 50
Global Trainning Accurancy: 0.13433838685930138
Global Trainning Loss: 2.296016802787781
Global test accurancy: 0.134230550094014
Global test_loss: 2.2969181680679323
Global Precision: 0.1044633164936376
Global Recall: 0.134230550094014
Global f1score: 0.09540966603966639
50
50
number of selected users 50
Global Trainning Accurancy: 0.1350921412338507
Global Trainning Loss: 2.2956652641296387
Global test accurancy: 0.13692591744274604
Global test_loss: 2.296647891998291
Global Precision: 0.1034977566194138
Global Recall: 0.13692591744274604
Global f1score: 0.09577670010215532
50
50
number of selected users 50
Global Trainning Accurancy: 0.1369563197689221
Global Trainning Loss: 2.2953116512298584
Global test accurancy: 0.140138489202293
Global test_loss: 2.2963739013671876
Global Precision: 0.11661278259586112
Global Recall: 0.140138489202293
Global f1score: 0.09898298681227173
50
50
number of selected users 50
Global Trainning Accurancy: 0.1374994858793987
Global Trainning Loss: 2.294953951835632
Global test accurancy: 0.14147698695416583
Global test_loss: 2.296096043586731
Global Precision: 0.12175161369066226
Global Recall: 0.14147698695416583
Global f1score: 0.09997995202775233
50
50
number of selected users 50
Global Trainning Accurancy: 0.13808600553012912
Global Trainning Loss: 2.294587574005127
Global test accurancy: 0.14396204417724337
Global test_loss: 2.2958107662200926
Global Precision: 0.12427441852054778
Global Recall: 0.14396204417724337
Global f1score: 0.10201361720611322
50
50
number of selected users 50
Global Trainning Accurancy: 0.13904312599595248
Global Trainning Loss: 2.2942198276519776
Global test accurancy: 0.1436648842736353
Global test_loss: 2.2955276107788087
Global Precision: 0.12333162818944314
Global Recall: 0.1436648842736353
Global f1score: 0.10138648849184193
50
50
number of selected users 50
Global Trainning Accurancy: 0.13968728730379704
Global Trainning Loss: 2.2938467264175415
Global test accurancy: 0.1441813167283715
Global test_loss: 2.2952417087554933
Global Precision: 0.12281494367811009
Global Recall: 0.1441813167283715
Global f1score: 0.10199384327631231
50
50
number of selected users 50
Global Trainning Accurancy: 0.1406296122385153
Global Trainning Loss: 2.293466353416443
Global test accurancy: 0.1440658478580172
Global test_loss: 2.2949490547180176
Global Precision: 0.12098666451401183
Global Recall: 0.1440658478580172
Global f1score: 0.10238380384575889
50
50
number of selected users 50
Global Trainning Accurancy: 0.1418266272332759
Global Trainning Loss: 2.2930850076675413
Global test accurancy: 0.1446123461764043
Global test_loss: 2.294655818939209
Global Precision: 0.1204913020609588
Global Recall: 0.1446123461764043
Global f1score: 0.1034798343466885
50
50
number of selected users 50
Global Trainning Accurancy: 0.14244443635284768
Global Trainning Loss: 2.292698564529419
Global test accurancy: 0.1452634181132668
Global test_loss: 2.294357318878174
Global Precision: 0.11901969217125039
Global Recall: 0.1452634181132668
Global f1score: 0.10378121174821076
50
50
number of selected users 50
Global Trainning Accurancy: 0.14248225550114466
Global Trainning Loss: 2.2923113346099853
Global test accurancy: 0.1457939350029675
Global test_loss: 2.294056911468506
Global Precision: 0.11872538719633298
Global Recall: 0.1457939350029675
Global f1score: 0.10378196996921442
50
50
number of selected users 50
Global Trainning Accurancy: 0.14304964199291997
Global Trainning Loss: 2.2919192361831664
Global test accurancy: 0.14498781242549538
Global test_loss: 2.293752827644348
Global Precision: 0.11211933404251198
Global Recall: 0.14498781242549538
Global f1score: 0.10313987024726774
50
50
number of selected users 50
Global Trainning Accurancy: 0.14348394992438626
Global Trainning Loss: 2.2915236282348634
Global test accurancy: 0.14473760854501644
Global test_loss: 2.29344850063324
Global Precision: 0.11066903718367438
Global Recall: 0.14473760854501644
Global f1score: 0.10331099430561061
50
50
number of selected users 50
Global Trainning Accurancy: 0.14367122947399838
Global Trainning Loss: 2.2911271476745605
Global test accurancy: 0.1436074793379344
Global test_loss: 2.2931427574157714
Global Precision: 0.10681013570201318
Global Recall: 0.1436074793379344
Global f1score: 0.1025416211551709
50
50
number of selected users 50
Global Trainning Accurancy: 0.14326468715990154
Global Trainning Loss: 2.290728783607483
Global test accurancy: 0.14313228783513965
Global test_loss: 2.292835307121277
Global Precision: 0.10183508646146613
Global Recall: 0.14313228783513965
Global f1score: 0.10185997441374352
50
50
number of selected users 50
Global Trainning Accurancy: 0.14447228475341348
Global Trainning Loss: 2.290327067375183
Global test accurancy: 0.14388713826357583
Global test_loss: 2.29252543926239
Global Precision: 0.09856341647321033
Global Recall: 0.14388713826357583
Global f1score: 0.10274850964123994
50
50
number of selected users 50
Global Trainning Accurancy: 0.1455911083320304
Global Trainning Loss: 2.289924855232239
Global test accurancy: 0.14347343789970937
Global test_loss: 2.29221734046936
Global Precision: 0.09827039982122804
Global Recall: 0.14347343789970937
Global f1score: 0.10232859547571009
50
50
number of selected users 50
Global Trainning Accurancy: 0.14603264269551178
Global Trainning Loss: 2.289522576332092
Global test accurancy: 0.1444771423277742
Global test_loss: 2.2919093894958498
Global Precision: 0.09840761041221963
Global Recall: 0.1444771423277742
Global f1score: 0.10311960568440934
50
50
number of selected users 50
Global Trainning Accurancy: 0.14635502504553755
Global Trainning Loss: 2.289114966392517
Global test accurancy: 0.144129443154237
Global test_loss: 2.291596522331238
Global Precision: 0.09737654784002482
Global Recall: 0.144129443154237
Global f1score: 0.10263718593522811
50
50
number of selected users 50
Global Trainning Accurancy: 0.14852108071619244
Global Trainning Loss: 2.288705883026123
Global test accurancy: 0.14560166115945045
Global test_loss: 2.291279273033142
Global Precision: 0.09891818561729268
Global Recall: 0.14560166115945045
Global f1score: 0.10398388168982832
50
50
number of selected users 50
Global Trainning Accurancy: 0.14870559778488282
Global Trainning Loss: 2.288296856880188
Global test accurancy: 0.14593399421428987
Global test_loss: 2.2909661960601806
Global Precision: 0.09848142546777802
Global Recall: 0.14593399421428987
Global f1score: 0.1042813649387623
50
50
number of selected users 50
Global Trainning Accurancy: 0.15010222619756047
Global Trainning Loss: 2.287893352508545
Global test accurancy: 0.14540843678707227
Global test_loss: 2.290657629966736
Global Precision: 0.09683040531903507
Global Recall: 0.14540843678707227
Global f1score: 0.10367361561341412
50
50
number of selected users 50
Global Trainning Accurancy: 0.1507171675268898
Global Trainning Loss: 2.2874846839904786
Global test accurancy: 0.14689409653386631
Global test_loss: 2.2903478479385377
Global Precision: 0.100067375056725
Global Recall: 0.14689409653386631
Global f1score: 0.10519985468424085
50
50
number of selected users 50
Global Trainning Accurancy: 0.1507410301799103
Global Trainning Loss: 2.2870812463760375
Global test accurancy: 0.1481866280870928
Global test_loss: 2.2900482177734376
Global Precision: 0.10033180578334595
Global Recall: 0.1481866280870928
Global f1score: 0.1060450451388079
50
50
number of selected users 50
Global Trainning Accurancy: 0.15039979230405245
Global Trainning Loss: 2.2866802978515626
Global test accurancy: 0.1481295398349898
Global test_loss: 2.2897491121292113
Global Precision: 0.09970701740653092
Global Recall: 0.1481295398349898
Global f1score: 0.10554726036103006
50
50
number of selected users 50
Global Trainning Accurancy: 0.15027986663576007
Global Trainning Loss: 2.286281695365906
Global test accurancy: 0.14819681867822468
Global test_loss: 2.289456124305725
Global Precision: 0.10237967846979769
Global Recall: 0.14819681867822468
Global f1score: 0.1064638953801807
50
50
number of selected users 50
Global Trainning Accurancy: 0.1511349983858524
Global Trainning Loss: 2.2858858823776247
Global test accurancy: 0.14863490755232353
Global test_loss: 2.289169640541077
Global Precision: 0.10163834169147179
Global Recall: 0.14863490755232353
Global f1score: 0.10634793873943628
50
50
number of selected users 50
Global Trainning Accurancy: 0.15200016768321123
Global Trainning Loss: 2.2854932975769042
Global test accurancy: 0.148366867836462
Global test_loss: 2.2888873481750487
Global Precision: 0.10134528453275256
Global Recall: 0.148366867836462
Global f1score: 0.10631729212137313
50
50
number of selected users 50
Global Trainning Accurancy: 0.1523318706407763
Global Trainning Loss: 2.2851050090789795
Global test accurancy: 0.14827908905135373
Global test_loss: 2.2886111736297607
Global Precision: 0.1013695131219631
Global Recall: 0.14827908905135373
Global f1score: 0.10633764059357496
50
50
number of selected users 50
Global Trainning Accurancy: 0.15261450531527315
Global Trainning Loss: 2.2847225522994994
Global test accurancy: 0.14840178722314526
Global test_loss: 2.28834228515625
Global Precision: 0.10143403576634323
Global Recall: 0.14840178722314526
Global f1score: 0.10633853301713737
50
50
number of selected users 50
Global Trainning Accurancy: 0.15268622115998856
Global Trainning Loss: 2.2843464374542237
Global test accurancy: 0.14891440590819977
Global test_loss: 2.2880820083618163
Global Precision: 0.10455937399877296
Global Recall: 0.14891440590819977
Global f1score: 0.10687424756576348
50
50
number of selected users 50
Global Trainning Accurancy: 0.1524188505676658
Global Trainning Loss: 2.28397515296936
Global test accurancy: 0.14896438555491837
Global test_loss: 2.2878270387649535
Global Precision: 0.10594573699202879
Global Recall: 0.14896438555491837
Global f1score: 0.10708752191211977
50
50
number of selected users 50
Global Trainning Accurancy: 0.1529561051727195
Global Trainning Loss: 2.2836075401306153
Global test accurancy: 0.14815206555400762
Global test_loss: 2.28757758140564
Global Precision: 0.10418771085109209
Global Recall: 0.14815206555400762
Global f1score: 0.10652620760803624
50
50
number of selected users 50
Global Trainning Accurancy: 0.1529111325493442
Global Trainning Loss: 2.283248324394226
Global test accurancy: 0.1480005521980406
Global test_loss: 2.2873346281051634
Global Precision: 0.1073741392074311
Global Recall: 0.1480005521980406
Global f1score: 0.10690099058360245
50
50
number of selected users 50
Global Trainning Accurancy: 0.15342270924656384
Global Trainning Loss: 2.282894506454468
Global test accurancy: 0.14847103530626304
Global test_loss: 2.2870980167388915
Global Precision: 0.10849201890306004
Global Recall: 0.14847103530626304
Global f1score: 0.10775016328547571
50
50
number of selected users 50
Global Trainning Accurancy: 0.15360836458410804
Global Trainning Loss: 2.2825499057769774
Global test accurancy: 0.14845234669211357
Global test_loss: 2.2868716382980345
Global Precision: 0.10909889941159805
Global Recall: 0.14845234669211357
Global f1score: 0.10791769884829737
50
50
number of selected users 50
Global Trainning Accurancy: 0.15383685904026512
Global Trainning Loss: 2.282204246520996
Global test accurancy: 0.1496916980089414
Global test_loss: 2.286648325920105
Global Precision: 0.11483354635147318
Global Recall: 0.1496916980089414
Global f1score: 0.11022847746108447
50
50
number of selected users 50
Global Trainning Accurancy: 0.15463989285702467
Global Trainning Loss: 2.2818727016448976
Global test accurancy: 0.14901016290446137
Global test_loss: 2.2864375257492067
Global Precision: 0.11475973898600182
Global Recall: 0.14901016290446137
Global f1score: 0.10998377200657572
50
50
number of selected users 50
Global Trainning Accurancy: 0.15485416148353387
Global Trainning Loss: 2.2815404653549196
Global test accurancy: 0.14933344438094154
Global test_loss: 2.286227593421936
Global Precision: 0.12282589407047545
Global Recall: 0.14933344438094154
Global f1score: 0.11120363141176672
50
50
number of selected users 50
Global Trainning Accurancy: 0.1548600233538797
Global Trainning Loss: 2.281209444999695
Global test accurancy: 0.14939427625639923
Global test_loss: 2.2860171031951904
Global Precision: 0.1273615074755478
Global Recall: 0.14939427625639923
Global f1score: 0.1118803441370625
50
50
number of selected users 50
Global Trainning Accurancy: 0.15443928736200618
Global Trainning Loss: 2.2808806562423705
Global test accurancy: 0.15007323170954823
Global test_loss: 2.2858177328109743
Global Precision: 0.1307880742330596
Global Recall: 0.15007323170954823
Global f1score: 0.11262516051029464
50
50
number of selected users 50
Global Trainning Accurancy: 0.15438981982714278
Global Trainning Loss: 2.280547752380371
Global test accurancy: 0.1499670594017013
Global test_loss: 2.285612859725952
Global Precision: 0.13018444439814514
Global Recall: 0.1499670594017013
Global f1score: 0.11262430364330524
50
50
number of selected users 50
Global Trainning Accurancy: 0.15414038843057318
Global Trainning Loss: 2.2802205514907836
Global test accurancy: 0.1498205121764214
Global test_loss: 2.2854195070266723
Global Precision: 0.128164694088312
Global Recall: 0.1498205121764214
Global f1score: 0.11269171067202051
50
50
number of selected users 50
Global Trainning Accurancy: 0.15496257383975057
Global Trainning Loss: 2.279902482032776
Global test accurancy: 0.15003107234200405
Global test_loss: 2.2852386236190796
Global Precision: 0.12885915581290103
Global Recall: 0.15003107234200405
Global f1score: 0.11324917263530702
50
50
number of selected users 50
Global Trainning Accurancy: 0.15460627407601008
Global Trainning Loss: 2.2795983839035032
Global test accurancy: 0.1516262957451925
Global test_loss: 2.285072798728943
Global Precision: 0.13826110881377265
Global Recall: 0.1516262957451925
Global f1score: 0.11594269364593915
50
50
number of selected users 50
Global Trainning Accurancy: 0.15484713481327883
Global Trainning Loss: 2.279299092292786
Global test accurancy: 0.1510986061523851
Global test_loss: 2.28491005897522
Global Precision: 0.13943937078649565
Global Recall: 0.1510986061523851
Global f1score: 0.11646857815926671
50
50
number of selected users 50
Global Trainning Accurancy: 0.15509361692637905
Global Trainning Loss: 2.2790042972564697
Global test accurancy: 0.1513863970482896
Global test_loss: 2.2847546339035034
Global Precision: 0.1374204088774419
Global Recall: 0.1513863970482896
Global f1score: 0.11693291389454483
50
50
number of selected users 50
Global Trainning Accurancy: 0.15491962256719644
Global Trainning Loss: 2.278714418411255
Global test accurancy: 0.1520272960117692
Global test_loss: 2.2846030235290526
Global Precision: 0.13908522456988215
Global Recall: 0.1520272960117692
Global f1score: 0.11835844645318225
50
50
number of selected users 50
Global Trainning Accurancy: 0.1554826486504273
Global Trainning Loss: 2.2784284067153933
Global test accurancy: 0.15335990025958712
Global test_loss: 2.284458541870117
Global Precision: 0.1400447185539293
Global Recall: 0.15335990025958712
Global f1score: 0.12034857750342476
50
50
number of selected users 50
Global Trainning Accurancy: 0.155985188966378
Global Trainning Loss: 2.2781522607803346
Global test accurancy: 0.15346363662269027
Global test_loss: 2.284324355125427
Global Precision: 0.1414696162424001
Global Recall: 0.15346363662269027
Global f1score: 0.12122414196860017
50
50
number of selected users 50
Global Trainning Accurancy: 0.15561431063460537
Global Trainning Loss: 2.2778828811645506
Global test accurancy: 0.15354046074675687
Global test_loss: 2.284197678565979
Global Precision: 0.13841576666320163
Global Recall: 0.15354046074675687
Global f1score: 0.12117203644589643
50
50
number of selected users 50
Global Trainning Accurancy: 0.1553647853727416
Global Trainning Loss: 2.277617835998535
Global test accurancy: 0.15426739520255664
Global test_loss: 2.2840692138671876
Global Precision: 0.13525774405214713
Global Recall: 0.15426739520255664
Global f1score: 0.12147202547247332
50
50
number of selected users 50
Global Trainning Accurancy: 0.15557369884612982
Global Trainning Loss: 2.277351107597351
Global test accurancy: 0.1534592566961982
Global test_loss: 2.283937406539917
Global Precision: 0.13288369644290535
Global Recall: 0.1534592566961982
Global f1score: 0.12113808835905898
50
50
number of selected users 50
Global Trainning Accurancy: 0.15591243522438702
Global Trainning Loss: 2.2770873594284056
Global test accurancy: 0.15351915597189866
Global test_loss: 2.2838066101074217
Global Precision: 0.13243314026441316
Global Recall: 0.15351915597189866
Global f1score: 0.12172345505908382
50
50
number of selected users 50
Global Trainning Accurancy: 0.1555842144506326
Global Trainning Loss: 2.2768125438690188
Global test accurancy: 0.15413895424105167
Global test_loss: 2.2836666011810305
Global Precision: 0.13200363834624548
Global Recall: 0.15413895424105167
Global f1score: 0.12234537695386201
50
50
number of selected users 50
Global Trainning Accurancy: 0.1565049035109414
Global Trainning Loss: 2.276537799835205
Global test accurancy: 0.15512953802466267
Global test_loss: 2.283537201881409
Global Precision: 0.13164565140555132
Global Recall: 0.15512953802466267
Global f1score: 0.12347078266599255
50
50
number of selected users 50
Global Trainning Accurancy: 0.1567010665407125
Global Trainning Loss: 2.2762669944763183
Global test accurancy: 0.15524805014346565
Global test_loss: 2.2834066486358644
Global Precision: 0.13090997533737567
Global Recall: 0.15524805014346565
Global f1score: 0.12389505786372632
50
50
number of selected users 50
Global Trainning Accurancy: 0.1565837420031503
Global Trainning Loss: 2.2759999942779543
Global test accurancy: 0.15546825731562708
Global test_loss: 2.283299741744995
Global Precision: 0.13081022667159287
Global Recall: 0.15546825731562708
Global f1score: 0.12459610822397274
50
50
number of selected users 50
Global Trainning Accurancy: 0.15695300571707896
Global Trainning Loss: 2.2757332515716553
Global test accurancy: 0.15604756328064887
Global test_loss: 2.283190608024597
Global Precision: 0.13071022571519936
Global Recall: 0.15604756328064887
Global f1score: 0.12528989354829076
50
50
number of selected users 50
Global Trainning Accurancy: 0.15691576337302565
Global Trainning Loss: 2.2754679107666016
Global test accurancy: 0.1549739114534781
Global test_loss: 2.283069624900818
Global Precision: 0.12839319756067238
Global Recall: 0.1549739114534781
Global f1score: 0.12478871896737617
50
50
number of selected users 50
Global Trainning Accurancy: 0.15784126595731973
Global Trainning Loss: 2.275207967758179
Global test accurancy: 0.154306626991459
Global test_loss: 2.282967176437378
Global Precision: 0.12880338256555568
Global Recall: 0.154306626991459
Global f1score: 0.12467852125013713
50
50
number of selected users 50
Global Trainning Accurancy: 0.15761275374447514
Global Trainning Loss: 2.274942789077759
Global test accurancy: 0.15420717055702338
Global test_loss: 2.282848291397095
Global Precision: 0.12714598841463876
Global Recall: 0.15420717055702338
Global f1score: 0.1245407365944738
50
50
number of selected users 50
Global Trainning Accurancy: 0.15772995679902818
Global Trainning Loss: 2.274670133590698
Global test accurancy: 0.1545229764726918
Global test_loss: 2.282730803489685
Global Precision: 0.12642436641025656
Global Recall: 0.1545229764726918
Global f1score: 0.12460530878475966
50
50
number of selected users 50
Global Trainning Accurancy: 0.15738800567673245
Global Trainning Loss: 2.274403839111328
Global test accurancy: 0.1545175313235755
Global test_loss: 2.2826086473464966
Global Precision: 0.12533712483303852
Global Recall: 0.1545175313235755
Global f1score: 0.12476138586125599
50
50
number of selected users 50
Global Trainning Accurancy: 0.15777149074289218
Global Trainning Loss: 2.274136209487915
Global test accurancy: 0.15373987212085685
Global test_loss: 2.2824822664260864
Global Precision: 0.12528706698662465
Global Recall: 0.15373987212085685
Global f1score: 0.1241871985891295
50
50
number of selected users 50
Global Trainning Accurancy: 0.15789384021958494
Global Trainning Loss: 2.2738758754730224
Global test accurancy: 0.15364464050555646
Global test_loss: 2.28236870765686
Global Precision: 0.12435817673670534
Global Recall: 0.15364464050555646
Global f1score: 0.1241439979612639
50
50
number of selected users 50
Global Trainning Accurancy: 0.15799662411418594
Global Trainning Loss: 2.2736091947555543
Global test accurancy: 0.15223810646079056
Global test_loss: 2.2822572326660158
Global Precision: 0.12375397337831197
Global Recall: 0.15223810646079056
Global f1score: 0.12367216146868745
50
50
number of selected users 50
Global Trainning Accurancy: 0.1584557769787186
Global Trainning Loss: 2.2733499908447268
Global test accurancy: 0.15297132244762165
Global test_loss: 2.282149872779846
Global Precision: 0.12497214243981847
Global Recall: 0.15297132244762165
Global f1score: 0.1247644517513975
50
50
number of selected users 50
Global Trainning Accurancy: 0.15885241357217977
Global Trainning Loss: 2.27308837890625
Global test accurancy: 0.1524556323005951
Global test_loss: 2.2820399951934816
Global Precision: 0.1248278060015822
Global Recall: 0.1524556323005951
Global f1score: 0.12462267272590834
50
50
number of selected users 50
Global Trainning Accurancy: 0.1594882449014611
Global Trainning Loss: 2.272825684547424
Global test accurancy: 0.15272590257086538
Global test_loss: 2.281927466392517
Global Precision: 0.1250829774531033
Global Recall: 0.15272590257086538
Global f1score: 0.12499285523573797
50
50
number of selected users 50
Global Trainning Accurancy: 0.15977251306238713
Global Trainning Loss: 2.2725620317459105
Global test accurancy: 0.15248921506383492
Global test_loss: 2.281821818351746
Global Precision: 0.12484839916548551
Global Recall: 0.15248921506383492
Global f1score: 0.12494270388085021
50
50
number of selected users 50
Global Trainning Accurancy: 0.15979827607084157
Global Trainning Loss: 2.2722968578338625
Global test accurancy: 0.15364693170161253
Global test_loss: 2.2817150926589966
Global Precision: 0.12637619803393765
Global Recall: 0.15364693170161253
Global f1score: 0.12648638200778128
50
50
number of selected users 50
Global Trainning Accurancy: 0.1603150447763017
Global Trainning Loss: 2.2720347833633423
Global test accurancy: 0.15345197688409662
Global test_loss: 2.2816047525405883
Global Precision: 0.12631493828249804
Global Recall: 0.15345197688409662
Global f1score: 0.1265135575487759
50
50
number of selected users 50
Global Trainning Accurancy: 0.15961712082314858
Global Trainning Loss: 2.2717789888381956
Global test accurancy: 0.15314258888377097
Global test_loss: 2.2815189456939695
Global Precision: 0.12974367910071566
Global Recall: 0.15314258888377097
Global f1score: 0.12686521672437215
50
50
number of selected users 50
Global Trainning Accurancy: 0.16020537002566393
Global Trainning Loss: 2.2715200710296632
Global test accurancy: 0.1518944748261578
Global test_loss: 2.2814165449142454
Global Precision: 0.12927504949874036
Global Recall: 0.1518944748261578
Global f1score: 0.12623040340068814
50
50
number of selected users 50
Global Trainning Accurancy: 0.1602844242612163
Global Trainning Loss: 2.2712515068054198
Global test accurancy: 0.15186337460616092
Global test_loss: 2.2813189458847045
Global Precision: 0.1281602964978615
Global Recall: 0.15186337460616092
Global f1score: 0.1264851042213596
50
50
number of selected users 50
Global Trainning Accurancy: 0.16099796994013774
Global Trainning Loss: 2.270988130569458
Global test accurancy: 0.15225064964049478
Global test_loss: 2.2812200117111208
Global Precision: 0.12742318721648857
Global Recall: 0.15225064964049478
Global f1score: 0.12642808204374542
50
50
number of selected users 50
Global Trainning Accurancy: 0.1609884889453403
Global Trainning Loss: 2.2707180166244507
Global test accurancy: 0.15370757121190337
Global test_loss: 2.2811203145980836
Global Precision: 0.13006271128889224
Global Recall: 0.15370757121190337
Global f1score: 0.12821626821748436
50
50
number of selected users 50
Global Trainning Accurancy: 0.16149068112438095
Global Trainning Loss: 2.270467233657837
Global test accurancy: 0.15366375271050195
Global test_loss: 2.2810453271865843
Global Precision: 0.13426981498677804
Global Recall: 0.15366375271050195
Global f1score: 0.12892121936177792
50
50
number of selected users 50
Global Trainning Accurancy: 0.1619301286532835
Global Trainning Loss: 2.270203881263733
Global test accurancy: 0.15379039568399033
Global test_loss: 2.2809518814086913
Global Precision: 0.13483687908836528
Global Recall: 0.15379039568399033
Global f1score: 0.12958558361576789
50
50
number of selected users 50
Global Trainning Accurancy: 0.1627705042450441
Global Trainning Loss: 2.269933319091797
Global test accurancy: 0.15314679715575993
Global test_loss: 2.280843243598938
Global Precision: 0.13441794119609032
Global Recall: 0.15314679715575993
Global f1score: 0.129122767957397
50
50
number of selected users 50
Global Trainning Accurancy: 0.16295504815773149
Global Trainning Loss: 2.269663257598877
Global test accurancy: 0.1533274111516104
Global test_loss: 2.2807410383224487
Global Precision: 0.13279628771700383
Global Recall: 0.1533274111516104
Global f1score: 0.12906532914531282
50
50
number of selected users 50
Global Trainning Accurancy: 0.16401032661427395
Global Trainning Loss: 2.2693870449066162
Global test accurancy: 0.15408479607268938
Global test_loss: 2.2806357622146605
Global Precision: 0.1338041063612226
Global Recall: 0.15408479607268938
Global f1score: 0.1301017017812286
50
50
number of selected users 50
Global Trainning Accurancy: 0.16398066116146853
Global Trainning Loss: 2.2691131448745727
Global test accurancy: 0.15414053334979888
Global test_loss: 2.2805333709716797
Global Precision: 0.13361221597073017
Global Recall: 0.15414053334979888
Global f1score: 0.13026576325306616
50
50
number of selected users 50
Global Trainning Accurancy: 0.16473655963249376
Global Trainning Loss: 2.2688493394851683
Global test accurancy: 0.15446871044831115
Global test_loss: 2.2804394149780274
Global Precision: 0.13378188771105373
Global Recall: 0.15446871044831115
Global f1score: 0.1306430477038763
50
50
number of selected users 50
Global Trainning Accurancy: 0.16488236358831243
Global Trainning Loss: 2.2685701751708987
Global test accurancy: 0.15457929504995074
Global test_loss: 2.2803145837783814
Global Precision: 0.1343258072390871
Global Recall: 0.15457929504995074
Global f1score: 0.13101497923397826
50
50
number of selected users 50
Global Trainning Accurancy: 0.16532775256687718
Global Trainning Loss: 2.2682974767684936
Global test accurancy: 0.1557081436665313
Global test_loss: 2.2801999711990355
Global Precision: 0.13593292580238767
Global Recall: 0.1557081436665313
Global f1score: 0.13219518855096446
50
50
number of selected users 50
Global Trainning Accurancy: 0.1649445748155668
Global Trainning Loss: 2.2680052423477175
Global test accurancy: 0.1564823614085777
Global test_loss: 2.280077509880066
Global Precision: 0.13742318818941074
Global Recall: 0.1564823614085777
Global f1score: 0.1331181580462374
50
50
number of selected users 50
Global Trainning Accurancy: 0.16534490147212416
Global Trainning Loss: 2.2677063846588137
Global test accurancy: 0.155612276991247
Global test_loss: 2.279931344985962
Global Precision: 0.1372190080839862
Global Recall: 0.155612276991247
Global f1score: 0.13282827182709322
50
50
number of selected users 50
Global Trainning Accurancy: 0.16542626373641367
Global Trainning Loss: 2.2674138927459717
Global test accurancy: 0.1545887355423248
Global test_loss: 2.279800043106079
Global Precision: 0.13644083820365463
Global Recall: 0.1545887355423248
Global f1score: 0.13226899243939128
50
50
number of selected users 50
Global Trainning Accurancy: 0.16627197042651962
Global Trainning Loss: 2.26712562084198
Global test accurancy: 0.1549340562498778
Global test_loss: 2.279690508842468
Global Precision: 0.13855403227435167
Global Recall: 0.1549340562498778
Global f1score: 0.1327560941792141
50
50
number of selected users 50
Global Trainning Accurancy: 0.1666897059944947
Global Trainning Loss: 2.2668373918533327
Global test accurancy: 0.15581659039260493
Global test_loss: 2.2795737552642823
Global Precision: 0.1395386747401653
Global Recall: 0.15581659039260493
Global f1score: 0.13368875699851546
50
50
number of selected users 50
Global Trainning Accurancy: 0.167105129255982
Global Trainning Loss: 2.2665523147583007
Global test accurancy: 0.1552844915361311
Global test_loss: 2.279469223022461
Global Precision: 0.13896878422371478
Global Recall: 0.1552844915361311
Global f1score: 0.13323954225047854
50
50
number of selected users 50
Global Trainning Accurancy: 0.16692207173902596
Global Trainning Loss: 2.2662577867507934
Global test accurancy: 0.1545976820772099
Global test_loss: 2.279360122680664
Global Precision: 0.14299127575151985
Global Recall: 0.1545976820772099
Global f1score: 0.1328007616258147
50
50
number of selected users 50
Global Trainning Accurancy: 0.16746284521422594
Global Trainning Loss: 2.265998501777649
Global test accurancy: 0.15516688399023287
Global test_loss: 2.2792910623550413
Global Precision: 0.14308962364984615
Global Recall: 0.15516688399023287
Global f1score: 0.13317029415911733
50
50
number of selected users 50
Global Trainning Accurancy: 0.16786721226776466
Global Trainning Loss: 2.265699152946472
Global test accurancy: 0.15450760908561237
Global test_loss: 2.2791760301589967
Global Precision: 0.1428977380381873
Global Recall: 0.15450760908561237
Global f1score: 0.13299147798385316
50
50
number of selected users 50
Global Trainning Accurancy: 0.16863959303381457
Global Trainning Loss: 2.265417718887329
Global test accurancy: 0.1555251063170631
Global test_loss: 2.2790870571136477
Global Precision: 0.1439609363755544
Global Recall: 0.1555251063170631
Global f1score: 0.1338229254241805
50
50
number of selected users 50
Global Trainning Accurancy: 0.16909257359767293
Global Trainning Loss: 2.2651328945159914
Global test accurancy: 0.15522618093236104
Global test_loss: 2.2789935159683226
Global Precision: 0.14384862258720987
Global Recall: 0.15522618093236104
Global f1score: 0.1338265678054157
50
50
number of selected users 50
Global Trainning Accurancy: 0.1690488441789078
Global Trainning Loss: 2.2648448514938355
Global test accurancy: 0.1555078441372781
Global test_loss: 2.278899083137512
Global Precision: 0.1441312683680323
Global Recall: 0.1555078441372781
Global f1score: 0.13433105506095736
50
50
number of selected users 50
Global Trainning Accurancy: 0.16903851360625968
Global Trainning Loss: 2.264558401107788
Global test accurancy: 0.15521107762863862
Global test_loss: 2.2787979316711424
Global Precision: 0.14547402585210573
Global Recall: 0.15521107762863862
Global f1score: 0.13430738858904173
50
50
number of selected users 50
Global Trainning Accurancy: 0.16920579196056842
Global Trainning Loss: 2.264245729446411
Global test accurancy: 0.15549128863498965
Global test_loss: 2.2786825466156007
Global Precision: 0.14345025083747015
Global Recall: 0.15549128863498965
Global f1score: 0.1344259208668935
50
50
number of selected users 50
Global Trainning Accurancy: 0.16922014357962106
Global Trainning Loss: 2.2639023160934446
Global test accurancy: 0.1572674413622202
Global test_loss: 2.2785432386398314
Global Precision: 0.14439129121243802
Global Recall: 0.1572674413622202
Global f1score: 0.136316879598718
50
50
number of selected users 50
Global Trainning Accurancy: 0.16868172218223207
Global Trainning Loss: 2.2635664319992066
Global test accurancy: 0.15679450827444028
Global test_loss: 2.2784187746047975
Global Precision: 0.14434709147984473
Global Recall: 0.15679450827444028
Global f1score: 0.13600056148544215
50
50
number of selected users 50
Global Trainning Accurancy: 0.16928935852449567
Global Trainning Loss: 2.26323046207428
Global test accurancy: 0.15859186502717226
Global test_loss: 2.2782918310165403
Global Precision: 0.15045407144334827
Global Recall: 0.15859186502717226
Global f1score: 0.13785943427017358
50
50
number of selected users 50
Global Trainning Accurancy: 0.16925042673099022
Global Trainning Loss: 2.262897324562073
Global test accurancy: 0.15860365142992444
Global test_loss: 2.2781804752349855
Global Precision: 0.15413414768766087
Global Recall: 0.15860365142992444
Global f1score: 0.13831593456840877
50
50
number of selected users 50
Global Trainning Accurancy: 0.16958289828274678
Global Trainning Loss: 2.2625645065307616
Global test accurancy: 0.15993452310683598
Global test_loss: 2.2780598831176757
Global Precision: 0.15675021537610628
Global Recall: 0.15993452310683598
Global f1score: 0.13991191758706004
50
50
number of selected users 50
Global Trainning Accurancy: 0.1700881765521768
Global Trainning Loss: 2.2622318029403687
Global test accurancy: 0.1601207925118715
Global test_loss: 2.2779706716537476
Global Precision: 0.1569083708755555
Global Recall: 0.1601207925118715
Global f1score: 0.1405705799292247
50
50
number of selected users 50
Global Trainning Accurancy: 0.17033289579087654
Global Trainning Loss: 2.261884970664978
Global test accurancy: 0.15982934348545805
Global test_loss: 2.277879672050476
Global Precision: 0.15603983963611456
Global Recall: 0.15982934348545805
Global f1score: 0.14054394304195705
50
50
number of selected users 50
Global Trainning Accurancy: 0.17061536256487003
Global Trainning Loss: 2.2615556621551516
Global test accurancy: 0.15910071455227381
Global test_loss: 2.2778030157089235
Global Precision: 0.15337482195244384
Global Recall: 0.15910071455227381
Global f1score: 0.14022709812345688
50
50
number of selected users 50
Global Trainning Accurancy: 0.17127386869278782
Global Trainning Loss: 2.2612184381484983
Global test accurancy: 0.1617899830803934
Global test_loss: 2.277710614204407
Global Precision: 0.15719945260466456
Global Recall: 0.1617899830803934
Global f1score: 0.1427158190990145
50
50
number of selected users 50
Global Trainning Accurancy: 0.17096526419195857
Global Trainning Loss: 2.260860915184021
Global test accurancy: 0.16228163757726521
Global test_loss: 2.277588038444519
Global Precision: 0.1577086448992424
Global Recall: 0.16228163757726521
Global f1score: 0.14337492316806927
50
50
number of selected users 50
Global Trainning Accurancy: 0.1714077739913893
Global Trainning Loss: 2.260507378578186
Global test accurancy: 0.1630753865979793
Global test_loss: 2.2774623107910155
Global Precision: 0.15853137154618305
Global Recall: 0.1630753865979793
Global f1score: 0.14424944175995338
50
50
number of selected users 50
Global Trainning Accurancy: 0.17110853898562228
Global Trainning Loss: 2.260111141204834
Global test accurancy: 0.16324543938552924
Global test_loss: 2.2773084163665773
Global Precision: 0.1600789765037142
Global Recall: 0.16324543938552924
Global f1score: 0.14519854611144478
50
50
number of selected users 50
Global Trainning Accurancy: 0.17129720155384448
Global Trainning Loss: 2.259730644226074
Global test accurancy: 0.16285203218733418
Global test_loss: 2.277172441482544
Global Precision: 0.15739616991280392
Global Recall: 0.16285203218733418
Global f1score: 0.14502313185077664
50
50
number of selected users 50
Global Trainning Accurancy: 0.1712346918725422
Global Trainning Loss: 2.2593207502365114
Global test accurancy: 0.16358285925665364
Global test_loss: 2.277012438774109
Global Precision: 0.1597271046516455
Global Recall: 0.16358285925665364
Global f1score: 0.14636252197829908
50
50
number of selected users 50
Global Trainning Accurancy: 0.17130355351679574
Global Trainning Loss: 2.2589007234573364
Global test accurancy: 0.16292488229808608
Global test_loss: 2.276865963935852
Global Precision: 0.16217368225864912
Global Recall: 0.16292488229808608
Global f1score: 0.14617909920476527
50
50
number of selected users 50
Global Trainning Accurancy: 0.17194959340542046
Global Trainning Loss: 2.2584968280792235
Global test accurancy: 0.16444126929065433
Global test_loss: 2.276728940010071
Global Precision: 0.16323362032075564
Global Recall: 0.16444126929065433
Global f1score: 0.1476384645202765
50
50
number of selected users 50
Global Trainning Accurancy: 0.17217511125256538
Global Trainning Loss: 2.2580904006958007
Global test accurancy: 0.16226087858096702
Global test_loss: 2.276633539199829
Global Precision: 0.1598195278760372
Global Recall: 0.16226087858096702
Global f1score: 0.14590173447036153
50
50
number of selected users 50
Global Trainning Accurancy: 0.17272634554860125
Global Trainning Loss: 2.257675943374634
Global test accurancy: 0.16326384229748625
Global test_loss: 2.2765306663513183
Global Precision: 0.1625106221119174
Global Recall: 0.16326384229748625
Global f1score: 0.14719266943189194
50
50
number of selected users 50
Global Trainning Accurancy: 0.17388260715530354
Global Trainning Loss: 2.257286648750305
Global test accurancy: 0.16284374736784674
Global test_loss: 2.2764543533325194
Global Precision: 0.16507621200272465
Global Recall: 0.16284374736784674
Global f1score: 0.14756769642463696
50
50
number of selected users 50
Global Trainning Accurancy: 0.1743380872945789
Global Trainning Loss: 2.256845531463623
Global test accurancy: 0.16284867636313796
Global test_loss: 2.276354513168335
Global Precision: 0.1649391487541033
Global Recall: 0.16284867636313796
Global f1score: 0.1473391774938437
50
50
number of selected users 50
Global Trainning Accurancy: 0.1748438783510269
Global Trainning Loss: 2.256422924995422
Global test accurancy: 0.16203035743256683
Global test_loss: 2.2762807035446166
Global Precision: 0.1655354439671351
Global Recall: 0.16203035743256683
Global f1score: 0.14751251847274552
50
50
number of selected users 50
Global Trainning Accurancy: 0.17486288308373665
Global Trainning Loss: 2.2559342050552367
Global test accurancy: 0.16173483782025938
Global test_loss: 2.2761788177490234
Global Precision: 0.1652313604935922
Global Recall: 0.16173483782025938
Global f1score: 0.14748856705159694
50
50
number of selected users 50
Global Trainning Accurancy: 0.17525048606658228
Global Trainning Loss: 2.2555095243453978
Global test accurancy: 0.1617784327230092
Global test_loss: 2.2761324739456175
Global Precision: 0.16518268277095785
Global Recall: 0.1617784327230092
Global f1score: 0.14707134591143
50
50
number of selected users 50
Global Trainning Accurancy: 0.17601559235502032
Global Trainning Loss: 2.2550824594497683
Global test accurancy: 0.16229946489673663
Global test_loss: 2.2760859823226927
Global Precision: 0.16362302761369524
Global Recall: 0.16229946489673663
Global f1score: 0.146997220051853
50
50
number of selected users 50
Global Trainning Accurancy: 0.17682643311390506
Global Trainning Loss: 2.254662446975708
Global test accurancy: 0.16172408167483848
Global test_loss: 2.2760614061355593
Global Precision: 0.16518856882330823
Global Recall: 0.16172408167483848
Global f1score: 0.1469907735094754
50
50
number of selected users 50
Global Trainning Accurancy: 0.17682229442498737
Global Trainning Loss: 2.254243030548096
Global test accurancy: 0.16129082976618225
Global test_loss: 2.276067223548889
Global Precision: 0.16177105694549984
Global Recall: 0.16129082976618225
Global f1score: 0.14644628239461735
50
50
number of selected users 50
Global Trainning Accurancy: 0.17694040709626369
Global Trainning Loss: 2.2537808990478516
Global test accurancy: 0.16117305055688436
Global test_loss: 2.2760204124450683
Global Precision: 0.16097995593251033
Global Recall: 0.16117305055688436
Global f1score: 0.14657241319054995
50
50
number of selected users 50
Global Trainning Accurancy: 0.17731292806293808
Global Trainning Loss: 2.2533302688598633
Global test accurancy: 0.16041616157617472
Global test_loss: 2.2760306644439696
Global Precision: 0.16134994548162232
Global Recall: 0.16041616157617472
Global f1score: 0.14687339530813628
50
50
number of selected users 50
Global Trainning Accurancy: 0.1765943567967375
Global Trainning Loss: 2.2528407526016236
Global test accurancy: 0.16006931267749147
Global test_loss: 2.2760182332992556
Global Precision: 0.161695254059716
Global Recall: 0.16006931267749147
Global f1score: 0.1471081242725554
50
50
number of selected users 50
Global Trainning Accurancy: 0.1772532280767461
Global Trainning Loss: 2.252432131767273
Global test accurancy: 0.15923922309726607
Global test_loss: 2.2760341691970827
Global Precision: 0.16075372736388477
Global Recall: 0.15923922309726607
Global f1score: 0.1466060096608106
50
50
number of selected users 50
Global Trainning Accurancy: 0.17723037632502492
Global Trainning Loss: 2.2520274782180785
Global test accurancy: 0.15974464625694793
Global test_loss: 2.2761135625839235
Global Precision: 0.16237998998663278
Global Recall: 0.15974464625694793
Global f1score: 0.14723297077132078
50
50
number of selected users 50
Global Trainning Accurancy: 0.1767841436100679
Global Trainning Loss: 2.2516515588760377
Global test accurancy: 0.15924377413127755
Global test_loss: 2.276219825744629
Global Precision: 0.162104124987574
Global Recall: 0.15924377413127755
Global f1score: 0.14694084464789564
50
50
number of selected users 50
Global Trainning Accurancy: 0.17670790982176426
Global Trainning Loss: 2.2512191104888917
Global test accurancy: 0.160666987359912
Global test_loss: 2.276276569366455
Global Precision: 0.16223139746241727
Global Recall: 0.160666987359912
Global f1score: 0.14810965262888864
50
50
number of selected users 50
Global Trainning Accurancy: 0.17648740026640636
Global Trainning Loss: 2.2508203077316282
Global test accurancy: 0.15919454876867806
Global test_loss: 2.276353607177734
Global Precision: 0.16053398356937973
Global Recall: 0.15919454876867806
Global f1score: 0.14707179951591628
50
50
number of selected users 50
Global Trainning Accurancy: 0.17731236966082986
Global Trainning Loss: 2.250466055870056
Global test accurancy: 0.15832624169625076
Global test_loss: 2.276455054283142
Global Precision: 0.15869302115301614
Global Recall: 0.15832624169625076
Global f1score: 0.14649434750513957
50
50
number of selected users 50
Global Trainning Accurancy: 0.17708317296726844
Global Trainning Loss: 2.2501110458374023
Global test accurancy: 0.15895059337816167
Global test_loss: 2.2765860605239867
Global Precision: 0.15745179140150142
Global Recall: 0.15895059337816167
Global f1score: 0.14714889954200644
50
50
number of selected users 50
Global Trainning Accurancy: 0.17716675373111335
Global Trainning Loss: 2.249809069633484
Global test accurancy: 0.15858729244551728
Global test_loss: 2.2767862939834593
Global Precision: 0.15615092062208544
Global Recall: 0.15858729244551728
Global f1score: 0.146651771746932
50
50
number of selected users 50
Global Trainning Accurancy: 0.17583770162339887
Global Trainning Loss: 2.2494279384613036
Global test accurancy: 0.15843234735008577
Global test_loss: 2.2769219160079954
Global Precision: 0.156854595394967
Global Recall: 0.15843234735008577
Global f1score: 0.14708940948018376
50
50
number of selected users 50
Global Trainning Accurancy: 0.17709578118577077
Global Trainning Loss: 2.249074649810791
Global test accurancy: 0.15862527655366754
Global test_loss: 2.277081265449524
Global Precision: 0.15647177777189503
Global Recall: 0.15862527655366754
Global f1score: 0.14708116283954634
50
50
number of selected users 50
Global Trainning Accurancy: 0.1771943319398842
Global Trainning Loss: 2.2486897563934325
Global test accurancy: 0.15679244176188956
Global test_loss: 2.2772506427764894
Global Precision: 0.15635396027263543
Global Recall: 0.15679244176188956
Global f1score: 0.1460215926675868
50
50
number of selected users 50
Global Trainning Accurancy: 0.17805458768298552
Global Trainning Loss: 2.248361072540283
Global test accurancy: 0.1564081895015087
Global test_loss: 2.2774542474746706
Global Precision: 0.15587858524261528
Global Recall: 0.1564081895015087
Global f1score: 0.14584358750623644
50
50
number of selected users 50
Global Trainning Accurancy: 0.17899198499165883
Global Trainning Loss: 2.2479104661941527
Global test accurancy: 0.15591148375750413
Global test_loss: 2.277513475418091
Global Precision: 0.15748007000998468
Global Recall: 0.15591148375750413
Global f1score: 0.14604275843020922
50
50
number of selected users 50
Global Trainning Accurancy: 0.18017483518181465
Global Trainning Loss: 2.247576885223389
Global test accurancy: 0.15723604830473117
Global test_loss: 2.277661528587341
Global Precision: 0.15752359931546828
Global Recall: 0.15723604830473117
Global f1score: 0.14674193655569223
50
50
number of selected users 50
Global Trainning Accurancy: 0.1811108363992961
Global Trainning Loss: 2.2471187257766725
Global test accurancy: 0.15573840870133784
Global test_loss: 2.2777668380737306
Global Precision: 0.16017157274143914
Global Recall: 0.15573840870133784
Global f1score: 0.14696530176830389
50
50
number of selected users 50
Global Trainning Accurancy: 0.18086488881993928
Global Trainning Loss: 2.2466646814346314
Global test accurancy: 0.15507144230049774
Global test_loss: 2.2778420639038086
Global Precision: 0.15881031611218346
Global Recall: 0.15507144230049774
Global f1score: 0.14639494522747185
50
50
number of selected users 50
Global Trainning Accurancy: 0.18156258347709087
Global Trainning Loss: 2.2462119674682617
Global test accurancy: 0.15663796371950733
Global test_loss: 2.2779485464096068
Global Precision: 0.16693950880879085
Global Recall: 0.15663796371950733
Global f1score: 0.14893539956489382
50
50
number of selected users 50
Global Trainning Accurancy: 0.1814237454888595
Global Trainning Loss: 2.245764412879944
Global test accurancy: 0.1563943797642286
Global test_loss: 2.278102984428406
Global Precision: 0.16858997127751177
Global Recall: 0.1563943797642286
Global f1score: 0.14945633576011758
50
50
number of selected users 50
Global Trainning Accurancy: 0.18124664013711977
Global Trainning Loss: 2.245356206893921
Global test accurancy: 0.15693326122242526
Global test_loss: 2.2783086490631104
Global Precision: 0.16858277684097006
Global Recall: 0.15693326122242526
Global f1score: 0.14992400811065584
50
50
number of selected users 50
Global Trainning Accurancy: 0.1819982074668947
Global Trainning Loss: 2.2449064016342164
Global test accurancy: 0.15608761609009175
Global test_loss: 2.2784216928482057
Global Precision: 0.16668747101802409
Global Recall: 0.15608761609009175
Global f1score: 0.14893531305474325
50
50
number of selected users 50
Global Trainning Accurancy: 0.18254210721149092
Global Trainning Loss: 2.2445261669158936
Global test accurancy: 0.15745063524429015
Global test_loss: 2.278666224479675
Global Precision: 0.1676680416447263
Global Recall: 0.15745063524429015
Global f1score: 0.14989979182530588
exp_no  0
0_dataset_CIFAR10_algorithm_MOON_KL_model_CNN_3_50_0.6_31_07_2024
