============================================================
Summary of training process:
FL Algorithm: FedProx
model: CNN
optimizer: SGD
Batch size: 124
Global_iters: 200
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
Proximal hyperparameter 1.0
============================================================
/proj/bhuyan24/fed-divergence
CIFAR10
./data/data/noisy/0.4_50_3/train/cifa_train.json
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:08<28:31,  8.60s/it]  1%|          | 2/200 [00:11<17:45,  5.38s/it]  2%|▏         | 3/200 [00:14<14:16,  4.35s/it]  2%|▏         | 4/200 [00:17<12:36,  3.86s/it]  2%|▎         | 5/200 [00:21<11:39,  3.59s/it]  3%|▎         | 6/200 [00:24<11:03,  3.42s/it]  4%|▎         | 7/200 [00:27<10:40,  3.32s/it]  4%|▍         | 8/200 [00:30<10:22,  3.24s/it]  4%|▍         | 9/200 [00:33<10:09,  3.19s/it]  5%|▌         | 10/200 [00:36<10:01,  3.16s/it]  6%|▌         | 11/200 [00:39<09:59,  3.17s/it]  6%|▌         | 12/200 [00:42<09:51,  3.15s/it]  6%|▋         | 13/200 [00:45<09:44,  3.12s/it]  7%|▋         | 14/200 [00:48<09:39,  3.11s/it]  8%|▊         | 15/200 [00:52<09:34,  3.11s/it]  8%|▊         | 16/200 [00:55<09:30,  3.10s/it]  8%|▊         | 17/200 [00:58<09:28,  3.11s/it]  9%|▉         | 18/200 [01:01<09:31,  3.14s/it] 10%|▉         | 19/200 [01:04<09:29,  3.15s/it] 10%|█         | 20/200 [01:07<09:23,  3.13s/it] 10%|█         | 21/200 [01:10<09:18,  3.12s/it] 11%|█         | 22/200 [01:13<09:13,  3.11s/it] 12%|█▏        | 23/200 [01:17<09:09,  3.11s/it] 12%|█▏        | 24/200 [01:20<09:06,  3.11s/it] 12%|█▎        | 25/200 [01:23<09:06,  3.13s/it] 13%|█▎        | 26/200 [01:26<09:06,  3.14s/it] 14%|█▎        | 27/200 [01:29<09:04,  3.15s/it] 14%|█▍        | 28/200 [01:32<09:02,  3.16s/it] 14%|█▍        | 29/200 [01:35<09:00,  3.16s/it] 15%|█▌        | 30/200 [01:39<08:57,  3.16s/it] 16%|█▌        | 31/200 [01:42<08:55,  3.17s/it] 16%|█▌        | 32/200 [01:45<08:52,  3.17s/it] 16%|█▋        | 33/200 [01:48<08:51,  3.18s/it] 17%|█▋        | 34/200 [01:51<08:47,  3.18s/it] 18%|█▊        | 35/200 [01:54<08:40,  3.15s/it] 18%|█▊        | 36/200 [01:58<08:33,  3.13s/it] 18%|█▊        | 37/200 [02:01<08:27,  3.11s/it] 19%|█▉        | 38/200 [02:04<08:22,  3.10s/it] 20%|█▉        | 39/200 [02:07<08:17,  3.09s/it] 20%|██        | 40/200 [02:10<08:13,  3.08s/it] 20%|██        | 41/200 [02:13<08:09,  3.08s/it] 21%|██        | 42/200 [02:16<08:06,  3.08s/it] 22%|██▏       | 43/200 [02:19<08:02,  3.08s/it] 22%|██▏       | 44/200 [02:22<07:59,  3.07s/it] 22%|██▎       | 45/200 [02:25<07:55,  3.07s/it] 23%|██▎       | 46/200 [02:28<07:52,  3.07s/it] 24%|██▎       | 47/200 [02:31<07:50,  3.08s/it] 24%|██▍       | 48/200 [02:34<07:48,  3.08s/it] 24%|██▍       | 49/200 [02:38<07:45,  3.09s/it] 25%|██▌       | 50/200 [02:41<07:43,  3.09s/it] 26%|██▌       | 51/200 [02:44<07:40,  3.09s/it] 26%|██▌       | 52/200 [02:47<07:37,  3.09s/it] 26%|██▋       | 53/200 [02:50<07:34,  3.09s/it] 27%|██▋       | 54/200 [02:53<07:31,  3.10s/it] 28%|██▊       | 55/200 [02:56<07:28,  3.09s/it] 28%|██▊       | 56/200 [02:59<07:25,  3.09s/it] 28%|██▊       | 57/200 [03:02<07:22,  3.10s/it] 29%|██▉       | 58/200 [03:05<07:20,  3.10s/it] 30%|██▉       | 59/200 [03:09<07:17,  3.10s/it] 30%|███       | 60/200 [03:12<07:13,  3.10s/it] 30%|███       | 61/200 [03:15<07:10,  3.09s/it] 31%|███       | 62/200 [03:18<07:06,  3.09s/it] 32%|███▏      | 63/200 [03:21<07:02,  3.09s/it] 32%|███▏      | 64/200 [03:24<06:58,  3.08s/it] 32%|███▎      | 65/200 [03:27<06:54,  3.07s/it] 33%|███▎      | 66/200 [03:30<06:51,  3.07s/it] 34%|███▎      | 67/200 [03:33<06:48,  3.07s/it] 34%|███▍      | 68/200 [03:36<06:45,  3.07s/it] 34%|███▍      | 69/200 [03:39<06:42,  3.07s/it] 35%|███▌      | 70/200 [03:42<06:39,  3.07s/it] 36%|███▌      | 71/200 [03:45<06:35,  3.07s/it] 36%|███▌      | 72/200 [03:48<06:32,  3.06s/it] 36%|███▋      | 73/200 [03:51<06:28,  3.06s/it] 37%|███▋      | 74/200 [03:55<06:25,  3.06s/it] 38%|███▊      | 75/200 [03:58<06:22,  3.06s/it] 38%|███▊      | 76/200 [04:01<06:19,  3.06s/it] 38%|███▊      | 77/200 [04:04<06:16,  3.06s/it] 39%|███▉      | 78/200 [04:07<06:13,  3.06s/it] 40%|███▉      | 79/200 [04:10<06:10,  3.06s/it] 40%|████      | 80/200 [04:13<06:06,  3.06s/it] 40%|████      | 81/200 [04:16<06:03,  3.05s/it] 41%|████      | 82/200 [04:19<06:00,  3.05s/it] 42%|████▏     | 83/200 [04:22<05:57,  3.05s/it] 42%|████▏     | 84/200 [04:25<05:54,  3.06s/it] 42%|████▎     | 85/200 [04:28<05:51,  3.06s/it] 43%|████▎     | 86/200 [04:31<05:48,  3.06s/it] 44%|████▎     | 87/200 [04:34<05:45,  3.05s/it] 44%|████▍     | 88/200 [04:37<05:42,  3.06s/it] 44%|████▍     | 89/200 [04:40<05:40,  3.06s/it] 45%|████▌     | 90/200 [04:43<05:36,  3.06s/it] 46%|████▌     | 91/200 [04:47<05:33,  3.06s/it] 46%|████▌     | 92/200 [04:50<05:30,  3.06s/it] 46%|████▋     | 93/200 [04:53<05:27,  3.06s/it] 47%|████▋     | 94/200 [04:56<05:24,  3.06s/it] 48%|████▊     | 95/200 [04:59<05:21,  3.06s/it] 48%|████▊     | 96/200 [05:02<05:17,  3.06s/it] 48%|████▊     | 97/200 [05:05<05:14,  3.06s/it] 49%|████▉     | 98/200 [05:08<05:12,  3.06s/it] 50%|████▉     | 99/200 [05:11<05:09,  3.06s/it] 50%|█████     | 100/200 [05:14<05:05,  3.06s/it] 50%|█████     | 101/200 [05:17<05:02,  3.06s/it] 51%|█████     | 102/200 [05:20<04:59,  3.06s/it] 52%|█████▏    | 103/200 [05:23<04:56,  3.06s/it] 52%|█████▏    | 104/200 [05:26<04:53,  3.06s/it] 52%|█████▎    | 105/200 [05:29<04:50,  3.06s/it] 53%|█████▎    | 106/200 [05:32<04:48,  3.06s/it] 54%|█████▎    | 107/200 [05:36<04:45,  3.07s/it] 54%|█████▍    | 108/200 [05:39<04:42,  3.07s/it] 55%|█████▍    | 109/200 [05:42<04:39,  3.07s/it] 55%|█████▌    | 110/200 [05:45<04:36,  3.07s/it] 56%|█████▌    | 111/200 [05:48<04:33,  3.07s/it] 56%|█████▌    | 112/200 [05:51<04:30,  3.07s/it] 56%|█████▋    | 113/200 [05:54<04:26,  3.07s/it] 57%|█████▋    | 114/200 [05:57<04:23,  3.07s/it] 57%|█████▊    | 115/200 [06:00<04:20,  3.07s/it] 58%|█████▊    | 116/200 [06:03<04:17,  3.07s/it] 58%|█████▊    | 117/200 [06:06<04:14,  3.07s/it] 59%|█████▉    | 118/200 [06:09<04:12,  3.08s/it] 60%|█████▉    | 119/200 [06:12<04:09,  3.08s/it] 60%|██████    | 120/200 [06:15<04:05,  3.07s/it] 60%|██████    | 121/200 [06:19<04:03,  3.08s/it] 61%|██████    | 122/200 [06:22<03:59,  3.08s/it] 62%|██████▏   | 123/200 [06:25<03:56,  3.07s/it] 62%|██████▏   | 124/200 [06:28<03:53,  3.07s/it] 62%|██████▎   | 125/200 [06:31<03:49,  3.06s/it] 63%|██████▎   | 126/200 [06:34<03:46,  3.06s/it] 64%|██████▎   | 127/200 [06:37<03:43,  3.06s/it] 64%|██████▍   | 128/200 [06:40<03:40,  3.06s/it] 64%|██████▍   | 129/200 [06:43<03:37,  3.06s/it] 65%|██████▌   | 130/200 [06:46<03:34,  3.06s/it] 66%|██████▌   | 131/200 [06:49<03:31,  3.07s/it] 66%|██████▌   | 132/200 [06:52<03:30,  3.09s/it] 66%|██████▋   | 133/200 [06:55<03:28,  3.12s/it] 67%|██████▋   | 134/200 [06:59<03:26,  3.13s/it] 68%|██████▊   | 135/200 [07:02<03:22,  3.11s/it] 68%|██████▊   | 136/200 [07:05<03:18,  3.10s/it] 68%|██████▊   | 137/200 [07:08<03:14,  3.09s/it] 69%|██████▉   | 138/200 [07:11<03:13,  3.12s/it] 70%|██████▉   | 139/200 [07:14<03:10,  3.13s/it] 70%|███████   | 140/200 [07:17<03:08,  3.14s/it] 70%|███████   | 141/200 [07:21<03:05,  3.14s/it] 71%|███████   | 142/200 [07:24<03:02,  3.15s/it] 72%|███████▏  | 143/200 [07:27<02:59,  3.15s/it] 72%|███████▏  | 144/200 [07:30<02:56,  3.16s/it] 72%|███████▎  | 145/200 [07:33<02:53,  3.16s/it] 73%|███████▎  | 146/200 [07:36<02:50,  3.16s/it] 74%|███████▎  | 147/200 [07:39<02:45,  3.12s/it] 74%|███████▍  | 148/200 [07:42<02:41,  3.11s/it] 74%|███████▍  | 149/200 [07:46<02:39,  3.12s/it] 75%|███████▌  | 150/200 [07:49<02:35,  3.10s/it] 76%|███████▌  | 151/200 [07:52<02:31,  3.09s/it] 76%|███████▌  | 152/200 [07:55<02:27,  3.08s/it] 76%|███████▋  | 153/200 [07:58<02:24,  3.07s/it] 77%|███████▋  | 154/200 [08:01<02:21,  3.07s/it] 78%|███████▊  | 155/200 [08:04<02:17,  3.07s/it] 78%|███████▊  | 156/200 [08:07<02:14,  3.06s/it] 78%|███████▊  | 157/200 [08:10<02:11,  3.06s/it] 79%|███████▉  | 158/200 [08:13<02:08,  3.06s/it] 80%|███████▉  | 159/200 [08:16<02:05,  3.06s/it] 80%|████████  | 160/200 [08:19<02:02,  3.06s/it] 80%|████████  | 161/200 [08:22<01:59,  3.06s/it] 81%|████████  | 162/200 [08:25<01:56,  3.06s/it] 82%|████████▏ | 163/200 [08:28<01:53,  3.06s/it] 82%|████████▏ | 164/200 [08:31<01:50,  3.06s/it] 82%|████████▎ | 165/200 [08:35<01:47,  3.06s/it] 83%|████████▎ | 166/200 [08:38<01:44,  3.06s/it] 84%|████████▎ | 167/200 [08:41<01:41,  3.06s/it] 84%|████████▍ | 168/200 [08:44<01:37,  3.06s/it] 84%|████████▍ | 169/200 [08:47<01:34,  3.06s/it] 85%|████████▌ | 170/200 [08:50<01:31,  3.06s/it] 86%|████████▌ | 171/200 [08:53<01:28,  3.05s/it] 86%|████████▌ | 172/200 [08:56<01:25,  3.05s/it] 86%|████████▋ | 173/200 [08:59<01:22,  3.05s/it] 87%|████████▋ | 174/200 [09:02<01:19,  3.05s/it] 88%|████████▊ | 175/200 [09:05<01:16,  3.06s/it] 88%|████████▊ | 176/200 [09:08<01:14,  3.09s/it] 88%|████████▊ | 177/200 [09:11<01:11,  3.10s/it] 89%|████████▉ | 178/200 [09:14<01:08,  3.10s/it] 90%|████████▉ | 179/200 [09:18<01:04,  3.09s/it] 90%|█████████ | 180/200 [09:21<01:01,  3.09s/it] 90%|█████████ | 181/200 [09:24<00:58,  3.08s/it] 91%|█████████ | 182/200 [09:27<00:55,  3.08s/it] 92%|█████████▏| 183/200 [09:30<00:52,  3.08s/it] 92%|█████████▏| 184/200 [09:33<00:49,  3.08s/it] 92%|█████████▎| 185/200 [09:36<00:46,  3.08s/it] 93%|█████████▎| 186/200 [09:39<00:43,  3.08s/it] 94%|█████████▎| 187/200 [09:42<00:39,  3.07s/it] 94%|█████████▍| 188/200 [09:45<00:36,  3.07s/it] 94%|█████████▍| 189/200 [09:48<00:33,  3.07s/it] 95%|█████████▌| 190/200 [09:51<00:30,  3.07s/it] 96%|█████████▌| 191/200 [09:54<00:27,  3.07s/it] 96%|█████████▌| 192/200 [09:57<00:24,  3.07s/it] 96%|█████████▋| 193/200 [10:01<00:21,  3.07s/it] 97%|█████████▋| 194/200 [10:04<00:18,  3.07s/it] 98%|█████████▊| 195/200 [10:07<00:15,  3.08s/it] 98%|█████████▊| 196/200 [10:10<00:12,  3.08s/it] 98%|█████████▊| 197/200 [10:13<00:09,  3.08s/it] 99%|█████████▉| 198/200 [10:16<00:06,  3.08s/it]100%|█████████▉| 199/200 [10:19<00:03,  3.07s/it]100%|██████████| 200/200 [10:22<00:00,  3.08s/it]100%|██████████| 200/200 [10:22<00:00,  3.11s/it]
50
50
number of selected users 50
Global Trainning Accurancy: 0.10111646519415564
Global Trainning Loss: 2.3036317825317383
Global test accurancy: 0.09976566423236043
Global test_loss: 2.3036698055267335
Global Precision: 0.0196944641697668
Global Recall: 0.09976566423236043
Global f1score: 0.031088996192530183
50
50
number of selected users 50
Global Trainning Accurancy: 0.10111646519415564
Global Trainning Loss: 2.303034429550171
Global test accurancy: 0.09976566423236043
Global test_loss: 2.3031059646606447
Global Precision: 0.0196944641697668
Global Recall: 0.09976566423236043
Global f1score: 0.031088996192530183
50
50
number of selected users 50
Global Trainning Accurancy: 0.10618277796124562
Global Trainning Loss: 2.302463321685791
Global test accurancy: 0.10558161001187112
Global test_loss: 2.3025619316101076
Global Precision: 0.04394496462780924
Global Recall: 0.10558161001187112
Global f1score: 0.043819360382422574
50
50
number of selected users 50
Global Trainning Accurancy: 0.11642745604443297
Global Trainning Loss: 2.3018925523757936
Global test accurancy: 0.11229471526200636
Global test_loss: 2.302023377418518
Global Precision: 0.04223895812530697
Global Recall: 0.11229471526200636
Global f1score: 0.053595195466733504
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.301307702064514
Global test accurancy: 0.10385790459408586
Global test_loss: 2.301471920013428
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.3007314825057983
Global test accurancy: 0.10385790459408586
Global test_loss: 2.300949730873108
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10438200886507909
Global Trainning Loss: 2.3001341676712035
Global test accurancy: 0.10385790459408586
Global test_loss: 2.3004188251495363
Global Precision: 0.021761514415189788
Global Recall: 0.10385790459408586
Global f1score: 0.03407674295592208
50
50
number of selected users 50
Global Trainning Accurancy: 0.10896463978137869
Global Trainning Loss: 2.2995553970336915
Global test accurancy: 0.10855456453271321
Global test_loss: 2.2999227809906007
Global Precision: 0.06071338545965268
Global Recall: 0.10855456453271321
Global f1score: 0.044693740452879695
50
50
number of selected users 50
Global Trainning Accurancy: 0.12812567426524135
Global Trainning Loss: 2.29906494140625
Global test accurancy: 0.12909211630756054
Global test_loss: 2.299519624710083
Global Precision: 0.05573379062427382
Global Recall: 0.12909211630756054
Global f1score: 0.07243832750782338
50
50
number of selected users 50
Global Trainning Accurancy: 0.11248069093137691
Global Trainning Loss: 2.2988476085662843
Global test accurancy: 0.11289049938128629
Global test_loss: 2.299388289451599
Global Precision: 0.05701985519094416
Global Recall: 0.11289049938128629
Global f1score: 0.044261097386880784
50
50
number of selected users 50
Global Trainning Accurancy: 0.11025140956303454
Global Trainning Loss: 2.29882266998291
Global test accurancy: 0.1104427360643162
Global test_loss: 2.299452404975891
Global Precision: 0.03267497364330619
Global Recall: 0.1104427360643162
Global f1score: 0.0383042675077101
50
50
number of selected users 50
Global Trainning Accurancy: 0.11016131947294446
Global Trainning Loss: 2.2985672664642336
Global test accurancy: 0.1106674551654398
Global test_loss: 2.2992416858673095
Global Precision: 0.038751503602487226
Global Recall: 0.1106674551654398
Global f1score: 0.03874944089960176
50
50
number of selected users 50
Global Trainning Accurancy: 0.11053458647571414
Global Trainning Loss: 2.2978646087646486
Global test accurancy: 0.1110308803960377
Global test_loss: 2.2985356998443605
Global Precision: 0.04774562446133545
Global Recall: 0.1110308803960377
Global f1score: 0.03943926656571915
50
50
number of selected users 50
Global Trainning Accurancy: 0.1473234502022288
Global Trainning Loss: 2.296700944900513
Global test accurancy: 0.1482001947961133
Global test_loss: 2.2972974395751953
Global Precision: 0.07807858253190214
Global Recall: 0.1482001947961133
Global f1score: 0.07966365943537611
50
50
number of selected users 50
Global Trainning Accurancy: 0.14183049359649189
Global Trainning Loss: 2.2956584453582765
Global test accurancy: 0.1439201945668023
Global test_loss: 2.2961124181747437
Global Precision: 0.06001812656251273
Global Recall: 0.1439201945668023
Global f1score: 0.07593339245276287
50
50
number of selected users 50
Global Trainning Accurancy: 0.13288470948807118
Global Trainning Loss: 2.2949733781814574
Global test accurancy: 0.13631433299341467
Global test_loss: 2.2953048658370974
Global Precision: 0.06298226390513365
Global Recall: 0.13631433299341467
Global f1score: 0.07032372437209554
50
50
number of selected users 50
Global Trainning Accurancy: 0.13135203386967198
Global Trainning Loss: 2.2943317794799807
Global test accurancy: 0.13470077249714715
Global test_loss: 2.294602131843567
Global Precision: 0.06334261943735744
Global Recall: 0.13470077249714715
Global f1score: 0.06932508978240567
50
50
number of selected users 50
Global Trainning Accurancy: 0.13344520514602098
Global Trainning Loss: 2.2935634517669676
Global test accurancy: 0.13628768729365262
Global test_loss: 2.29381058216095
Global Precision: 0.06327592272194792
Global Recall: 0.13628768729365262
Global f1score: 0.07073571167649592
50
50
number of selected users 50
Global Trainning Accurancy: 0.13604960204594355
Global Trainning Loss: 2.2927247190475466
Global test accurancy: 0.1384478662289373
Global test_loss: 2.29296217918396
Global Precision: 0.06302121432913231
Global Recall: 0.1384478662289373
Global f1score: 0.07277811681349057
50
50
number of selected users 50
Global Trainning Accurancy: 0.13754653581685253
Global Trainning Loss: 2.291834979057312
Global test accurancy: 0.14012815633246717
Global test_loss: 2.292064862251282
Global Precision: 0.06294804512061429
Global Recall: 0.14012815633246717
Global f1score: 0.07416314405814989
50
50
number of selected users 50
Global Trainning Accurancy: 0.1394270857766741
Global Trainning Loss: 2.290890350341797
Global test accurancy: 0.14155851865901892
Global test_loss: 2.2911101245880126
Global Precision: 0.07301935608549054
Global Recall: 0.14155851865901892
Global f1score: 0.07515657100957016
50
50
number of selected users 50
Global Trainning Accurancy: 0.1413786177976222
Global Trainning Loss: 2.2898997116088866
Global test accurancy: 0.14387489543621199
Global test_loss: 2.2900986099243164
Global Precision: 0.07287569806645805
Global Recall: 0.14387489543621199
Global f1score: 0.07641509768031737
50
50
number of selected users 50
Global Trainning Accurancy: 0.14325520744272152
Global Trainning Loss: 2.288856072425842
Global test accurancy: 0.14624794374189834
Global test_loss: 2.2890392208099364
Global Precision: 0.08752739513278984
Global Recall: 0.14624794374189834
Global f1score: 0.0782536448372267
50
50
number of selected users 50
Global Trainning Accurancy: 0.14436110239448047
Global Trainning Loss: 2.287763319015503
Global test accurancy: 0.14729065773267702
Global test_loss: 2.2879291725158692
Global Precision: 0.0876167169639701
Global Recall: 0.14729065773267702
Global f1score: 0.07848348354557397
50
50
number of selected users 50
Global Trainning Accurancy: 0.14517565705139016
Global Trainning Loss: 2.2866305112838745
Global test accurancy: 0.14859231849004356
Global test_loss: 2.2867809915542603
Global Precision: 0.09909730156101242
Global Recall: 0.14859231849004356
Global f1score: 0.08041665659066471
50
50
number of selected users 50
Global Trainning Accurancy: 0.14632128042700795
Global Trainning Loss: 2.285458722114563
Global test accurancy: 0.14798549922300833
Global test_loss: 2.2855927610397337
Global Precision: 0.09428396954086034
Global Recall: 0.14798549922300833
Global f1score: 0.08001467196756815
50
50
number of selected users 50
Global Trainning Accurancy: 0.1471040769788171
Global Trainning Loss: 2.2842343521118162
Global test accurancy: 0.14827129098036765
Global test_loss: 2.2843514108657836
Global Precision: 0.08369891687218754
Global Recall: 0.14827129098036765
Global f1score: 0.08069463361789085
50
50
number of selected users 50
Global Trainning Accurancy: 0.14786592236952087
Global Trainning Loss: 2.282954468727112
Global test accurancy: 0.14880982955709368
Global test_loss: 2.2830535268783567
Global Precision: 0.08152774323475685
Global Recall: 0.14880982955709368
Global f1score: 0.08099985690736768
50
50
number of selected users 50
Global Trainning Accurancy: 0.1481503870025795
Global Trainning Loss: 2.2816143655776977
Global test accurancy: 0.15101033900077443
Global test_loss: 2.281694283485413
Global Precision: 0.0919043977581645
Global Recall: 0.15101033900077443
Global f1score: 0.08324613619886857
50
50
number of selected users 50
Global Trainning Accurancy: 0.1492574277580816
Global Trainning Loss: 2.280218482017517
Global test accurancy: 0.15101090301479753
Global test_loss: 2.28027747631073
Global Precision: 0.0931913464037015
Global Recall: 0.15101090301479753
Global f1score: 0.08405520612818457
50
50
number of selected users 50
Global Trainning Accurancy: 0.14934004845527077
Global Trainning Loss: 2.278772068023682
Global test accurancy: 0.151208949163173
Global test_loss: 2.2788034868240357
Global Precision: 0.09223410846243309
Global Recall: 0.151208949163173
Global f1score: 0.0842733841780677
50
50
number of selected users 50
Global Trainning Accurancy: 0.1499664502534979
Global Trainning Loss: 2.277258768081665
Global test accurancy: 0.15254482948403028
Global test_loss: 2.277257089614868
Global Precision: 0.08889523592540549
Global Recall: 0.15254482948403028
Global f1score: 0.086273342618691
50
50
number of selected users 50
Global Trainning Accurancy: 0.15172635224143902
Global Trainning Loss: 2.2756918716430663
Global test accurancy: 0.15347332726731627
Global test_loss: 2.2756567668914793
Global Precision: 0.09142453904666385
Global Recall: 0.15347332726731627
Global f1score: 0.08807806941804346
50
50
number of selected users 50
Global Trainning Accurancy: 0.15277202686742658
Global Trainning Loss: 2.2740773582458496
Global test accurancy: 0.15515720945037054
Global test_loss: 2.274008331298828
Global Precision: 0.09716358134093683
Global Recall: 0.15515720945037054
Global f1score: 0.09115078238150615
50
50
number of selected users 50
Global Trainning Accurancy: 0.15347558437185221
Global Trainning Loss: 2.2724250507354737
Global test accurancy: 0.15669955607966699
Global test_loss: 2.272319755554199
Global Precision: 0.11190651710164115
Global Recall: 0.15669955607966699
Global f1score: 0.09476835850747145
50
50
number of selected users 50
Global Trainning Accurancy: 0.15583883941857288
Global Trainning Loss: 2.2707267236709594
Global test accurancy: 0.15906927326289827
Global test_loss: 2.2705862760543822
Global Precision: 0.11842036709113443
Global Recall: 0.15906927326289827
Global f1score: 0.098523154879074
50
50
number of selected users 50
Global Trainning Accurancy: 0.15718966764200099
Global Trainning Loss: 2.269001007080078
Global test accurancy: 0.1606677842877501
Global test_loss: 2.268821120262146
Global Precision: 0.14073757115393576
Global Recall: 0.1606677842877501
Global f1score: 0.10151812950467447
50
50
number of selected users 50
Global Trainning Accurancy: 0.15945861887866647
Global Trainning Loss: 2.267250738143921
Global test accurancy: 0.1616727175916497
Global test_loss: 2.267026400566101
Global Precision: 0.14918713485994398
Global Recall: 0.1616727175916497
Global f1score: 0.10421264055353001
50
50
number of selected users 50
Global Trainning Accurancy: 0.16058802453795026
Global Trainning Loss: 2.2655082607269286
Global test accurancy: 0.16338693771431978
Global test_loss: 2.265236010551453
Global Precision: 0.15071841787876206
Global Recall: 0.16338693771431978
Global f1score: 0.10733794570017897
50
50
number of selected users 50
Global Trainning Accurancy: 0.16166838694252625
Global Trainning Loss: 2.2637554454803466
Global test accurancy: 0.16526818395913528
Global test_loss: 2.2634353494644164
Global Precision: 0.15466594122434085
Global Recall: 0.16526818395913528
Global f1score: 0.11016919624142382
50
50
number of selected users 50
Global Trainning Accurancy: 0.16451423847073285
Global Trainning Loss: 2.2620095300674437
Global test accurancy: 0.16595636175492753
Global test_loss: 2.261640257835388
Global Precision: 0.16414550589392202
Global Recall: 0.16595636175492753
Global f1score: 0.11188694254875074
50
50
number of selected users 50
Global Trainning Accurancy: 0.16604317915085112
Global Trainning Loss: 2.2602913570404053
Global test accurancy: 0.16718839092753748
Global test_loss: 2.25986496925354
Global Precision: 0.18646885165144775
Global Recall: 0.16718839092753748
Global f1score: 0.11574760960433715
50
50
number of selected users 50
Global Trainning Accurancy: 0.16757407687774245
Global Trainning Loss: 2.2585990381240846
Global test accurancy: 0.17017127811373606
Global test_loss: 2.258110857009888
Global Precision: 0.20209990895759933
Global Recall: 0.17017127811373606
Global f1score: 0.12131538989428595
50
50
number of selected users 50
Global Trainning Accurancy: 0.1693616308821921
Global Trainning Loss: 2.2569301891326905
Global test accurancy: 0.1687373652551677
Global test_loss: 2.256375732421875
Global Precision: 0.19750497197681618
Global Recall: 0.1687373652551677
Global f1score: 0.12091784231869318
50
50
number of selected users 50
Global Trainning Accurancy: 0.1702241063504682
Global Trainning Loss: 2.2552990913391113
Global test accurancy: 0.17075679055198423
Global test_loss: 2.254673500061035
Global Precision: 0.19642776799683534
Global Recall: 0.17075679055198423
Global f1score: 0.12525404669189613
50
50
number of selected users 50
Global Trainning Accurancy: 0.1720144807843149
Global Trainning Loss: 2.253719868659973
Global test accurancy: 0.17138301306935758
Global test_loss: 2.253029146194458
Global Precision: 0.1973272503597552
Global Recall: 0.17138301306935758
Global f1score: 0.12686079830722732
50
50
number of selected users 50
Global Trainning Accurancy: 0.1730386766109062
Global Trainning Loss: 2.252185354232788
Global test accurancy: 0.17385396412545823
Global test_loss: 2.251438841819763
Global Precision: 0.20535417423402896
Global Recall: 0.17385396412545823
Global f1score: 0.1307797700057973
50
50
number of selected users 50
Global Trainning Accurancy: 0.17430907634861062
Global Trainning Loss: 2.2506912612915038
Global test accurancy: 0.1752749452421836
Global test_loss: 2.2498862886428834
Global Precision: 0.21168202343002604
Global Recall: 0.1752749452421836
Global f1score: 0.13447735383578302
50
50
number of selected users 50
Global Trainning Accurancy: 0.1750320038429476
Global Trainning Loss: 2.2492393159866335
Global test accurancy: 0.1762834110506349
Global test_loss: 2.2483789110183716
Global Precision: 0.2086179878777793
Global Recall: 0.1762834110506349
Global f1score: 0.136354375246498
50
50
number of selected users 50
Global Trainning Accurancy: 0.17687999470292837
Global Trainning Loss: 2.247825388908386
Global test accurancy: 0.17747267124166635
Global test_loss: 2.246912817955017
Global Precision: 0.22441525752769773
Global Recall: 0.17747267124166635
Global f1score: 0.13909834458616063
50
50
number of selected users 50
Global Trainning Accurancy: 0.17806154245407155
Global Trainning Loss: 2.246432647705078
Global test accurancy: 0.17956016717387246
Global test_loss: 2.245475573539734
Global Precision: 0.22232464818709036
Global Recall: 0.17956016717387246
Global f1score: 0.14223173145180254
50
50
number of selected users 50
Global Trainning Accurancy: 0.1783639872516995
Global Trainning Loss: 2.245067753791809
Global test accurancy: 0.18262178549627003
Global test_loss: 2.244064359664917
Global Precision: 0.22524362268307882
Global Recall: 0.18262178549627003
Global f1score: 0.14721141570564697
50
50
number of selected users 50
Global Trainning Accurancy: 0.18082535277831463
Global Trainning Loss: 2.2437414073944093
Global test accurancy: 0.1834010671569339
Global test_loss: 2.242688522338867
Global Precision: 0.22229438175449562
Global Recall: 0.1834010671569339
Global f1score: 0.14870728983861314
50
50
number of selected users 50
Global Trainning Accurancy: 0.18351720137475092
Global Trainning Loss: 2.2424528884887693
Global test accurancy: 0.1845144022571983
Global test_loss: 2.241354546546936
Global Precision: 0.22319349863756896
Global Recall: 0.1845144022571983
Global f1score: 0.15102465546980287
50
50
number of selected users 50
Global Trainning Accurancy: 0.18472278375393686
Global Trainning Loss: 2.2411639404296877
Global test accurancy: 0.18753730587453252
Global test_loss: 2.2400347900390627
Global Precision: 0.2275928406038804
Global Recall: 0.18753730587453252
Global f1score: 0.15487241227102685
50
50
number of selected users 50
Global Trainning Accurancy: 0.18556959868826803
Global Trainning Loss: 2.2399342393875123
Global test accurancy: 0.18976956398809003
Global test_loss: 2.238757915496826
Global Precision: 0.23358890860817436
Global Recall: 0.18976956398809003
Global f1score: 0.15811088954245175
50
50
number of selected users 50
Global Trainning Accurancy: 0.1865522716941025
Global Trainning Loss: 2.23874719619751
Global test accurancy: 0.18963703597688555
Global test_loss: 2.23750958442688
Global Precision: 0.22876315235483385
Global Recall: 0.18963703597688555
Global f1score: 0.15839128504150843
50
50
number of selected users 50
Global Trainning Accurancy: 0.1867926680789635
Global Trainning Loss: 2.2375861597061157
Global test accurancy: 0.19115449867371015
Global test_loss: 2.23629819393158
Global Precision: 0.22876085279983088
Global Recall: 0.19115449867371015
Global f1score: 0.16053018956861967
50
50
number of selected users 50
Global Trainning Accurancy: 0.18799162262907773
Global Trainning Loss: 2.236441788673401
Global test accurancy: 0.19218811439144537
Global test_loss: 2.2351103115081785
Global Precision: 0.23239841942888115
Global Recall: 0.19218811439144537
Global f1score: 0.16248149374243556
50
50
number of selected users 50
Global Trainning Accurancy: 0.18847464048611626
Global Trainning Loss: 2.2352953958511352
Global test accurancy: 0.19342307973576062
Global test_loss: 2.2339193868637084
Global Precision: 0.23260874194104122
Global Recall: 0.19342307973576062
Global f1score: 0.1644972344403714
50
50
number of selected users 50
Global Trainning Accurancy: 0.18971086443281687
Global Trainning Loss: 2.234240660667419
Global test accurancy: 0.19355509605187618
Global test_loss: 2.232813377380371
Global Precision: 0.2272646122121979
Global Recall: 0.19355509605187618
Global f1score: 0.1653591062061222
50
50
number of selected users 50
Global Trainning Accurancy: 0.19086907278642487
Global Trainning Loss: 2.233163194656372
Global test accurancy: 0.19487616934735658
Global test_loss: 2.2316898488998413
Global Precision: 0.23863625883818096
Global Recall: 0.19487616934735658
Global f1score: 0.1672759320446752
50
50
number of selected users 50
Global Trainning Accurancy: 0.19072987971335306
Global Trainning Loss: 2.232109365463257
Global test accurancy: 0.19801637942174238
Global test_loss: 2.2305908250808715
Global Precision: 0.24134350844204355
Global Recall: 0.19801637942174238
Global f1score: 0.17114410439866903
50
50
number of selected users 50
Global Trainning Accurancy: 0.1914504150770397
Global Trainning Loss: 2.231082491874695
Global test accurancy: 0.19883531391864884
Global test_loss: 2.2295119619369506
Global Precision: 0.24476103110742897
Global Recall: 0.19883531391864884
Global f1score: 0.17225743800824314
50
50
number of selected users 50
Global Trainning Accurancy: 0.19272573322435288
Global Trainning Loss: 2.2300640678405763
Global test accurancy: 0.19933139401355793
Global test_loss: 2.228457112312317
Global Precision: 0.24983817879317113
Global Recall: 0.19933139401355793
Global f1score: 0.17335292057537272
50
50
number of selected users 50
Global Trainning Accurancy: 0.19295134880834294
Global Trainning Loss: 2.2290772771835328
Global test accurancy: 0.2000239352260776
Global test_loss: 2.22743670463562
Global Precision: 0.24903427635761424
Global Recall: 0.2000239352260776
Global f1score: 0.17441087143326034
50
50
number of selected users 50
Global Trainning Accurancy: 0.19430909283028086
Global Trainning Loss: 2.2280857753753662
Global test accurancy: 0.20085559093148272
Global test_loss: 2.2263974571228027
Global Precision: 0.2527280518703024
Global Recall: 0.20085559093148272
Global f1score: 0.17593254590062282
50
50
number of selected users 50
Global Trainning Accurancy: 0.19533750535546854
Global Trainning Loss: 2.2270730113983155
Global test accurancy: 0.202347252255581
Global test_loss: 2.2253415536880494
Global Precision: 0.2599575468147213
Global Recall: 0.202347252255581
Global f1score: 0.17813447611084493
50
50
number of selected users 50
Global Trainning Accurancy: 0.1956872748886117
Global Trainning Loss: 2.226109824180603
Global test accurancy: 0.20329331725580732
Global test_loss: 2.2243675422668456
Global Precision: 0.27064675109922903
Global Recall: 0.20329331725580732
Global f1score: 0.18015560572555925
50
50
number of selected users 50
Global Trainning Accurancy: 0.19604934151886574
Global Trainning Loss: 2.225160098075867
Global test accurancy: 0.20360370822363014
Global test_loss: 2.22340802192688
Global Precision: 0.27394210390236007
Global Recall: 0.20360370822363014
Global f1score: 0.1808003098523053
50
50
number of selected users 50
Global Trainning Accurancy: 0.1968231736760058
Global Trainning Loss: 2.224193630218506
Global test accurancy: 0.20470410083659826
Global test_loss: 2.22245325088501
Global Precision: 0.273695832057873
Global Recall: 0.20470410083659826
Global f1score: 0.18236072372823137
50
50
number of selected users 50
Global Trainning Accurancy: 0.19756618794533576
Global Trainning Loss: 2.223220567703247
Global test accurancy: 0.2060946670171616
Global test_loss: 2.2214834976196287
Global Precision: 0.2732378846671962
Global Recall: 0.2060946670171616
Global f1score: 0.1843287551288614
50
50
number of selected users 50
Global Trainning Accurancy: 0.19905637221399844
Global Trainning Loss: 2.2222279500961304
Global test accurancy: 0.20779390676665954
Global test_loss: 2.220499930381775
Global Precision: 0.28282376713104845
Global Recall: 0.20779390676665954
Global f1score: 0.18645018375050107
50
50
number of selected users 50
Global Trainning Accurancy: 0.20013614521773743
Global Trainning Loss: 2.221265263557434
Global test accurancy: 0.20743299482571237
Global test_loss: 2.219557733535767
Global Precision: 0.28364107354152085
Global Recall: 0.20743299482571237
Global f1score: 0.1869397903259543
50
50
number of selected users 50
Global Trainning Accurancy: 0.20049584032259804
Global Trainning Loss: 2.22029878616333
Global test accurancy: 0.20715508348648487
Global test_loss: 2.2186308670043946
Global Precision: 0.27499737856993045
Global Recall: 0.20715508348648487
Global f1score: 0.18743837055848564
50
50
number of selected users 50
Global Trainning Accurancy: 0.2011974999232667
Global Trainning Loss: 2.219320311546326
Global test accurancy: 0.20862115349352225
Global test_loss: 2.217691264152527
Global Precision: 0.28460307752308667
Global Recall: 0.20862115349352225
Global f1score: 0.18976272969394473
50
50
number of selected users 50
Global Trainning Accurancy: 0.20203430869353767
Global Trainning Loss: 2.218356761932373
Global test accurancy: 0.20923430188741557
Global test_loss: 2.2167677783966067
Global Precision: 0.2899444538677335
Global Recall: 0.20923430188741557
Global f1score: 0.1915485693822064
50
50
number of selected users 50
Global Trainning Accurancy: 0.20282686689384852
Global Trainning Loss: 2.217428045272827
Global test accurancy: 0.20918005192252734
Global test_loss: 2.215885000228882
Global Precision: 0.2896653066995379
Global Recall: 0.20918005192252734
Global f1score: 0.1919842214242706
50
50
number of selected users 50
Global Trainning Accurancy: 0.2028927234649803
Global Trainning Loss: 2.216487617492676
Global test accurancy: 0.20929100139977333
Global test_loss: 2.215001344680786
Global Precision: 0.2894213519249866
Global Recall: 0.20929100139977333
Global f1score: 0.19265287137935952
50
50
number of selected users 50
Global Trainning Accurancy: 0.20372476545721754
Global Trainning Loss: 2.2155393457412718
Global test accurancy: 0.20878642441527778
Global test_loss: 2.214118843078613
Global Precision: 0.28646092375404647
Global Recall: 0.20878642441527778
Global f1score: 0.19257486006741534
50
50
number of selected users 50
Global Trainning Accurancy: 0.20486304522442841
Global Trainning Loss: 2.214612112045288
Global test accurancy: 0.20815181706653177
Global test_loss: 2.2132435178756715
Global Precision: 0.29276610694392174
Global Recall: 0.20815181706653177
Global f1score: 0.19218831345233992
50
50
number of selected users 50
Global Trainning Accurancy: 0.2044940077905564
Global Trainning Loss: 2.2136392831802367
Global test accurancy: 0.20967651678393304
Global test_loss: 2.212322964668274
Global Precision: 0.29185558774573656
Global Recall: 0.20967651678393304
Global f1score: 0.19499352985887394
50
50
number of selected users 50
Global Trainning Accurancy: 0.20540072481134178
Global Trainning Loss: 2.2127110242843626
Global test accurancy: 0.20967939361083776
Global test_loss: 2.21145432472229
Global Precision: 0.2915136649164342
Global Recall: 0.20967939361083776
Global f1score: 0.19562795573612657
50
50
number of selected users 50
Global Trainning Accurancy: 0.20593152825558492
Global Trainning Loss: 2.2118384265899658
Global test accurancy: 0.21014593858661415
Global test_loss: 2.210647954940796
Global Precision: 0.2957732756120099
Global Recall: 0.21014593858661415
Global f1score: 0.19726913680148073
50
50
number of selected users 50
Global Trainning Accurancy: 0.2069495038572279
Global Trainning Loss: 2.2109448766708373
Global test accurancy: 0.21191688139901538
Global test_loss: 2.2098236989974978
Global Precision: 0.29551400193659205
Global Recall: 0.21191688139901538
Global f1score: 0.19964205940390597
50
50
number of selected users 50
Global Trainning Accurancy: 0.20734735817557925
Global Trainning Loss: 2.2100662994384765
Global test accurancy: 0.2143708365115291
Global test_loss: 2.209036183357239
Global Precision: 0.30499539963371525
Global Recall: 0.2143708365115291
Global f1score: 0.20339868051029655
50
50
number of selected users 50
Global Trainning Accurancy: 0.20768242512971904
Global Trainning Loss: 2.2091953945159912
Global test accurancy: 0.2163012581236867
Global test_loss: 2.208266897201538
Global Precision: 0.30556100816676807
Global Recall: 0.2163012581236867
Global f1score: 0.20583354077910165
50
50
number of selected users 50
Global Trainning Accurancy: 0.20851252635863632
Global Trainning Loss: 2.208320527076721
Global test accurancy: 0.21792913395978356
Global test_loss: 2.207503905296326
Global Precision: 0.3141078495770264
Global Recall: 0.21792913395978356
Global f1score: 0.20899791544186838
50
50
number of selected users 50
Global Trainning Accurancy: 0.20974498957403556
Global Trainning Loss: 2.2074249458312987
Global test accurancy: 0.217685661304001
Global test_loss: 2.2067300701141357
Global Precision: 0.31573613967542097
Global Recall: 0.217685661304001
Global f1score: 0.20971700701251475
50
50
number of selected users 50
Global Trainning Accurancy: 0.21056844745537764
Global Trainning Loss: 2.206504235267639
Global test accurancy: 0.21732429264282266
Global test_loss: 2.205920767784119
Global Precision: 0.3137507260554564
Global Recall: 0.21732429264282266
Global f1score: 0.2103795987769612
50
50
number of selected users 50
Global Trainning Accurancy: 0.21146880745125143
Global Trainning Loss: 2.205598430633545
Global test accurancy: 0.21860988302071604
Global test_loss: 2.205132508277893
Global Precision: 0.3157232718367053
Global Recall: 0.21860988302071604
Global f1score: 0.21311769150431406
50
50
number of selected users 50
Global Trainning Accurancy: 0.2133298123908688
Global Trainning Loss: 2.20468879699707
Global test accurancy: 0.21786650479909608
Global test_loss: 2.2043466567993164
Global Precision: 0.3169878059817636
Global Recall: 0.21786650479909608
Global f1score: 0.21362265587668225
50
50
number of selected users 50
Global Trainning Accurancy: 0.21455745369081825
Global Trainning Loss: 2.2037781620025636
Global test accurancy: 0.21870934758668714
Global test_loss: 2.203573751449585
Global Precision: 0.3162759617999224
Global Recall: 0.21870934758668714
Global f1score: 0.21471778862104451
50
50
number of selected users 50
Global Trainning Accurancy: 0.2155777824996573
Global Trainning Loss: 2.2028237533569337
Global test accurancy: 0.21776744845496407
Global test_loss: 2.2027561807632448
Global Precision: 0.31593611356741846
Global Recall: 0.21776744845496407
Global f1score: 0.2140186732716341
50
50
number of selected users 50
Global Trainning Accurancy: 0.21601342922108555
Global Trainning Loss: 2.2019062566757204
Global test accurancy: 0.21773621304991791
Global test_loss: 2.201990065574646
Global Precision: 0.31526250976300124
Global Recall: 0.21773621304991791
Global f1score: 0.21450463063043843
50
50
number of selected users 50
Global Trainning Accurancy: 0.21674900555052204
Global Trainning Loss: 2.200969443321228
Global test accurancy: 0.21810280775102941
Global test_loss: 2.2012122011184694
Global Precision: 0.31650524972207417
Global Recall: 0.21810280775102941
Global f1score: 0.215774499208906
50
50
number of selected users 50
Global Trainning Accurancy: 0.21756260448960268
Global Trainning Loss: 2.199998116493225
Global test accurancy: 0.21938091814689784
Global test_loss: 2.2004037141799926
Global Precision: 0.3186487199257398
Global Recall: 0.21938091814689784
Global f1score: 0.21810119790146443
50
50
number of selected users 50
Global Trainning Accurancy: 0.21848085301779904
Global Trainning Loss: 2.199063124656677
Global test accurancy: 0.21970150649303016
Global test_loss: 2.1996383094787597
Global Precision: 0.31576161061299307
Global Recall: 0.21970150649303016
Global f1score: 0.2195839517585899
50
50
number of selected users 50
Global Trainning Accurancy: 0.2202826281278993
Global Trainning Loss: 2.1981142234802244
Global test accurancy: 0.22013190410274858
Global test_loss: 2.1988709115982057
Global Precision: 0.31796354242795405
Global Recall: 0.22013190410274858
Global f1score: 0.22007678223228838
50
50
number of selected users 50
Global Trainning Accurancy: 0.22150640315226464
Global Trainning Loss: 2.1971530961990355
Global test accurancy: 0.22152970964072424
Global test_loss: 2.198075952529907
Global Precision: 0.31471680948678044
Global Recall: 0.22152970964072424
Global f1score: 0.22183815484049152
50
50
number of selected users 50
Global Trainning Accurancy: 0.22170881845651091
Global Trainning Loss: 2.196233701705933
Global test accurancy: 0.22198668320333317
Global test_loss: 2.1973513841629027
Global Precision: 0.3171801454956234
Global Recall: 0.22198668320333317
Global f1score: 0.2239034124382792
50
50
number of selected users 50
Global Trainning Accurancy: 0.22250685689531943
Global Trainning Loss: 2.195330719947815
Global test accurancy: 0.22308408766290927
Global test_loss: 2.1966601467132567
Global Precision: 0.31689346356619236
Global Recall: 0.22308408766290927
Global f1score: 0.225720993356853
50
50
number of selected users 50
Global Trainning Accurancy: 0.22260184199613764
Global Trainning Loss: 2.194383964538574
Global test accurancy: 0.22414492466571867
Global test_loss: 2.1959208917617796
Global Precision: 0.32241538426917854
Global Recall: 0.22414492466571867
Global f1score: 0.22809507872198226
50
50
number of selected users 50
Global Trainning Accurancy: 0.2245180904554731
Global Trainning Loss: 2.193517732620239
Global test accurancy: 0.22447841770506624
Global test_loss: 2.1952817916870115
Global Precision: 0.32332616912405465
Global Recall: 0.22447841770506624
Global f1score: 0.2288941457505056
50
50
number of selected users 50
Global Trainning Accurancy: 0.2258939331554073
Global Trainning Loss: 2.192621264457703
Global test accurancy: 0.22565306701147692
Global test_loss: 2.1946107292175294
Global Precision: 0.3261252830744501
Global Recall: 0.22565306701147692
Global f1score: 0.23115933501183816
50
50
number of selected users 50
Global Trainning Accurancy: 0.22732114213360208
Global Trainning Loss: 2.191734776496887
Global test accurancy: 0.22535121924504628
Global test_loss: 2.1939554738998415
Global Precision: 0.32394085917144744
Global Recall: 0.22535121924504628
Global f1score: 0.23210885196924225
50
50
number of selected users 50
Global Trainning Accurancy: 0.2283975961975652
Global Trainning Loss: 2.1908724451065065
Global test accurancy: 0.22639588972278693
Global test_loss: 2.1933412837982176
Global Precision: 0.326569825406715
Global Recall: 0.22639588972278693
Global f1score: 0.23447898444859838
50
50
number of selected users 50
Global Trainning Accurancy: 0.22987320565560337
Global Trainning Loss: 2.189906578063965
Global test accurancy: 0.22679676302320168
Global test_loss: 2.1926302528381347
Global Precision: 0.32775921365435157
Global Recall: 0.22679676302320168
Global f1score: 0.23599380866771366
50
50
number of selected users 50
Global Trainning Accurancy: 0.22984324369297454
Global Trainning Loss: 2.1890316724777223
Global test accurancy: 0.22788888894228226
Global test_loss: 2.19204026222229
Global Precision: 0.3271648594500464
Global Recall: 0.22788888894228226
Global f1score: 0.23718079493689928
50
50
number of selected users 50
Global Trainning Accurancy: 0.23129780986048257
Global Trainning Loss: 2.1881163692474366
Global test accurancy: 0.22811211957063773
Global test_loss: 2.191417350769043
Global Precision: 0.3276683320287618
Global Recall: 0.22811211957063773
Global f1score: 0.23763641627405424
50
50
number of selected users 50
Global Trainning Accurancy: 0.23259148717916406
Global Trainning Loss: 2.1872206830978396
Global test accurancy: 0.229741138915152
Global test_loss: 2.1908180999755857
Global Precision: 0.3284240760465324
Global Recall: 0.229741138915152
Global f1score: 0.23933687971225187
50
50
number of selected users 50
Global Trainning Accurancy: 0.23341533479351012
Global Trainning Loss: 2.1863280725479126
Global test accurancy: 0.23115407208986932
Global test_loss: 2.190209107398987
Global Precision: 0.32958845747958915
Global Recall: 0.23115407208986932
Global f1score: 0.24076814700726504
50
50
number of selected users 50
Global Trainning Accurancy: 0.23355432618150682
Global Trainning Loss: 2.1854923152923584
Global test accurancy: 0.23207343454388085
Global test_loss: 2.1896616125106814
Global Precision: 0.3299561325141603
Global Recall: 0.23207343454388085
Global f1score: 0.2418321635425321
50
50
number of selected users 50
Global Trainning Accurancy: 0.23429248686433873
Global Trainning Loss: 2.184621477127075
Global test accurancy: 0.23322512186746955
Global test_loss: 2.1890892028808593
Global Precision: 0.3315905611121211
Global Recall: 0.23322512186746955
Global f1score: 0.24316910953011536
50
50
number of selected users 50
Global Trainning Accurancy: 0.23511695448823677
Global Trainning Loss: 2.183902316093445
Global test accurancy: 0.23363160424658244
Global test_loss: 2.188689823150635
Global Precision: 0.3364407874880761
Global Recall: 0.23363160424658244
Global f1score: 0.24415951488869325
50
50
number of selected users 50
Global Trainning Accurancy: 0.23523986199220817
Global Trainning Loss: 2.1831138229370115
Global test accurancy: 0.2346401133991774
Global test_loss: 2.1882068634033205
Global Precision: 0.34326555404912035
Global Recall: 0.2346401133991774
Global f1score: 0.2464815057478314
50
50
number of selected users 50
Global Trainning Accurancy: 0.23518515367575182
Global Trainning Loss: 2.1822956228256225
Global test accurancy: 0.2352852304956779
Global test_loss: 2.1877206993103027
Global Precision: 0.343328287291688
Global Recall: 0.2352852304956779
Global f1score: 0.24713440473417078
50
50
number of selected users 50
Global Trainning Accurancy: 0.2349355290127316
Global Trainning Loss: 2.181492018699646
Global test accurancy: 0.2348511568049727
Global test_loss: 2.187239146232605
Global Precision: 0.3420863015464912
Global Recall: 0.2348511568049727
Global f1score: 0.24742200378683843
50
50
number of selected users 50
Global Trainning Accurancy: 0.23570839397227938
Global Trainning Loss: 2.1807888174057006
Global test accurancy: 0.23521139969953958
Global test_loss: 2.1868757104873655
Global Precision: 0.34091447656856144
Global Recall: 0.23521139969953958
Global f1score: 0.24864100656930738
50
50
number of selected users 50
Global Trainning Accurancy: 0.23649392865823743
Global Trainning Loss: 2.179963483810425
Global test accurancy: 0.23691721512800631
Global test_loss: 2.1864072370529173
Global Precision: 0.3399081830131472
Global Recall: 0.23691721512800631
Global f1score: 0.2500354625769426
50
50
number of selected users 50
Global Trainning Accurancy: 0.23817092531096415
Global Trainning Loss: 2.179221959114075
Global test accurancy: 0.23695326610370437
Global test_loss: 2.1860315704345705
Global Precision: 0.34167189499540485
Global Recall: 0.23695326610370437
Global f1score: 0.25068993839141523
50
50
number of selected users 50
Global Trainning Accurancy: 0.23906417253963272
Global Trainning Loss: 2.1783917570114135
Global test accurancy: 0.2385887774733658
Global test_loss: 2.1855571699142455
Global Precision: 0.33904235442875386
Global Recall: 0.2385887774733658
Global f1score: 0.2526317467855168
50
50
number of selected users 50
Global Trainning Accurancy: 0.2394800874401585
Global Trainning Loss: 2.177617964744568
Global test accurancy: 0.23802556371960362
Global test_loss: 2.185163221359253
Global Precision: 0.33911910267888895
Global Recall: 0.23802556371960362
Global f1score: 0.25258270392892623
50
50
number of selected users 50
Global Trainning Accurancy: 0.24019610604186395
Global Trainning Loss: 2.176864380836487
Global test accurancy: 0.23939848847469386
Global test_loss: 2.1847749519348145
Global Precision: 0.3383148637508964
Global Recall: 0.23939848847469386
Global f1score: 0.2541950543226599
50
50
number of selected users 50
Global Trainning Accurancy: 0.24088325065454175
Global Trainning Loss: 2.176030888557434
Global test accurancy: 0.2397243836882638
Global test_loss: 2.1843246269226073
Global Precision: 0.3392553358368464
Global Recall: 0.2397243836882638
Global f1score: 0.25519172864096534
50
50
number of selected users 50
Global Trainning Accurancy: 0.2415711074207118
Global Trainning Loss: 2.1752601528167723
Global test accurancy: 0.23902242783686026
Global test_loss: 2.1839447975158692
Global Precision: 0.3373647956007873
Global Recall: 0.23902242783686026
Global f1score: 0.2544704637362525
50
50
number of selected users 50
Global Trainning Accurancy: 0.24214132064873978
Global Trainning Loss: 2.174484658241272
Global test accurancy: 0.23866136826699366
Global test_loss: 2.1835640239715577
Global Precision: 0.3376858074730985
Global Recall: 0.23866136826699366
Global f1score: 0.25458281894870793
50
50
number of selected users 50
Global Trainning Accurancy: 0.24226909564769994
Global Trainning Loss: 2.173743963241577
Global test accurancy: 0.2391309905112429
Global test_loss: 2.1832220125198365
Global Precision: 0.3397285075368231
Global Recall: 0.2391309905112429
Global f1score: 0.2552632726069585
50
50
number of selected users 50
Global Trainning Accurancy: 0.24274458184493838
Global Trainning Loss: 2.1729862833023073
Global test accurancy: 0.23871395550318839
Global test_loss: 2.1828791999816897
Global Precision: 0.33861118194646683
Global Recall: 0.23871395550318839
Global f1score: 0.2543162951875341
50
50
number of selected users 50
Global Trainning Accurancy: 0.24314156880419235
Global Trainning Loss: 2.1722595262527467
Global test accurancy: 0.23813351171881042
Global test_loss: 2.182568655014038
Global Precision: 0.34106603089229653
Global Recall: 0.23813351171881042
Global f1score: 0.253605100100237
50
50
number of selected users 50
Global Trainning Accurancy: 0.24371221992025652
Global Trainning Loss: 2.171496562957764
Global test accurancy: 0.23914731378268791
Global test_loss: 2.1822199487686156
Global Precision: 0.33977688806886996
Global Recall: 0.23914731378268791
Global f1score: 0.2549128969089594
50
50
number of selected users 50
Global Trainning Accurancy: 0.2442745553966174
Global Trainning Loss: 2.1707703495025634
Global test accurancy: 0.2365922920385701
Global test_loss: 2.181883912086487
Global Precision: 0.33617014184672794
Global Recall: 0.2365922920385701
Global f1score: 0.25225928485731475
50
50
number of selected users 50
Global Trainning Accurancy: 0.2454172816184775
Global Trainning Loss: 2.1700498914718627
Global test accurancy: 0.2358940005213531
Global test_loss: 2.181567144393921
Global Precision: 0.3361094261002312
Global Recall: 0.2358940005213531
Global f1score: 0.2522982378868047
50
50
number of selected users 50
Global Trainning Accurancy: 0.24601780022710643
Global Trainning Loss: 2.169243588447571
Global test accurancy: 0.23744816528429488
Global test_loss: 2.1811693620681765
Global Precision: 0.338196303480389
Global Recall: 0.23744816528429488
Global f1score: 0.25407415926649835
50
50
number of selected users 50
Global Trainning Accurancy: 0.24509655548659182
Global Trainning Loss: 2.168602714538574
Global test accurancy: 0.2375275687962576
Global test_loss: 2.1809363174438476
Global Precision: 0.338222188511074
Global Recall: 0.2375275687962576
Global f1score: 0.25431564891789427
50
50
number of selected users 50
Global Trainning Accurancy: 0.24620181317093348
Global Trainning Loss: 2.1678265810012816
Global test accurancy: 0.23836439338984386
Global test_loss: 2.1805729389190676
Global Precision: 0.33862823555069466
Global Recall: 0.23836439338984386
Global f1score: 0.2554003030473448
50
50
number of selected users 50
Global Trainning Accurancy: 0.24631446225759948
Global Trainning Loss: 2.1671057891845704
Global test accurancy: 0.2380442680900611
Global test_loss: 2.180278515815735
Global Precision: 0.337869350930124
Global Recall: 0.2380442680900611
Global f1score: 0.2551097085257841
50
50
number of selected users 50
Global Trainning Accurancy: 0.2465240586078125
Global Trainning Loss: 2.166387610435486
Global test accurancy: 0.2386306242854265
Global test_loss: 2.179969687461853
Global Precision: 0.340350164015235
Global Recall: 0.2386306242854265
Global f1score: 0.25616326108886306
50
50
number of selected users 50
Global Trainning Accurancy: 0.2471134718573723
Global Trainning Loss: 2.165782747268677
Global test accurancy: 0.23821769338413445
Global test_loss: 2.179809980392456
Global Precision: 0.3377820883943274
Global Recall: 0.23821769338413445
Global f1score: 0.2560563070059838
50
50
number of selected users 50
Global Trainning Accurancy: 0.2478915359415165
Global Trainning Loss: 2.165012826919556
Global test accurancy: 0.23745800056837404
Global test_loss: 2.179474153518677
Global Precision: 0.3368597703487853
Global Recall: 0.23745800056837404
Global f1score: 0.2554914812377054
50
50
number of selected users 50
Global Trainning Accurancy: 0.24817239904503596
Global Trainning Loss: 2.1643051052093507
Global test accurancy: 0.2377427600719711
Global test_loss: 2.1792079162597657
Global Precision: 0.33602268313406136
Global Recall: 0.2377427600719711
Global f1score: 0.25568581536547336
50
50
number of selected users 50
Global Trainning Accurancy: 0.24735112902688347
Global Trainning Loss: 2.163580365180969
Global test accurancy: 0.2371768986205326
Global test_loss: 2.1789085626602174
Global Precision: 0.3349027475288821
Global Recall: 0.2371768986205326
Global f1score: 0.2553033239396507
50
50
number of selected users 50
Global Trainning Accurancy: 0.2482112101799571
Global Trainning Loss: 2.162966980934143
Global test accurancy: 0.23933345576832665
Global test_loss: 2.178705682754517
Global Precision: 0.3359937358721453
Global Recall: 0.23933345576832665
Global f1score: 0.25780268653833915
50
50
number of selected users 50
Global Trainning Accurancy: 0.24838264017756423
Global Trainning Loss: 2.162256817817688
Global test accurancy: 0.24067864497240357
Global test_loss: 2.178378281593323
Global Precision: 0.3385916880477047
Global Recall: 0.24067864497240357
Global f1score: 0.25961209416205694
50
50
number of selected users 50
Global Trainning Accurancy: 0.2489061232510604
Global Trainning Loss: 2.1616856336593626
Global test accurancy: 0.24002356391907065
Global test_loss: 2.1782345294952394
Global Precision: 0.3375240152007441
Global Recall: 0.24002356391907065
Global f1score: 0.2588563074915098
50
50
number of selected users 50
Global Trainning Accurancy: 0.24876086431284294
Global Trainning Loss: 2.1610392141342163
Global test accurancy: 0.23926942945486462
Global test_loss: 2.177980275154114
Global Precision: 0.335537633123895
Global Recall: 0.23926942945486462
Global f1score: 0.2584838927194794
50
50
number of selected users 50
Global Trainning Accurancy: 0.24959180879406048
Global Trainning Loss: 2.1603118658065794
Global test accurancy: 0.24028396024001988
Global test_loss: 2.1776477766036986
Global Precision: 0.33718908516310053
Global Recall: 0.24028396024001988
Global f1score: 0.260031828686719
50
50
number of selected users 50
Global Trainning Accurancy: 0.2497543275018741
Global Trainning Loss: 2.159600954055786
Global test accurancy: 0.24278309305130014
Global test_loss: 2.1773679304122924
Global Precision: 0.3401083777447972
Global Recall: 0.24278309305130014
Global f1score: 0.2626617912774608
50
50
number of selected users 50
Global Trainning Accurancy: 0.2502816582031556
Global Trainning Loss: 2.1588829803466796
Global test accurancy: 0.24151991840791157
Global test_loss: 2.1770914220809936
Global Precision: 0.3384428300529148
Global Recall: 0.24151991840791157
Global f1score: 0.26118278698349656
50
50
number of selected users 50
Global Trainning Accurancy: 0.2503957538328317
Global Trainning Loss: 2.1580600261688234
Global test accurancy: 0.24215927052263406
Global test_loss: 2.176712794303894
Global Precision: 0.33998320583792246
Global Recall: 0.24215927052263406
Global f1score: 0.26207643978593737
50
50
number of selected users 50
Global Trainning Accurancy: 0.25072586208834985
Global Trainning Loss: 2.1574900054931643
Global test accurancy: 0.2423122881277234
Global test_loss: 2.1765810012817384
Global Precision: 0.33851490319214467
Global Recall: 0.2423122881277234
Global f1score: 0.2618276939476765
50
50
number of selected users 50
Global Trainning Accurancy: 0.25110928399386184
Global Trainning Loss: 2.156854500770569
Global test accurancy: 0.24248969716944407
Global test_loss: 2.1764411544799804
Global Precision: 0.3390985270267572
Global Recall: 0.24248969716944407
Global f1score: 0.26273744293898316
50
50
number of selected users 50
Global Trainning Accurancy: 0.252278928873985
Global Trainning Loss: 2.15600510597229
Global test accurancy: 0.2423711116053583
Global test_loss: 2.1760478115081785
Global Precision: 0.33807477492482957
Global Recall: 0.2423711116053583
Global f1score: 0.26278340015227414
50
50
number of selected users 50
Global Trainning Accurancy: 0.25276098003025266
Global Trainning Loss: 2.1553036165237427
Global test accurancy: 0.24193537218991917
Global test_loss: 2.1758232164382934
Global Precision: 0.33764808819273623
Global Recall: 0.24193537218991917
Global f1score: 0.2622009216940156
50
50
number of selected users 50
Global Trainning Accurancy: 0.2526665479325523
Global Trainning Loss: 2.1546116638183594
Global test accurancy: 0.24244929274484325
Global test_loss: 2.175653133392334
Global Precision: 0.3377933246922601
Global Recall: 0.24244929274484325
Global f1score: 0.2626947052953745
50
50
number of selected users 50
Global Trainning Accurancy: 0.25345714889850435
Global Trainning Loss: 2.153880543708801
Global test accurancy: 0.24202805831094096
Global test_loss: 2.1754369640350344
Global Precision: 0.3376976029105943
Global Recall: 0.24202805831094096
Global f1score: 0.2631552823685613
50
50
number of selected users 50
Global Trainning Accurancy: 0.25426782348648297
Global Trainning Loss: 2.1533806324005127
Global test accurancy: 0.2422031414611493
Global test_loss: 2.175511512756348
Global Precision: 0.3351156645997228
Global Recall: 0.2422031414611493
Global f1score: 0.26237021346042355
50
50
number of selected users 50
Global Trainning Accurancy: 0.2541319019072369
Global Trainning Loss: 2.152622365951538
Global test accurancy: 0.24224258614460945
Global test_loss: 2.1752817678451537
Global Precision: 0.3364196904954771
Global Recall: 0.24224258614460945
Global f1score: 0.26263806765386705
50
50
number of selected users 50
Global Trainning Accurancy: 0.2546629937544075
Global Trainning Loss: 2.151910400390625
Global test accurancy: 0.24183751640551587
Global test_loss: 2.1750913429260255
Global Precision: 0.3347936311831941
Global Recall: 0.24183751640551587
Global f1score: 0.261998164980585
50
50
number of selected users 50
Global Trainning Accurancy: 0.2548441327556725
Global Trainning Loss: 2.151271357536316
Global test accurancy: 0.24169769934086135
Global test_loss: 2.1749761629104616
Global Precision: 0.335974197008635
Global Recall: 0.24169769934086135
Global f1score: 0.26251402164167337
50
50
number of selected users 50
Global Trainning Accurancy: 0.2552087157222909
Global Trainning Loss: 2.1506229972839357
Global test accurancy: 0.2411162936480285
Global test_loss: 2.1748493576049803
Global Precision: 0.3361861058325952
Global Recall: 0.2411162936480285
Global f1score: 0.2624333121272152
50
50
number of selected users 50
Global Trainning Accurancy: 0.25629757901575556
Global Trainning Loss: 2.150089101791382
Global test accurancy: 0.2396160820033999
Global test_loss: 2.1747892904281616
Global Precision: 0.33813970978822866
Global Recall: 0.2396160820033999
Global f1score: 0.2620615618048356
50
50
number of selected users 50
Global Trainning Accurancy: 0.2553161016581959
Global Trainning Loss: 2.149068660736084
Global test accurancy: 0.24165643180584953
Global test_loss: 2.1742670154571533
Global Precision: 0.33889708002448415
Global Recall: 0.24165643180584953
Global f1score: 0.2636021157519467
50
50
number of selected users 50
Global Trainning Accurancy: 0.2560823900881099
Global Trainning Loss: 2.1484026432037355
Global test accurancy: 0.24121710118671943
Global test_loss: 2.1741516685485838
Global Precision: 0.3389773993459413
Global Recall: 0.24121710118671943
Global f1score: 0.2635788679905068
50
50
number of selected users 50
Global Trainning Accurancy: 0.25612157737052665
Global Trainning Loss: 2.147662515640259
Global test accurancy: 0.2421774398598738
Global test_loss: 2.174003829956055
Global Precision: 0.3392620748647592
Global Recall: 0.2421774398598738
Global f1score: 0.264194199114799
50
50
number of selected users 50
Global Trainning Accurancy: 0.2566779070098936
Global Trainning Loss: 2.1470840549468995
Global test accurancy: 0.242469983974011
Global test_loss: 2.1740287256240847
Global Precision: 0.33975674271258854
Global Recall: 0.242469983974011
Global f1score: 0.26453506963931606
50
50
number of selected users 50
Global Trainning Accurancy: 0.25711303938788144
Global Trainning Loss: 2.146416845321655
Global test accurancy: 0.24212736961130463
Global test_loss: 2.1739427709579466
Global Precision: 0.33931641360456977
Global Recall: 0.24212736961130463
Global f1score: 0.26468369378966417
50
50
number of selected users 50
Global Trainning Accurancy: 0.2581493378749987
Global Trainning Loss: 2.1458520984649656
Global test accurancy: 0.24209710783538796
Global test_loss: 2.1739648723602296
Global Precision: 0.3346912782196434
Global Recall: 0.24209710783538796
Global f1score: 0.2642905106733694
50
50
number of selected users 50
Global Trainning Accurancy: 0.25834211741433805
Global Trainning Loss: 2.1452277517318725
Global test accurancy: 0.24280781446254984
Global test_loss: 2.173945083618164
Global Precision: 0.3347564437294982
Global Recall: 0.24280781446254984
Global f1score: 0.2647964849025227
50
50
number of selected users 50
Global Trainning Accurancy: 0.2582117276228993
Global Trainning Loss: 2.144582762718201
Global test accurancy: 0.2408523516984095
Global test_loss: 2.173960738182068
Global Precision: 0.3347402881013615
Global Recall: 0.2408523516984095
Global f1score: 0.2631380957331769
50
50
number of selected users 50
Global Trainning Accurancy: 0.2582399382128721
Global Trainning Loss: 2.1439578199386595
Global test accurancy: 0.2396337859808704
Global test_loss: 2.174073042869568
Global Precision: 0.33563564877666163
Global Recall: 0.2396337859808704
Global f1score: 0.262196349054015
50
50
number of selected users 50
Global Trainning Accurancy: 0.25800960426432534
Global Trainning Loss: 2.1434388017654418
Global test accurancy: 0.24051176610674108
Global test_loss: 2.174243550300598
Global Precision: 0.3369705127654249
Global Recall: 0.24051176610674108
Global f1score: 0.26340592836210525
50
50
number of selected users 50
Global Trainning Accurancy: 0.258573351812713
Global Trainning Loss: 2.142758021354675
Global test accurancy: 0.2401229059033369
Global test_loss: 2.174178538322449
Global Precision: 0.3358748802482672
Global Recall: 0.2401229059033369
Global f1score: 0.26268607235358915
50
50
number of selected users 50
Global Trainning Accurancy: 0.25840424123451106
Global Trainning Loss: 2.142045817375183
Global test accurancy: 0.24070652354136762
Global test_loss: 2.174102210998535
Global Precision: 0.33562638461203437
Global Recall: 0.24070652354136762
Global f1score: 0.2630956185658411
50
50
number of selected users 50
Global Trainning Accurancy: 0.2582194183324586
Global Trainning Loss: 2.1415186500549317
Global test accurancy: 0.23978311872430075
Global test_loss: 2.174224271774292
Global Precision: 0.33512030420401595
Global Recall: 0.23978311872430075
Global f1score: 0.26193926428406505
50
50
number of selected users 50
Global Trainning Accurancy: 0.2581777409437555
Global Trainning Loss: 2.1409445571899415
Global test accurancy: 0.2399925008736876
Global test_loss: 2.1742517280578615
Global Precision: 0.33579314382505954
Global Recall: 0.2399925008736876
Global f1score: 0.26209898826944206
50
50
number of selected users 50
Global Trainning Accurancy: 0.2589092381265019
Global Trainning Loss: 2.140304298400879
Global test accurancy: 0.24002517872760012
Global test_loss: 2.1741773891448974
Global Precision: 0.33696820324042975
Global Recall: 0.24002517872760012
Global f1score: 0.26278898147139595
50
50
number of selected users 50
Global Trainning Accurancy: 0.2592501871960375
Global Trainning Loss: 2.1398413801193237
Global test accurancy: 0.23850200674947997
Global test_loss: 2.1742883205413817
Global Precision: 0.3345797942669745
Global Recall: 0.23850200674947997
Global f1score: 0.26084768604136593
50
50
number of selected users 50
Global Trainning Accurancy: 0.25859334076436635
Global Trainning Loss: 2.139237823486328
Global test accurancy: 0.2388320914830345
Global test_loss: 2.1743223571777346
Global Precision: 0.33570958000198475
Global Recall: 0.2388320914830345
Global f1score: 0.2617352720455141
50
50
number of selected users 50
Global Trainning Accurancy: 0.2578322677818206
Global Trainning Loss: 2.1388236045837403
Global test accurancy: 0.23782075244122283
Global test_loss: 2.1745767164230347
Global Precision: 0.33515175431963873
Global Recall: 0.23782075244122283
Global f1score: 0.26062977418266303
50
50
number of selected users 50
Global Trainning Accurancy: 0.2585703122679481
Global Trainning Loss: 2.1382692193984987
Global test accurancy: 0.23924265666696612
Global test_loss: 2.1747202253341675
Global Precision: 0.3399028539122728
Global Recall: 0.23924265666696612
Global f1score: 0.2628539676463988
50
50
number of selected users 50
Global Trainning Accurancy: 0.2588268239322982
Global Trainning Loss: 2.1374092721939086
Global test accurancy: 0.23993374251845584
Global test_loss: 2.1746136283874513
Global Precision: 0.3383414059026786
Global Recall: 0.23993374251845584
Global f1score: 0.2631538084349071
50
50
number of selected users 50
Global Trainning Accurancy: 0.258690917830774
Global Trainning Loss: 2.136837196350098
Global test accurancy: 0.23754939014865778
Global test_loss: 2.174760069847107
Global Precision: 0.3361388762614014
Global Recall: 0.23754939014865778
Global f1score: 0.26078183480475364
50
50
number of selected users 50
Global Trainning Accurancy: 0.25838068213369897
Global Trainning Loss: 2.1359563446044922
Global test accurancy: 0.23809777338765625
Global test_loss: 2.1746448707580566
Global Precision: 0.3371487490475249
Global Recall: 0.23809777338765625
Global f1score: 0.26158725482772893
50
50
number of selected users 50
Global Trainning Accurancy: 0.25853287186358487
Global Trainning Loss: 2.135253949165344
Global test accurancy: 0.23680402862765704
Global test_loss: 2.174745588302612
Global Precision: 0.33554460396865476
Global Recall: 0.23680402862765704
Global f1score: 0.26042022446933083
50
50
number of selected users 50
Global Trainning Accurancy: 0.25895926745605546
Global Trainning Loss: 2.134764633178711
Global test accurancy: 0.23771914958028478
Global test_loss: 2.1750169801712036
Global Precision: 0.3357633089449002
Global Recall: 0.23771914958028478
Global f1score: 0.2613304672572081
50
50
number of selected users 50
Global Trainning Accurancy: 0.2590412842568907
Global Trainning Loss: 2.13394748210907
Global test accurancy: 0.23699917073673815
Global test_loss: 2.175069818496704
Global Precision: 0.33394615960545376
Global Recall: 0.23699917073673815
Global f1score: 0.260521192081876
50
50
number of selected users 50
Global Trainning Accurancy: 0.25998448710444205
Global Trainning Loss: 2.133215103149414
Global test accurancy: 0.23749808774276637
Global test_loss: 2.1750674200057984
Global Precision: 0.3327508298979515
Global Recall: 0.23749808774276637
Global f1score: 0.26107266758300257
50
50
number of selected users 50
Global Trainning Accurancy: 0.2591805219116485
Global Trainning Loss: 2.1326969051361084
Global test accurancy: 0.23803307433080512
Global test_loss: 2.17543571472168
Global Precision: 0.3341159139848645
Global Recall: 0.23803307433080512
Global f1score: 0.2617177763772357
50
50
number of selected users 50
Global Trainning Accurancy: 0.26153894954724666
Global Trainning Loss: 2.131843099594116
Global test accurancy: 0.23625610259516444
Global test_loss: 2.1755210399627685
Global Precision: 0.3323423513632527
Global Recall: 0.23625610259516444
Global f1score: 0.259352040180568
50
50
number of selected users 50
Global Trainning Accurancy: 0.2608320537769697
Global Trainning Loss: 2.1316812372207643
Global test accurancy: 0.23649092926613743
Global test_loss: 2.176283082962036
Global Precision: 0.33230151124591195
Global Recall: 0.23649092926613743
Global f1score: 0.25976386277815205
50
50
number of selected users 50
Global Trainning Accurancy: 0.26132246947295334
Global Trainning Loss: 2.1308010721206667
Global test accurancy: 0.23703596616548067
Global test_loss: 2.1763530588150024
Global Precision: 0.3347104641113597
Global Recall: 0.23703596616548067
Global f1score: 0.2600997188703881
50
50
number of selected users 50
Global Trainning Accurancy: 0.261753264996614
Global Trainning Loss: 2.1301475191116332
Global test accurancy: 0.23728225411658285
Global test_loss: 2.176707468032837
Global Precision: 0.3356317444782089
Global Recall: 0.23728225411658285
Global f1score: 0.2606985779615489
50
50
number of selected users 50
Global Trainning Accurancy: 0.2621539370860133
Global Trainning Loss: 2.1294080781936646
Global test accurancy: 0.2362170623468944
Global test_loss: 2.1769633674621582
Global Precision: 0.33472113982746
Global Recall: 0.2362170623468944
Global f1score: 0.259452072040298
50
50
number of selected users 50
Global Trainning Accurancy: 0.26315424211899896
Global Trainning Loss: 2.1286048793792727
Global test accurancy: 0.23624635841503838
Global test_loss: 2.177180976867676
Global Precision: 0.33872697965385845
Global Recall: 0.23624635841503838
Global f1score: 0.26012285098684845
50
50
number of selected users 50
Global Trainning Accurancy: 0.26414377602738076
Global Trainning Loss: 2.1280841779708863
Global test accurancy: 0.23515767353004713
Global test_loss: 2.1775146436691286
Global Precision: 0.33776241928153933
Global Recall: 0.23515767353004713
Global f1score: 0.2592234455815853
50
50
number of selected users 50
Global Trainning Accurancy: 0.26288128353782764
Global Trainning Loss: 2.127528657913208
Global test accurancy: 0.23546359547572365
Global test_loss: 2.17787691116333
Global Precision: 0.338726567854622
Global Recall: 0.23546359547572365
Global f1score: 0.25898325254524
50
50
number of selected users 50
Global Trainning Accurancy: 0.26490000695629773
Global Trainning Loss: 2.1263233327865603
Global test accurancy: 0.23444133821379695
Global test_loss: 2.1775994348526
Global Precision: 0.3374211238390636
Global Recall: 0.23444133821379695
Global f1score: 0.25836275351857213
50
50
number of selected users 50
Global Trainning Accurancy: 0.26573603404209245
Global Trainning Loss: 2.1253225135803224
Global test accurancy: 0.23456469720000844
Global test_loss: 2.1776435947418213
Global Precision: 0.33803295043656184
Global Recall: 0.23456469720000844
Global f1score: 0.25857008546442417
50
50
number of selected users 50
Global Trainning Accurancy: 0.26576779495077785
Global Trainning Loss: 2.1247302961349486
Global test accurancy: 0.23276231746796028
Global test_loss: 2.1779054546356202
Global Precision: 0.3359766275796992
Global Recall: 0.23276231746796028
Global f1score: 0.25710616498102146
exp_no  0
0_dataset_CIFAR10_algorithm_FedProx_model_CNN_3_50_0.4_31_07_2024
