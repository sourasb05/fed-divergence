wandb: Currently logged in as: sourasb05 (sourasb). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /proj/bhuyan24/fed-divergence/wandb/run-20240731_034045-84mhm2a8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MOON_KL_2024-07-31_03-40-44
wandb: ‚≠êÔ∏è View project at https://wandb.ai/sourasb/DIPA2-loss-function
wandb: üöÄ View run at https://wandb.ai/sourasb/DIPA2-loss-function/runs/84mhm2a8
============================================================
Summary of training process:
FL Algorithm: MOON_KL
model: CNN
optimizer: SGD
Batch size: 124
Global_iters: 100
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
============================================================
/proj/bhuyan24/fed-divergence
cnn_Cifar10_MOON(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (fc1): Linear(in_features=2048, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=10, bias=True)
)
CrossEntropyLoss()
CIFAR10
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:55<1:30:59, 55.15s/it]  2%|‚ñè         | 2/100 [01:42<1:23:00, 50.82s/it]  3%|‚ñé         | 3/100 [02:30<1:20:03, 49.53s/it]  4%|‚ñç         | 4/100 [03:19<1:18:25, 49.02s/it]  5%|‚ñå         | 5/100 [04:07<1:17:18, 48.83s/it]  6%|‚ñå         | 6/100 [04:56<1:16:17, 48.69s/it]  7%|‚ñã         | 7/100 [05:44<1:15:19, 48.60s/it]  8%|‚ñä         | 8/100 [06:32<1:14:23, 48.52s/it]  9%|‚ñâ         | 9/100 [07:21<1:13:29, 48.46s/it] 10%|‚ñà         | 10/100 [08:09<1:12:37, 48.42s/it] 11%|‚ñà         | 11/100 [08:57<1:11:46, 48.39s/it] 12%|‚ñà‚ñè        | 12/100 [09:46<1:10:55, 48.36s/it] 13%|‚ñà‚ñé        | 13/100 [10:34<1:10:03, 48.31s/it] 14%|‚ñà‚ñç        | 14/100 [11:22<1:09:11, 48.28s/it] 15%|‚ñà‚ñå        | 15/100 [12:10<1:08:23, 48.27s/it] 16%|‚ñà‚ñå        | 16/100 [12:59<1:07:35, 48.28s/it] 17%|‚ñà‚ñã        | 17/100 [13:47<1:06:48, 48.29s/it] 18%|‚ñà‚ñä        | 18/100 [14:35<1:06:00, 48.30s/it] 19%|‚ñà‚ñâ        | 19/100 [15:24<1:05:14, 48.33s/it] 20%|‚ñà‚ñà        | 20/100 [16:12<1:04:27, 48.35s/it] 21%|‚ñà‚ñà        | 21/100 [17:00<1:03:39, 48.35s/it] 22%|‚ñà‚ñà‚ñè       | 22/100 [17:49<1:02:51, 48.35s/it] 23%|‚ñà‚ñà‚ñé       | 23/100 [18:37<1:02:04, 48.38s/it] 24%|‚ñà‚ñà‚ñç       | 24/100 [19:26<1:01:17, 48.39s/it] 25%|‚ñà‚ñà‚ñå       | 25/100 [20:14<1:00:31, 48.41s/it] 26%|‚ñà‚ñà‚ñå       | 26/100 [21:02<59:43, 48.42s/it]   27%|‚ñà‚ñà‚ñã       | 27/100 [21:51<58:57, 48.46s/it] 28%|‚ñà‚ñà‚ñä       | 28/100 [22:40<58:12, 48.51s/it] 29%|‚ñà‚ñà‚ñâ       | 29/100 [23:28<57:24, 48.51s/it] 30%|‚ñà‚ñà‚ñà       | 30/100 [24:17<56:33, 48.48s/it] 31%|‚ñà‚ñà‚ñà       | 31/100 [25:05<55:44, 48.48s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [25:54<54:57, 48.49s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [26:42<54:05, 48.44s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 34/100 [27:30<53:16, 48.44s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [28:19<52:31, 48.48s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [29:07<51:42, 48.48s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 37/100 [29:56<50:55, 48.49s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [30:44<50:05, 48.48s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [31:33<49:16, 48.47s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [32:21<48:27, 48.45s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [33:10<47:37, 48.44s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [33:58<46:49, 48.44s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/100 [34:46<46:00, 48.42s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [35:35<45:10, 48.41s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [36:23<44:20, 48.37s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/100 [37:12<43:34, 48.42s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [38:00<42:48, 48.45s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [38:49<42:00, 48.47s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [39:37<41:13, 48.50s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [40:26<40:22, 48.45s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [41:14<39:33, 48.44s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/100 [42:02<38:45, 48.45s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [42:51<37:56, 48.44s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [43:39<37:08, 48.44s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/100 [44:28<36:19, 48.44s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [45:16<35:30, 48.42s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [46:05<34:41, 48.41s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 58/100 [46:53<33:54, 48.44s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [47:41<33:05, 48.42s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [48:30<32:17, 48.43s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/100 [49:18<31:30, 48.48s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [50:07<30:44, 48.53s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [50:56<29:57, 48.57s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 [51:44<29:08, 48.57s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [52:33<28:20, 48.58s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [53:21<27:31, 48.56s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/100 [54:10<26:41, 48.54s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [54:59<25:55, 48.60s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [55:47<25:06, 48.61s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [56:36<24:17, 48.57s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [57:24<23:27, 48.54s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [58:13<22:38, 48.53s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [59:01<21:50, 48.52s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [59:50<21:00, 48.48s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [1:00:38<20:11, 48.47s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/100 [1:01:27<19:23, 48.48s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [1:02:15<18:34, 48.44s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [1:03:03<17:45, 48.45s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 79/100 [1:03:52<16:57, 48.44s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [1:04:40<16:08, 48.44s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [1:05:29<15:20, 48.44s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 82/100 [1:06:17<14:31, 48.40s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [1:07:05<13:42, 48.37s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [1:07:54<12:53, 48.34s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/100 [1:08:42<12:05, 48.35s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [1:09:30<11:16, 48.35s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [1:10:19<10:28, 48.33s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [1:11:07<09:39, 48.33s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [1:11:55<08:51, 48.32s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [1:12:44<08:03, 48.33s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/100 [1:13:32<07:14, 48.31s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [1:14:20<06:26, 48.31s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [1:15:08<05:38, 48.30s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/100 [1:15:57<04:49, 48.29s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [1:16:45<04:01, 48.23s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [1:17:33<03:12, 48.20s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [1:18:21<02:24, 48.19s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [1:19:09<01:36, 48.14s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [1:19:57<00:48, 48.15s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [1:20:46<00:00, 48.19s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [1:20:46<00:00, 48.46s/it]
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.027 MB of 0.027 MB uploadedwandb: | 0.027 MB of 0.072 MB uploadedwandb: / 0.027 MB of 0.072 MB uploadedwandb: - 0.072 MB of 0.072 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:         global_F1 ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:  global_precision ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:     global_recall ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:  global_test_accs ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:  global_test_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: global_train_accs ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: global_train_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:         global_F1 0.6117
wandb:  global_precision 0.83524
wandb:     global_recall 0.50843
wandb:  global_test_accs 0.50843
wandb:  global_test_loss 1.35555
wandb: global_train_accs 0.53267
wandb: global_train_loss 1.31564
wandb: 
wandb: üöÄ View run MOON_KL_2024-07-31_03-40-44 at: https://wandb.ai/sourasb/DIPA2-loss-function/runs/84mhm2a8
wandb: Ô∏è‚ö° View job at https://wandb.ai/sourasb/DIPA2-loss-function/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjM0OTM0NDEyMA==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240731_034045-84mhm2a8/logs
50
50
number of selected users 50
Global Trainning Accurancy: 0.10596417889321877
Global Trainning Loss: 2.297737469673157
Global test accurancy: 0.10342681706389029
Global test_loss: 2.2981497478485107
Global Precision: 0.046894675222944604
Global Recall: 0.10342681706389029
Global f1score: 0.06083974328566288
50
50
number of selected users 50
Global Trainning Accurancy: 0.110726888305345
Global Trainning Loss: 2.2890073251724243
Global test accurancy: 0.10401428948345123
Global test_loss: 2.2911480712890624
Global Precision: 0.14588553757049455
Global Recall: 0.10401428948345123
Global f1score: 0.07173894419023578
50
50
number of selected users 50
Global Trainning Accurancy: 0.13560594949543486
Global Trainning Loss: 2.2863443875312806
Global test accurancy: 0.13460319049584485
Global test_loss: 2.2894334530830385
Global Precision: 0.09091004462662891
Global Recall: 0.13460319049584485
Global f1score: 0.09461204958878715
50
50
number of selected users 50
Global Trainning Accurancy: 0.1356279752514582
Global Trainning Loss: 2.2750306010246275
Global test accurancy: 0.1345262674189218
Global test_loss: 2.277060809135437
Global Precision: 0.08067020995388531
Global Recall: 0.1345262674189218
Global f1score: 0.09444843356263034
50
50
number of selected users 50
Global Trainning Accurancy: 0.13831980354288861
Global Trainning Loss: 2.2623180294036866
Global test accurancy: 0.1370648136240279
Global test_loss: 2.2633344078063966
Global Precision: 0.13604096167880653
Global Recall: 0.1370648136240279
Global f1score: 0.09970817534400658
50
50
number of selected users 50
Global Trainning Accurancy: 0.16277833815515397
Global Trainning Loss: 2.2465179634094237
Global test accurancy: 0.15726204145098246
Global test_loss: 2.2468357276916504
Global Precision: 0.2680632351328892
Global Recall: 0.15726204145098246
Global f1score: 0.13184842145882678
50
50
number of selected users 50
Global Trainning Accurancy: 0.20675063380889977
Global Trainning Loss: 2.224453945159912
Global test accurancy: 0.1963429706067662
Global test_loss: 2.2243358206748964
Global Precision: 0.3709536436346764
Global Recall: 0.1963429706067662
Global f1score: 0.1896622845032803
50
50
number of selected users 50
Global Trainning Accurancy: 0.25104913387033867
Global Trainning Loss: 2.195841190814972
Global test accurancy: 0.24998265295432975
Global test_loss: 2.195486297607422
Global Precision: 0.566094307033216
Global Recall: 0.24998265295432975
Global f1score: 0.28237668852636705
50
50
number of selected users 50
Global Trainning Accurancy: 0.2743553948136339
Global Trainning Loss: 2.1637322735786437
Global test accurancy: 0.27473538762801203
Global test_loss: 2.1632545042037963
Global Precision: 0.6398754493384758
Global Recall: 0.27473538762801203
Global f1score: 0.3277854222052773
50
50
number of selected users 50
Global Trainning Accurancy: 0.2744091114897709
Global Trainning Loss: 2.131656050682068
Global test accurancy: 0.2879862315596744
Global test_loss: 2.1314081716537476
Global Precision: 0.6522265644608225
Global Recall: 0.2879862315596744
Global f1score: 0.3548045883822232
50
50
number of selected users 50
Global Trainning Accurancy: 0.27683826130720113
Global Trainning Loss: 2.101844711303711
Global test accurancy: 0.28942820915737993
Global test_loss: 2.1020966601371764
Global Precision: 0.6467750663065488
Global Recall: 0.28942820915737993
Global f1score: 0.3592868128791195
50
50
number of selected users 50
Global Trainning Accurancy: 0.2804790471602033
Global Trainning Loss: 2.0744615268707274
Global test accurancy: 0.2938381364373038
Global test_loss: 2.0752701330184937
Global Precision: 0.6447792091781982
Global Recall: 0.2938381364373038
Global f1score: 0.3668766256449401
50
50
number of selected users 50
Global Trainning Accurancy: 0.28670166248259293
Global Trainning Loss: 2.050339822769165
Global test accurancy: 0.29849269146392754
Global test_loss: 2.051552088260651
Global Precision: 0.6471109361788036
Global Recall: 0.29849269146392754
Global f1score: 0.3744447039091211
50
50
number of selected users 50
Global Trainning Accurancy: 0.2942564417206797
Global Trainning Loss: 2.028358452320099
Global test accurancy: 0.30391046243434106
Global test_loss: 2.0299232840538024
Global Precision: 0.6473381974431576
Global Recall: 0.30391046243434106
Global f1score: 0.38148146398665594
50
50
number of selected users 50
Global Trainning Accurancy: 0.29932691767336866
Global Trainning Loss: 2.0083661890029907
Global test accurancy: 0.30640321243394175
Global test_loss: 2.0102302384376527
Global Precision: 0.6505554362252732
Global Recall: 0.30640321243394175
Global f1score: 0.38549682161449467
50
50
number of selected users 50
Global Trainning Accurancy: 0.30525953388919547
Global Trainning Loss: 1.9888010978698731
Global test accurancy: 0.3155909348985071
Global test_loss: 1.9906932044029235
Global Precision: 0.6507222960059841
Global Recall: 0.3155909348985071
Global f1score: 0.3930612770722588
50
50
number of selected users 50
Global Trainning Accurancy: 0.30944625140825177
Global Trainning Loss: 1.9721119594573975
Global test accurancy: 0.3192063365665388
Global test_loss: 1.9739595794677733
Global Precision: 0.6575804342925584
Global Recall: 0.3192063365665388
Global f1score: 0.3974848907718245
50
50
number of selected users 50
Global Trainning Accurancy: 0.3125602675424499
Global Trainning Loss: 1.9556906580924989
Global test accurancy: 0.32234081935478626
Global test_loss: 1.957378032207489
Global Precision: 0.6598299780044334
Global Recall: 0.32234081935478626
Global f1score: 0.4014864824286713
50
50
number of selected users 50
Global Trainning Accurancy: 0.31806056821957307
Global Trainning Loss: 1.9396978092193604
Global test accurancy: 0.3286433099122837
Global test_loss: 1.9411470770835877
Global Precision: 0.6697431996118576
Global Recall: 0.3286433099122837
Global f1score: 0.4089724686099013
50
50
number of selected users 50
Global Trainning Accurancy: 0.3220006543303559
Global Trainning Loss: 1.9246600270271301
Global test accurancy: 0.3314471225286173
Global test_loss: 1.9258735060691834
Global Precision: 0.6717250857665623
Global Recall: 0.3314471225286173
Global f1score: 0.41271964342244166
50
50
number of selected users 50
Global Trainning Accurancy: 0.3280501599817358
Global Trainning Loss: 1.9100309753417968
Global test accurancy: 0.33836203275722665
Global test_loss: 1.9109861946105957
Global Precision: 0.6824068486874885
Global Recall: 0.33836203275722665
Global f1score: 0.4221250790231303
50
50
number of selected users 50
Global Trainning Accurancy: 0.331190280625294
Global Trainning Loss: 1.8955475187301636
Global test accurancy: 0.3418039380593688
Global test_loss: 1.8962778902053834
Global Precision: 0.6837178456579059
Global Recall: 0.3418039380593688
Global f1score: 0.4259903102902757
50
50
number of selected users 50
Global Trainning Accurancy: 0.33509932241597395
Global Trainning Loss: 1.8812794518470763
Global test accurancy: 0.34418541011128523
Global test_loss: 1.8817232418060303
Global Precision: 0.6853378622260092
Global Recall: 0.34418541011128523
Global f1score: 0.4296005593984707
50
50
number of selected users 50
Global Trainning Accurancy: 0.34085261036860603
Global Trainning Loss: 1.8672300052642823
Global test accurancy: 0.3478234976485307
Global test_loss: 1.8674685192108154
Global Precision: 0.6872905871507021
Global Recall: 0.3478234976485307
Global f1score: 0.4344970286576062
50
50
number of selected users 50
Global Trainning Accurancy: 0.34617550410176656
Global Trainning Loss: 1.8532801365852356
Global test accurancy: 0.3559292404877807
Global test_loss: 1.8533643651008607
Global Precision: 0.6950957456847517
Global Recall: 0.3559292404877807
Global f1score: 0.4440062303799408
50
50
number of selected users 50
Global Trainning Accurancy: 0.3490402279913025
Global Trainning Loss: 1.8392323875427246
Global test accurancy: 0.3625268017364284
Global test_loss: 1.839211266040802
Global Precision: 0.7184507631900235
Global Recall: 0.3625268017364284
Global f1score: 0.4515005706405829
50
50
number of selected users 50
Global Trainning Accurancy: 0.3537413503877454
Global Trainning Loss: 1.8255505418777467
Global test accurancy: 0.3664908845185745
Global test_loss: 1.8255068802833556
Global Precision: 0.7202548306105726
Global Recall: 0.3664908845185745
Global f1score: 0.4562726304258258
50
50
number of selected users 50
Global Trainning Accurancy: 0.3575049842910926
Global Trainning Loss: 1.8121211004257203
Global test accurancy: 0.370000163235514
Global test_loss: 1.8120531606674195
Global Precision: 0.722285997260443
Global Recall: 0.370000163235514
Global f1score: 0.46045530173137594
50
50
number of selected users 50
Global Trainning Accurancy: 0.36126166719639147
Global Trainning Loss: 1.7986681365966797
Global test accurancy: 0.3765274755517875
Global test_loss: 1.7985766339302063
Global Precision: 0.7353239684601353
Global Recall: 0.3765274755517875
Global f1score: 0.468380265020778
50
50
number of selected users 50
Global Trainning Accurancy: 0.36458270327259323
Global Trainning Loss: 1.7859497308731078
Global test accurancy: 0.3809378486253321
Global test_loss: 1.7860194718837739
Global Precision: 0.7472157613884287
Global Recall: 0.3809378486253321
Global f1score: 0.4747175045555726
50
50
number of selected users 50
Global Trainning Accurancy: 0.36731108166080295
Global Trainning Loss: 1.7738267230987548
Global test accurancy: 0.3840627497124927
Global test_loss: 1.7739778065681457
Global Precision: 0.756791741917224
Global Recall: 0.38406274971249266
Global f1score: 0.47839230693295204
50
50
number of selected users 50
Global Trainning Accurancy: 0.3712576824850028
Global Trainning Loss: 1.761918113231659
Global test accurancy: 0.38958521243397115
Global test_loss: 1.7623816752433776
Global Precision: 0.7721034258041953
Global Recall: 0.38958521243397115
Global f1score: 0.48581414853931426
50
50
number of selected users 50
Global Trainning Accurancy: 0.3749485445296394
Global Trainning Loss: 1.750316436290741
Global test accurancy: 0.3904161678398405
Global test_loss: 1.750925008058548
Global Precision: 0.7735220748059553
Global Recall: 0.3904161678398405
Global f1score: 0.48835548816174446
50
50
number of selected users 50
Global Trainning Accurancy: 0.3780799607313851
Global Trainning Loss: 1.7390992736816406
Global test accurancy: 0.39932632914591815
Global test_loss: 1.7397751462459565
Global Precision: 0.7798997258509539
Global Recall: 0.39932632914591815
Global f1score: 0.49826654309363094
50
50
number of selected users 50
Global Trainning Accurancy: 0.38079297618070196
Global Trainning Loss: 1.7286964178085327
Global test accurancy: 0.4011346624097375
Global test_loss: 1.7295022904872894
Global Precision: 0.7813596040496471
Global Recall: 0.4011346624097375
Global f1score: 0.500367890157805
50
50
number of selected users 50
Global Trainning Accurancy: 0.38408744906242936
Global Trainning Loss: 1.7184879088401794
Global test accurancy: 0.4038002915828767
Global test_loss: 1.7193785297870636
Global Precision: 0.782272678460216
Global Recall: 0.4038002915828767
Global f1score: 0.5034021246483615
50
50
number of selected users 50
Global Trainning Accurancy: 0.3867998240481943
Global Trainning Loss: 1.7087604808807373
Global test accurancy: 0.4092969851407809
Global test_loss: 1.7098444390296936
Global Precision: 0.7876052580851137
Global Recall: 0.4092969851407809
Global f1score: 0.5091043324758823
50
50
number of selected users 50
Global Trainning Accurancy: 0.38845979213248977
Global Trainning Loss: 1.6994002652168274
Global test accurancy: 0.4130864306314153
Global test_loss: 1.7007287847995758
Global Precision: 0.7921061643991757
Global Recall: 0.4130864306314153
Global f1score: 0.513299093920042
50
50
number of selected users 50
Global Trainning Accurancy: 0.3902814451452452
Global Trainning Loss: 1.6903913593292237
Global test accurancy: 0.4163560574255766
Global test_loss: 1.6919397568702699
Global Precision: 0.7934144802508386
Global Recall: 0.4163560574255766
Global f1score: 0.516843420554935
50
50
number of selected users 50
Global Trainning Accurancy: 0.3938446249552241
Global Trainning Loss: 1.6814805614948272
Global test accurancy: 0.41971540615179986
Global test_loss: 1.6833037507534028
Global Precision: 0.8000017139556898
Global Recall: 0.41971540615179986
Global f1score: 0.5204037884405317
50
50
number of selected users 50
Global Trainning Accurancy: 0.3963425099798439
Global Trainning Loss: 1.6729025375843047
Global test accurancy: 0.4218166923278096
Global test_loss: 1.6749425721168518
Global Precision: 0.8014510154868743
Global Recall: 0.4218166923278096
Global f1score: 0.5228246936625882
50
50
number of selected users 50
Global Trainning Accurancy: 0.39921767935944547
Global Trainning Loss: 1.6645081412792206
Global test accurancy: 0.4227949244305009
Global test_loss: 1.6669885396957398
Global Precision: 0.8012850781661169
Global Recall: 0.4227949244305009
Global f1score: 0.523997945299272
50
50
number of selected users 50
Global Trainning Accurancy: 0.402216091681809
Global Trainning Loss: 1.6562726283073426
Global test accurancy: 0.42463489769488244
Global test_loss: 1.658978486061096
Global Precision: 0.8005345885891723
Global Recall: 0.42463489769488244
Global f1score: 0.5261588466078342
50
50
number of selected users 50
Global Trainning Accurancy: 0.40716744950549916
Global Trainning Loss: 1.6480482971668244
Global test accurancy: 0.4270455175557243
Global test_loss: 1.651031265258789
Global Precision: 0.8019870771843398
Global Recall: 0.4270455175557243
Global f1score: 0.5291360469663796
50
50
number of selected users 50
Global Trainning Accurancy: 0.40918260202677575
Global Trainning Loss: 1.6400884997844696
Global test accurancy: 0.43079745505696204
Global test_loss: 1.6434865367412568
Global Precision: 0.8032353156447994
Global Recall: 0.43079745505696204
Global f1score: 0.5326649691339648
50
50
number of selected users 50
Global Trainning Accurancy: 0.41168476362501805
Global Trainning Loss: 1.632208069562912
Global test accurancy: 0.432884679213515
Global test_loss: 1.635928430557251
Global Precision: 0.8040369748364917
Global Recall: 0.432884679213515
Global f1score: 0.5353272042456039
50
50
number of selected users 50
Global Trainning Accurancy: 0.4134823359594816
Global Trainning Loss: 1.6242911696434021
Global test accurancy: 0.43696458311178527
Global test_loss: 1.6283191072940826
Global Precision: 0.803096295793306
Global Recall: 0.43696458311178527
Global f1score: 0.5383694570158314
50
50
number of selected users 50
Global Trainning Accurancy: 0.41646603190063514
Global Trainning Loss: 1.6163010835647582
Global test accurancy: 0.43758089485636653
Global test_loss: 1.620808584690094
Global Precision: 0.8037878788939764
Global Recall: 0.43758089485636653
Global f1score: 0.5392177519329275
50
50
number of selected users 50
Global Trainning Accurancy: 0.41906563498382376
Global Trainning Loss: 1.6083477008342744
Global test accurancy: 0.4427647297413905
Global test_loss: 1.6133100986480713
Global Precision: 0.8054519870355201
Global Recall: 0.4427647297413905
Global f1score: 0.5438511075624375
50
50
number of selected users 50
Global Trainning Accurancy: 0.42107243393380683
Global Trainning Loss: 1.6002849626541138
Global test accurancy: 0.4437565162400546
Global test_loss: 1.6055540776252746
Global Precision: 0.80616600945862
Global Recall: 0.4437565162400546
Global f1score: 0.5452647985037563
50
50
number of selected users 50
Global Trainning Accurancy: 0.423350703411849
Global Trainning Loss: 1.5925402867794036
Global test accurancy: 0.44593301007846176
Global test_loss: 1.598586345911026
Global Precision: 0.805670430934803
Global Recall: 0.44593301007846176
Global f1score: 0.5474755308274009
50
50
number of selected users 50
Global Trainning Accurancy: 0.42588816894913345
Global Trainning Loss: 1.5843293380737304
Global test accurancy: 0.4491201393194346
Global test_loss: 1.5905401694774628
Global Precision: 0.8072498579480637
Global Recall: 0.4491201393194346
Global f1score: 0.550537305591573
50
50
number of selected users 50
Global Trainning Accurancy: 0.4265307727510663
Global Trainning Loss: 1.576438604593277
Global test accurancy: 0.4511919017650305
Global test_loss: 1.5832583022117614
Global Precision: 0.8086387841315881
Global Recall: 0.4511919017650305
Global f1score: 0.5522597038056076
50
50
number of selected users 50
Global Trainning Accurancy: 0.4292995580086776
Global Trainning Loss: 1.5684330308437346
Global test accurancy: 0.4533311132231642
Global test_loss: 1.5759093737602234
Global Precision: 0.8090285765768261
Global Recall: 0.4533311132231642
Global f1score: 0.5544712600459077
50
50
number of selected users 50
Global Trainning Accurancy: 0.4338202041397148
Global Trainning Loss: 1.5604718506336213
Global test accurancy: 0.4556245538190093
Global test_loss: 1.5685979151725769
Global Precision: 0.8100157046367612
Global Recall: 0.4556245538190093
Global f1score: 0.5570086798177232
50
50
number of selected users 50
Global Trainning Accurancy: 0.4363009674789166
Global Trainning Loss: 1.5526176571846009
Global test accurancy: 0.4573145236676019
Global test_loss: 1.5613898861408233
Global Precision: 0.8119483012934616
Global Recall: 0.4573145236676019
Global f1score: 0.5587912746147332
50
50
number of selected users 50
Global Trainning Accurancy: 0.4390447818234802
Global Trainning Loss: 1.5447242510318757
Global test accurancy: 0.4592669574698338
Global test_loss: 1.5541936802864074
Global Precision: 0.8125222132668415
Global Recall: 0.4592669574698338
Global f1score: 0.5607171029774904
50
50
number of selected users 50
Global Trainning Accurancy: 0.44147574773235704
Global Trainning Loss: 1.5368762397766114
Global test accurancy: 0.45799672826592264
Global test_loss: 1.5470982694625854
Global Precision: 0.8126936844055181
Global Recall: 0.45799672826592264
Global f1score: 0.559744628212229
50
50
number of selected users 50
Global Trainning Accurancy: 0.44438364793960505
Global Trainning Loss: 1.5293022096157074
Global test accurancy: 0.46090035560123543
Global test_loss: 1.5403193855285644
Global Precision: 0.8154089303924065
Global Recall: 0.46090035560123543
Global f1score: 0.5630424796481973
50
50
number of selected users 50
Global Trainning Accurancy: 0.44724475257186597
Global Trainning Loss: 1.5216995894908905
Global test accurancy: 0.46266998957806516
Global test_loss: 1.5336210072040557
Global Precision: 0.8163774498628634
Global Recall: 0.46266998957806516
Global f1score: 0.5648877047610636
50
50
number of selected users 50
Global Trainning Accurancy: 0.4491167251385154
Global Trainning Loss: 1.514140647649765
Global test accurancy: 0.464311459213431
Global test_loss: 1.5268431544303893
Global Precision: 0.8163387568890891
Global Recall: 0.464311459213431
Global f1score: 0.5660397705470123
50
50
number of selected users 50
Global Trainning Accurancy: 0.45071456384272324
Global Trainning Loss: 1.5067561173439026
Global test accurancy: 0.465858389874226
Global test_loss: 1.5201905119419097
Global Precision: 0.8157270642899966
Global Recall: 0.465858389874226
Global f1score: 0.5677402243094336
50
50
number of selected users 50
Global Trainning Accurancy: 0.4532629253509944
Global Trainning Loss: 1.4991759026050568
Global test accurancy: 0.4627096177791996
Global test_loss: 1.5130584919452668
Global Precision: 0.8149386645872201
Global Recall: 0.4627096177791996
Global f1score: 0.5662434112655554
50
50
number of selected users 50
Global Trainning Accurancy: 0.45576147832065284
Global Trainning Loss: 1.4923818910121918
Global test accurancy: 0.46323196620334195
Global test_loss: 1.5073780989646912
Global Precision: 0.8160383207603025
Global Recall: 0.46323196620334195
Global f1score: 0.5671843349164628
50
50
number of selected users 50
Global Trainning Accurancy: 0.45776723114313994
Global Trainning Loss: 1.4852802193164825
Global test accurancy: 0.46811003655848693
Global test_loss: 1.5006931078433992
Global Precision: 0.8161893726350974
Global Recall: 0.46811003655848693
Global f1score: 0.5712988688195348
50
50
number of selected users 50
Global Trainning Accurancy: 0.460576230625046
Global Trainning Loss: 1.4786550414562225
Global test accurancy: 0.4701139637975569
Global test_loss: 1.4948807239532471
Global Precision: 0.8167508535150174
Global Recall: 0.4701139637975569
Global f1score: 0.5732892156034176
50
50
number of selected users 50
Global Trainning Accurancy: 0.4635956545890056
Global Trainning Loss: 1.4720192015171052
Global test accurancy: 0.47173997992659283
Global test_loss: 1.4893777096271514
Global Precision: 0.8165719745723858
Global Recall: 0.47173997992659283
Global f1score: 0.5752081436122706
50
50
number of selected users 50
Global Trainning Accurancy: 0.4680859199941443
Global Trainning Loss: 1.4662216627597808
Global test accurancy: 0.4682394945139775
Global test_loss: 1.4846849620342255
Global Precision: 0.8107410987296383
Global Recall: 0.4682394945139775
Global f1score: 0.5716647033345995
50
50
number of selected users 50
Global Trainning Accurancy: 0.4710378675813615
Global Trainning Loss: 1.4603694355487824
Global test accurancy: 0.47043124838286204
Global test_loss: 1.4797113120555878
Global Precision: 0.8110726291620958
Global Recall: 0.47043124838286204
Global f1score: 0.5737184867336753
50
50
number of selected users 50
Global Trainning Accurancy: 0.47284917918528857
Global Trainning Loss: 1.45435023188591
Global test accurancy: 0.4692075617121661
Global test_loss: 1.4744352030754089
Global Precision: 0.8093861404591574
Global Recall: 0.4692075617121661
Global f1score: 0.5715280600498444
50
50
number of selected users 50
Global Trainning Accurancy: 0.47460753198584116
Global Trainning Loss: 1.4485037112236023
Global test accurancy: 0.4711439989274423
Global test_loss: 1.4693749046325684
Global Precision: 0.8106776794223466
Global Recall: 0.4711439989274423
Global f1score: 0.5735095355453015
50
50
number of selected users 50
Global Trainning Accurancy: 0.47576024475899353
Global Trainning Loss: 1.4427441799640655
Global test accurancy: 0.47263424385679376
Global test_loss: 1.4641341805458068
Global Precision: 0.8116996381370283
Global Recall: 0.47263424385679376
Global f1score: 0.5750933015861471
50
50
number of selected users 50
Global Trainning Accurancy: 0.47619734153509
Global Trainning Loss: 1.4373074686527252
Global test accurancy: 0.47425223034385317
Global test_loss: 1.4593413734436036
Global Precision: 0.8124040216480461
Global Recall: 0.47425223034385317
Global f1score: 0.5763002419095838
50
50
number of selected users 50
Global Trainning Accurancy: 0.4790967551931343
Global Trainning Loss: 1.4319976603984832
Global test accurancy: 0.47600228166566233
Global test_loss: 1.454878764152527
Global Precision: 0.8116649886878352
Global Recall: 0.47600228166566233
Global f1score: 0.5776749730527596
50
50
number of selected users 50
Global Trainning Accurancy: 0.4814812002379733
Global Trainning Loss: 1.426830747127533
Global test accurancy: 0.4772379842217657
Global test_loss: 1.4505168521404266
Global Precision: 0.8122734479179217
Global Recall: 0.4772379842217657
Global f1score: 0.5789669392572467
50
50
number of selected users 50
Global Trainning Accurancy: 0.48320208036649015
Global Trainning Loss: 1.4216422033309937
Global test accurancy: 0.47772576905932657
Global test_loss: 1.446240746974945
Global Precision: 0.8091652957442064
Global Recall: 0.47772576905932657
Global f1score: 0.5788627358825361
50
50
number of selected users 50
Global Trainning Accurancy: 0.48550269540987023
Global Trainning Loss: 1.4167458093166352
Global test accurancy: 0.4768276297271687
Global test_loss: 1.4421259593963622
Global Precision: 0.8073381557877208
Global Recall: 0.4768276297271687
Global f1score: 0.5775170311418721
50
50
number of selected users 50
Global Trainning Accurancy: 0.48722712889177056
Global Trainning Loss: 1.4116780722141267
Global test accurancy: 0.4808381694512601
Global test_loss: 1.4378631329536438
Global Precision: 0.8110281335649636
Global Recall: 0.4808381694512601
Global f1score: 0.5818593956011078
50
50
number of selected users 50
Global Trainning Accurancy: 0.4911919941334742
Global Trainning Loss: 1.4067577731609344
Global test accurancy: 0.48229027853822115
Global test_loss: 1.4336392450332642
Global Precision: 0.812753379506697
Global Recall: 0.48229027853822115
Global f1score: 0.583587795341641
50
50
number of selected users 50
Global Trainning Accurancy: 0.49271556613559625
Global Trainning Loss: 1.401962194442749
Global test accurancy: 0.48305217240766196
Global test_loss: 1.4297022914886475
Global Precision: 0.8134491268613386
Global Recall: 0.48305217240766196
Global f1score: 0.5843278491752121
50
50
number of selected users 50
Global Trainning Accurancy: 0.494530604866215
Global Trainning Loss: 1.3972144794464112
Global test accurancy: 0.4840997811717667
Global test_loss: 1.425674592256546
Global Precision: 0.8137028381596964
Global Recall: 0.4840997811717667
Global f1score: 0.5853534245397187
50
50
number of selected users 50
Global Trainning Accurancy: 0.4969618607437447
Global Trainning Loss: 1.3926456189155578
Global test accurancy: 0.4854767304247628
Global test_loss: 1.4217846703529358
Global Precision: 0.8137610754894641
Global Recall: 0.4854767304247628
Global f1score: 0.586370740879872
50
50
number of selected users 50
Global Trainning Accurancy: 0.4981654852227518
Global Trainning Loss: 1.3880883991718291
Global test accurancy: 0.4869901220859313
Global test_loss: 1.4179343831539155
Global Precision: 0.813850304123507
Global Recall: 0.4869901220859313
Global f1score: 0.5875941354817255
50
50
number of selected users 50
Global Trainning Accurancy: 0.5004169462498607
Global Trainning Loss: 1.3836505115032196
Global test accurancy: 0.48815966365441366
Global test_loss: 1.4141713345050813
Global Precision: 0.8145736008572396
Global Recall: 0.48815966365441366
Global f1score: 0.5887128380170844
50
50
number of selected users 50
Global Trainning Accurancy: 0.5017897931479395
Global Trainning Loss: 1.3791154968738555
Global test accurancy: 0.4891844680852273
Global test_loss: 1.4102768540382384
Global Precision: 0.8145277693870462
Global Recall: 0.4891844680852273
Global f1score: 0.5895363584412224
50
50
number of selected users 50
Global Trainning Accurancy: 0.5027810168397892
Global Trainning Loss: 1.3746533286571503
Global test accurancy: 0.49103795351849117
Global test_loss: 1.4065083277225494
Global Precision: 0.8150268730355416
Global Recall: 0.49103795351849117
Global f1score: 0.5914941792802484
50
50
number of selected users 50
Global Trainning Accurancy: 0.5038333787111736
Global Trainning Loss: 1.3702827548980714
Global test accurancy: 0.4931811519225576
Global test_loss: 1.4027711308002473
Global Precision: 0.8165319583916807
Global Recall: 0.4931811519225577
Global f1score: 0.5938186197179013
50
50
number of selected users 50
Global Trainning Accurancy: 0.5059744453486886
Global Trainning Loss: 1.3659367978572845
Global test accurancy: 0.49383479172789013
Global test_loss: 1.3991087937355042
Global Precision: 0.8179776318361811
Global Recall: 0.49383479172789013
Global f1score: 0.5944981449786653
50
50
number of selected users 50
Global Trainning Accurancy: 0.5074185099202335
Global Trainning Loss: 1.3615240001678466
Global test accurancy: 0.4948870310788892
Global test_loss: 1.3953020477294922
Global Precision: 0.8190207230119307
Global Recall: 0.4948870310788892
Global f1score: 0.5954288618759498
50
50
number of selected users 50
Global Trainning Accurancy: 0.5085875579580331
Global Trainning Loss: 1.3571901535987854
Global test accurancy: 0.4958103281004451
Global test_loss: 1.39165385723114
Global Precision: 0.8257199495958141
Global Recall: 0.4958103281004451
Global f1score: 0.5986679121003721
50
50
number of selected users 50
Global Trainning Accurancy: 0.5103692761606194
Global Trainning Loss: 1.352941074371338
Global test accurancy: 0.4970302176643755
Global test_loss: 1.3880729758739472
Global Precision: 0.8262412221196825
Global Recall: 0.4970302176643755
Global f1score: 0.5997482707480324
50
50
number of selected users 50
Global Trainning Accurancy: 0.5143430908027395
Global Trainning Loss: 1.3486341726779938
Global test accurancy: 0.4983881815196558
Global test_loss: 1.3843958282470703
Global Precision: 0.8269992976226737
Global Recall: 0.4983881815196558
Global f1score: 0.6009824342598217
50
50
number of selected users 50
Global Trainning Accurancy: 0.5160775191844892
Global Trainning Loss: 1.3444032382965088
Global test accurancy: 0.49911831089909675
Global test_loss: 1.3807073974609374
Global Precision: 0.8275580706556287
Global Recall: 0.49911831089909675
Global f1score: 0.6020502545835297
50
50
number of selected users 50
Global Trainning Accurancy: 0.5185466317849778
Global Trainning Loss: 1.3401696252822877
Global test accurancy: 0.5004415242079342
Global test_loss: 1.3771001529693603
Global Precision: 0.827478324874385
Global Recall: 0.5004415242079342
Global f1score: 0.6030680595205566
50
50
number of selected users 50
Global Trainning Accurancy: 0.5200072706080413
Global Trainning Loss: 1.3359236240386962
Global test accurancy: 0.5020781584214264
Global test_loss: 1.373334549665451
Global Precision: 0.8287234091515783
Global Recall: 0.5020781584214263
Global f1score: 0.604837626382518
50
50
number of selected users 50
Global Trainning Accurancy: 0.5231431008826455
Global Trainning Loss: 1.3317923295497893
Global test accurancy: 0.5053701709365007
Global test_loss: 1.369783309698105
Global Precision: 0.8338390857522849
Global Recall: 0.5053701709365007
Global f1score: 0.6088561696102113
50
50
number of selected users 50
Global Trainning Accurancy: 0.5253900662089654
Global Trainning Loss: 1.3276391541957855
Global test accurancy: 0.5067813558902304
Global test_loss: 1.3661142539978028
Global Precision: 0.8348790882950489
Global Recall: 0.5067813558902304
Global f1score: 0.6101653688308643
50
50
number of selected users 50
Global Trainning Accurancy: 0.5292871941586419
Global Trainning Loss: 1.323660444021225
Global test accurancy: 0.5070583004478243
Global test_loss: 1.3625649785995484
Global Precision: 0.8353222183188941
Global Recall: 0.5070583004478243
Global f1score: 0.6104545606985221
50
50
number of selected users 50
Global Trainning Accurancy: 0.5319398300270664
Global Trainning Loss: 1.319610322713852
Global test accurancy: 0.5080727531787335
Global test_loss: 1.3589813339710235
Global Precision: 0.835501443140896
Global Recall: 0.5080727531787335
Global f1score: 0.6112201572864829
50
50
number of selected users 50
Global Trainning Accurancy: 0.5326674806555354
Global Trainning Loss: 1.31564111828804
Global test accurancy: 0.5084337628909553
Global test_loss: 1.355548951625824
Global Precision: 0.8352374975360164
Global Recall: 0.5084337628909553
Global f1score: 0.611703987813788
exp_no  0
0_dataset_CIFAR10algorithm_MOON_KL_model_CNN_31_07_2024
