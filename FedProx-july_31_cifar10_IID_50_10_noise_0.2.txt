============================================================
Summary of training process:
FL Algorithm: FedProx
model: CNN
optimizer: SGD
Batch size: 124
Global_iters: 200
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
Proximal hyperparameter 1.0
============================================================
/proj/bhuyan24/fed-divergence
CIFAR10
./data/data/noisy/0.2_50_10/train/cifa_train.json
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:09<30:27,  9.18s/it]  1%|          | 2/200 [00:15<24:48,  7.52s/it]  2%|▏         | 3/200 [00:22<23:09,  7.06s/it]  2%|▏         | 4/200 [00:28<22:08,  6.78s/it]  2%|▎         | 5/200 [00:34<21:40,  6.67s/it]  3%|▎         | 6/200 [00:41<21:16,  6.58s/it]  4%|▎         | 7/200 [00:47<21:10,  6.58s/it]  4%|▍         | 8/200 [00:54<20:59,  6.56s/it]  4%|▍         | 9/200 [01:00<20:42,  6.50s/it]  5%|▌         | 10/200 [01:07<20:25,  6.45s/it]  6%|▌         | 11/200 [01:13<20:11,  6.41s/it]  6%|▌         | 12/200 [01:19<19:59,  6.38s/it]  6%|▋         | 13/200 [01:26<19:49,  6.36s/it]  7%|▋         | 14/200 [01:32<19:42,  6.36s/it]  8%|▊         | 15/200 [01:38<19:36,  6.36s/it]  8%|▊         | 16/200 [01:45<19:30,  6.36s/it]  8%|▊         | 17/200 [01:51<19:23,  6.36s/it]  9%|▉         | 18/200 [01:57<19:14,  6.34s/it] 10%|▉         | 19/200 [02:04<19:08,  6.34s/it] 10%|█         | 20/200 [02:10<19:01,  6.34s/it] 10%|█         | 21/200 [02:16<18:55,  6.34s/it] 11%|█         | 22/200 [02:23<18:49,  6.35s/it] 12%|█▏        | 23/200 [02:29<18:43,  6.35s/it] 12%|█▏        | 24/200 [02:35<18:37,  6.35s/it] 12%|█▎        | 25/200 [02:42<18:31,  6.35s/it] 13%|█▎        | 26/200 [02:48<18:25,  6.35s/it] 14%|█▎        | 27/200 [02:54<18:19,  6.35s/it] 14%|█▍        | 28/200 [03:01<18:12,  6.35s/it] 14%|█▍        | 29/200 [03:07<18:06,  6.35s/it] 15%|█▌        | 30/200 [03:13<17:59,  6.35s/it] 16%|█▌        | 31/200 [03:20<17:53,  6.35s/it] 16%|█▌        | 32/200 [03:26<17:46,  6.35s/it] 16%|█▋        | 33/200 [03:33<17:39,  6.35s/it] 17%|█▋        | 34/200 [03:39<17:33,  6.35s/it] 18%|█▊        | 35/200 [03:45<17:30,  6.37s/it] 18%|█▊        | 36/200 [03:52<17:26,  6.38s/it] 18%|█▊        | 37/200 [03:58<17:22,  6.40s/it] 19%|█▉        | 38/200 [04:05<17:17,  6.41s/it] 20%|█▉        | 39/200 [04:11<17:13,  6.42s/it] 20%|██        | 40/200 [04:17<17:10,  6.44s/it] 20%|██        | 41/200 [04:24<17:03,  6.44s/it] 21%|██        | 42/200 [04:30<16:53,  6.41s/it] 22%|██▏       | 43/200 [04:37<16:44,  6.40s/it] 22%|██▏       | 44/200 [04:43<16:35,  6.38s/it] 22%|██▎       | 45/200 [04:49<16:28,  6.38s/it] 23%|██▎       | 46/200 [04:56<16:20,  6.37s/it] 24%|██▎       | 47/200 [05:02<16:13,  6.36s/it] 24%|██▍       | 48/200 [05:08<16:07,  6.36s/it] 24%|██▍       | 49/200 [05:15<16:05,  6.40s/it] 25%|██▌       | 50/200 [05:21<15:57,  6.38s/it] 26%|██▌       | 51/200 [05:28<15:50,  6.38s/it] 26%|██▌       | 52/200 [05:34<15:44,  6.38s/it] 26%|██▋       | 53/200 [05:40<15:37,  6.38s/it] 27%|██▋       | 54/200 [05:47<15:41,  6.45s/it] 28%|██▊       | 55/200 [05:54<15:42,  6.50s/it] 28%|██▊       | 56/200 [06:00<15:40,  6.53s/it] 28%|██▊       | 57/200 [06:07<15:30,  6.51s/it] 29%|██▉       | 58/200 [06:13<15:16,  6.46s/it] 30%|██▉       | 59/200 [06:19<15:04,  6.42s/it] 30%|███       | 60/200 [06:26<14:55,  6.40s/it] 30%|███       | 61/200 [06:32<14:47,  6.39s/it] 31%|███       | 62/200 [06:38<14:40,  6.38s/it] 32%|███▏      | 63/200 [06:45<14:33,  6.38s/it] 32%|███▏      | 64/200 [06:51<14:26,  6.37s/it] 32%|███▎      | 65/200 [06:57<14:19,  6.36s/it] 33%|███▎      | 66/200 [07:04<14:12,  6.36s/it] 34%|███▎      | 67/200 [07:10<14:06,  6.36s/it] 34%|███▍      | 68/200 [07:17<14:01,  6.37s/it] 34%|███▍      | 69/200 [07:23<13:54,  6.37s/it] 35%|███▌      | 70/200 [07:29<13:48,  6.37s/it] 36%|███▌      | 71/200 [07:36<13:41,  6.37s/it] 36%|███▌      | 72/200 [07:42<13:34,  6.36s/it] 36%|███▋      | 73/200 [07:48<13:27,  6.36s/it] 37%|███▋      | 74/200 [07:55<13:21,  6.36s/it] 38%|███▊      | 75/200 [08:01<13:13,  6.35s/it] 38%|███▊      | 76/200 [08:07<13:07,  6.35s/it] 38%|███▊      | 77/200 [08:14<13:01,  6.36s/it] 39%|███▉      | 78/200 [08:20<12:55,  6.36s/it] 40%|███▉      | 79/200 [08:26<12:48,  6.35s/it] 40%|████      | 80/200 [08:33<12:42,  6.35s/it] 40%|████      | 81/200 [08:39<12:35,  6.35s/it] 41%|████      | 82/200 [08:46<12:29,  6.36s/it] 42%|████▏     | 83/200 [08:52<12:24,  6.36s/it] 42%|████▏     | 84/200 [08:58<12:17,  6.36s/it] 42%|████▎     | 85/200 [09:05<12:11,  6.36s/it] 43%|████▎     | 86/200 [09:11<12:04,  6.36s/it] 44%|████▎     | 87/200 [09:17<12:00,  6.38s/it] 44%|████▍     | 88/200 [09:24<11:53,  6.37s/it] 44%|████▍     | 89/200 [09:30<11:46,  6.36s/it] 45%|████▌     | 90/200 [09:36<11:39,  6.36s/it] 46%|████▌     | 91/200 [09:43<11:32,  6.36s/it] 46%|████▌     | 92/200 [09:49<11:26,  6.35s/it] 46%|████▋     | 93/200 [09:55<11:19,  6.35s/it] 47%|████▋     | 94/200 [10:02<11:13,  6.35s/it] 48%|████▊     | 95/200 [10:08<11:06,  6.35s/it] 48%|████▊     | 96/200 [10:15<11:00,  6.35s/it] 48%|████▊     | 97/200 [10:21<10:53,  6.35s/it] 49%|████▉     | 98/200 [10:27<10:48,  6.35s/it] 50%|████▉     | 99/200 [10:34<10:42,  6.36s/it] 50%|█████     | 100/200 [10:40<10:35,  6.36s/it] 50%|█████     | 101/200 [10:46<10:28,  6.35s/it] 51%|█████     | 102/200 [10:53<10:22,  6.35s/it] 52%|█████▏    | 103/200 [10:59<10:17,  6.36s/it] 52%|█████▏    | 104/200 [11:05<10:10,  6.36s/it] 52%|█████▎    | 105/200 [11:12<10:04,  6.36s/it] 53%|█████▎    | 106/200 [11:18<09:58,  6.36s/it] 54%|█████▎    | 107/200 [11:25<09:51,  6.36s/it] 54%|█████▍    | 108/200 [11:31<09:45,  6.36s/it] 55%|█████▍    | 109/200 [11:37<09:38,  6.35s/it] 55%|█████▌    | 110/200 [11:44<09:31,  6.35s/it] 56%|█████▌    | 111/200 [11:50<09:25,  6.35s/it] 56%|█████▌    | 112/200 [11:56<09:18,  6.35s/it] 56%|█████▋    | 113/200 [12:03<09:13,  6.36s/it] 57%|█████▋    | 114/200 [12:09<09:07,  6.36s/it] 57%|█████▊    | 115/200 [12:15<09:00,  6.36s/it] 58%|█████▊    | 116/200 [12:22<08:54,  6.36s/it] 58%|█████▊    | 117/200 [12:28<08:47,  6.36s/it] 59%|█████▉    | 118/200 [12:34<08:41,  6.36s/it] 60%|█████▉    | 119/200 [12:41<08:34,  6.36s/it] 60%|██████    | 120/200 [12:47<08:29,  6.37s/it] 60%|██████    | 121/200 [12:54<08:23,  6.37s/it] 61%|██████    | 122/200 [13:00<08:17,  6.38s/it] 62%|██████▏   | 123/200 [13:06<08:10,  6.37s/it] 62%|██████▏   | 124/200 [13:13<08:04,  6.38s/it] 62%|██████▎   | 125/200 [13:19<07:58,  6.38s/it] 63%|██████▎   | 126/200 [13:25<07:51,  6.37s/it] 64%|██████▎   | 127/200 [13:32<07:45,  6.37s/it] 64%|██████▍   | 128/200 [13:38<07:39,  6.38s/it] 64%|██████▍   | 129/200 [13:45<07:33,  6.39s/it] 65%|██████▌   | 130/200 [13:51<07:26,  6.38s/it] 66%|██████▌   | 131/200 [13:57<07:18,  6.36s/it] 66%|██████▌   | 132/200 [14:04<07:12,  6.36s/it] 66%|██████▋   | 133/200 [14:10<07:05,  6.36s/it] 67%|██████▋   | 134/200 [14:16<06:59,  6.36s/it] 68%|██████▊   | 135/200 [14:23<06:53,  6.36s/it] 68%|██████▊   | 136/200 [14:29<06:46,  6.35s/it] 68%|██████▊   | 137/200 [14:35<06:40,  6.35s/it] 69%|██████▉   | 138/200 [14:42<06:33,  6.34s/it] 70%|██████▉   | 139/200 [14:48<06:26,  6.34s/it] 70%|███████   | 140/200 [14:54<06:20,  6.34s/it] 70%|███████   | 141/200 [15:01<06:14,  6.34s/it] 71%|███████   | 142/200 [15:07<06:08,  6.35s/it] 72%|███████▏  | 143/200 [15:13<06:02,  6.35s/it] 72%|███████▏  | 144/200 [15:20<05:56,  6.36s/it] 72%|███████▎  | 145/200 [15:26<05:49,  6.36s/it] 73%|███████▎  | 146/200 [15:33<05:43,  6.36s/it] 74%|███████▎  | 147/200 [15:39<05:36,  6.36s/it] 74%|███████▍  | 148/200 [15:45<05:30,  6.35s/it] 74%|███████▍  | 149/200 [15:52<05:23,  6.35s/it] 75%|███████▌  | 150/200 [15:58<05:17,  6.35s/it] 76%|███████▌  | 151/200 [16:04<05:11,  6.35s/it] 76%|███████▌  | 152/200 [16:11<05:04,  6.35s/it] 76%|███████▋  | 153/200 [16:17<04:58,  6.36s/it] 77%|███████▋  | 154/200 [16:23<04:52,  6.36s/it] 78%|███████▊  | 155/200 [16:30<04:46,  6.36s/it] 78%|███████▊  | 156/200 [16:36<04:39,  6.36s/it] 78%|███████▊  | 157/200 [16:42<04:33,  6.36s/it] 79%|███████▉  | 158/200 [16:49<04:27,  6.36s/it] 80%|███████▉  | 159/200 [16:55<04:21,  6.37s/it] 80%|████████  | 160/200 [17:02<04:14,  6.37s/it] 80%|████████  | 161/200 [17:08<04:09,  6.41s/it] 81%|████████  | 162/200 [17:15<04:05,  6.45s/it] 82%|████████▏ | 163/200 [17:21<03:58,  6.45s/it] 82%|████████▏ | 164/200 [17:27<03:51,  6.42s/it] 82%|████████▎ | 165/200 [17:34<03:43,  6.40s/it] 83%|████████▎ | 166/200 [17:40<03:37,  6.38s/it] 84%|████████▎ | 167/200 [17:47<03:30,  6.38s/it] 84%|████████▍ | 168/200 [17:53<03:23,  6.37s/it] 84%|████████▍ | 169/200 [17:59<03:17,  6.37s/it] 85%|████████▌ | 170/200 [18:06<03:11,  6.37s/it] 86%|████████▌ | 171/200 [18:12<03:04,  6.37s/it] 86%|████████▌ | 172/200 [18:18<02:58,  6.37s/it] 86%|████████▋ | 173/200 [18:25<02:51,  6.36s/it] 87%|████████▋ | 174/200 [18:31<02:45,  6.37s/it] 88%|████████▊ | 175/200 [18:37<02:39,  6.37s/it] 88%|████████▊ | 176/200 [18:44<02:32,  6.37s/it] 88%|████████▊ | 177/200 [18:50<02:26,  6.37s/it] 89%|████████▉ | 178/200 [18:57<02:19,  6.36s/it] 90%|████████▉ | 179/200 [19:03<02:13,  6.36s/it] 90%|█████████ | 180/200 [19:09<02:07,  6.35s/it] 90%|█████████ | 181/200 [19:16<02:00,  6.34s/it] 91%|█████████ | 182/200 [19:22<01:54,  6.35s/it] 92%|█████████▏| 183/200 [19:28<01:47,  6.35s/it] 92%|█████████▏| 184/200 [19:35<01:41,  6.35s/it] 92%|█████████▎| 185/200 [19:41<01:35,  6.35s/it] 93%|█████████▎| 186/200 [19:47<01:28,  6.35s/it] 94%|█████████▎| 187/200 [19:54<01:22,  6.35s/it] 94%|█████████▍| 188/200 [20:00<01:16,  6.36s/it] 94%|█████████▍| 189/200 [20:06<01:10,  6.37s/it] 95%|█████████▌| 190/200 [20:13<01:03,  6.37s/it] 96%|█████████▌| 191/200 [20:19<00:57,  6.37s/it] 96%|█████████▌| 192/200 [20:26<00:50,  6.37s/it] 96%|█████████▋| 193/200 [20:32<00:44,  6.36s/it] 97%|█████████▋| 194/200 [20:38<00:38,  6.36s/it] 98%|█████████▊| 195/200 [20:45<00:31,  6.35s/it] 98%|█████████▊| 196/200 [20:51<00:25,  6.34s/it] 98%|█████████▊| 197/200 [20:57<00:19,  6.34s/it] 99%|█████████▉| 198/200 [21:04<00:12,  6.34s/it]100%|█████████▉| 199/200 [21:10<00:06,  6.34s/it]100%|██████████| 200/200 [21:16<00:00,  6.35s/it]100%|██████████| 200/200 [21:16<00:00,  6.38s/it]
50
50
number of selected users 50
Global Trainning Accurancy: 0.10104317611219303
Global Trainning Loss: 2.302839756011963
Global test accurancy: 0.10133355773381743
Global test_loss: 2.3028610610961913
Global Precision: 0.011357498819518608
Global Recall: 0.10133355773381743
Global f1score: 0.02013507957645835
50
50
number of selected users 50
Global Trainning Accurancy: 0.10107301640076644
Global Trainning Loss: 2.302332582473755
Global test accurancy: 0.10142263804631288
Global test_loss: 2.3023662090301515
Global Precision: 0.018028186715021217
Global Recall: 0.10142263804631288
Global f1score: 0.02045750809675151
50
50
number of selected users 50
Global Trainning Accurancy: 0.10856296211284605
Global Trainning Loss: 2.3018195819854737
Global test accurancy: 0.10946790807664364
Global test_loss: 2.30186870098114
Global Precision: 0.0439313117840577
Global Recall: 0.10946790807664364
Global f1score: 0.039749387949573625
50
50
number of selected users 50
Global Trainning Accurancy: 0.1511612235995871
Global Trainning Loss: 2.301268081665039
Global test accurancy: 0.15129713123269856
Global test_loss: 2.3013349771499634
Global Precision: 0.0523576502828527
Global Recall: 0.15129713123269856
Global f1score: 0.07419056304064421
50
50
number of selected users 50
Global Trainning Accurancy: 0.1502677296748984
Global Trainning Loss: 2.3006381464004515
Global test accurancy: 0.15071536000411465
Global test_loss: 2.300724515914917
Global Precision: 0.06253246658383875
Global Recall: 0.15071536000411465
Global f1score: 0.07447546611202613
50
50
number of selected users 50
Global Trainning Accurancy: 0.13142796770927803
Global Trainning Loss: 2.299947452545166
Global test accurancy: 0.12920362451893208
Global test_loss: 2.3000549983978273
Global Precision: 0.06767388681844703
Global Recall: 0.12920362451893208
Global f1score: 0.05402225739825666
50
50
number of selected users 50
Global Trainning Accurancy: 0.11904147019964625
Global Trainning Loss: 2.2992306613922118
Global test accurancy: 0.1174431148568859
Global test_loss: 2.2993568849563597
Global Precision: 0.052827991653033944
Global Recall: 0.1174431148568859
Global f1score: 0.042247743753028914
50
50
number of selected users 50
Global Trainning Accurancy: 0.11278647920224662
Global Trainning Loss: 2.2984344148635865
Global test accurancy: 0.11171941860079929
Global test_loss: 2.298573751449585
Global Precision: 0.04863101448283299
Global Recall: 0.11171941860079929
Global f1score: 0.03472804467377828
50
50
number of selected users 50
Global Trainning Accurancy: 0.10884893948112606
Global Trainning Loss: 2.2974905157089234
Global test accurancy: 0.10896990315847964
Global test_loss: 2.2976444149017334
Global Precision: 0.05825620403570677
Global Recall: 0.10896990315847964
Global f1score: 0.03050723145683023
50
50
number of selected users 50
Global Trainning Accurancy: 0.10635126994811961
Global Trainning Loss: 2.2963846158981323
Global test accurancy: 0.1070292302825099
Global test_loss: 2.296557240486145
Global Precision: 0.05660577715972037
Global Recall: 0.1070292302825099
Global f1score: 0.02703528539836851
50
50
number of selected users 50
Global Trainning Accurancy: 0.1059791072244161
Global Trainning Loss: 2.295040259361267
Global test accurancy: 0.10659487262478343
Global test_loss: 2.2952300453186036
Global Precision: 0.054784524847715156
Global Recall: 0.10659487262478343
Global f1score: 0.02629199274192988
50
50
number of selected users 50
Global Trainning Accurancy: 0.10726877010467747
Global Trainning Loss: 2.2933692932128906
Global test accurancy: 0.10812737259879025
Global test_loss: 2.2935833501815797
Global Precision: 0.06698673691441988
Global Recall: 0.10812737259879025
Global f1score: 0.02901386530011261
50
50
number of selected users 50
Global Trainning Accurancy: 0.11484153874546948
Global Trainning Loss: 2.2912518644332884
Global test accurancy: 0.11335681925850721
Global test_loss: 2.29149046421051
Global Precision: 0.09724230783798855
Global Recall: 0.11335681925850721
Global f1score: 0.03910158381528315
50
50
number of selected users 50
Global Trainning Accurancy: 0.13812162631767674
Global Trainning Loss: 2.2885180282592774
Global test accurancy: 0.13898584183007778
Global test_loss: 2.2887857294082643
Global Precision: 0.12942057850532812
Global Recall: 0.13898584183007778
Global f1score: 0.0785187089514352
50
50
number of selected users 50
Global Trainning Accurancy: 0.1639751971664044
Global Trainning Loss: 2.2849339056015014
Global test accurancy: 0.16462478258111007
Global test_loss: 2.285250368118286
Global Precision: 0.1704556059581503
Global Recall: 0.16462478258111007
Global f1score: 0.10768055193608543
50
50
number of selected users 50
Global Trainning Accurancy: 0.18740101386569064
Global Trainning Loss: 2.2801481437683107
Global test accurancy: 0.18695276901490335
Global test_loss: 2.2805307054519655
Global Precision: 0.15686327870908845
Global Recall: 0.18695276901490335
Global f1score: 0.13020626303341334
50
50
number of selected users 50
Global Trainning Accurancy: 0.1964153113925251
Global Trainning Loss: 2.2735679721832276
Global test accurancy: 0.19480075065453656
Global test_loss: 2.2740311002731324
Global Precision: 0.1680354051312781
Global Recall: 0.19480075065453656
Global f1score: 0.14077331146450103
50
50
number of selected users 50
Global Trainning Accurancy: 0.20185074571489894
Global Trainning Loss: 2.2643526363372803
Global test accurancy: 0.20013704187310496
Global test_loss: 2.264933457374573
Global Precision: 0.16881746291459443
Global Recall: 0.20013704187310496
Global f1score: 0.15088666771210776
50
50
number of selected users 50
Global Trainning Accurancy: 0.20477244679995482
Global Trainning Loss: 2.2518501758575438
Global test accurancy: 0.20095840370937512
Global test_loss: 2.252587904930115
Global Precision: 0.18109282612915462
Global Recall: 0.20095840370937512
Global f1score: 0.15397588799822903
50
50
number of selected users 50
Global Trainning Accurancy: 0.20511696889145686
Global Trainning Loss: 2.2359456062316894
Global test accurancy: 0.20059256004722728
Global test_loss: 2.2368664264678957
Global Precision: 0.1874090035735234
Global Recall: 0.20059256004722728
Global f1score: 0.1535413424957082
50
50
number of selected users 50
Global Trainning Accurancy: 0.20699094561324333
Global Trainning Loss: 2.217978491783142
Global test accurancy: 0.20570465128726959
Global test_loss: 2.219110269546509
Global Precision: 0.19543340461066588
Global Recall: 0.20570465128726959
Global f1score: 0.16113931600438128
50
50
number of selected users 50
Global Trainning Accurancy: 0.2128362108689616
Global Trainning Loss: 2.2000973987579346
Global test accurancy: 0.21182373254570475
Global test_loss: 2.2015062236785887
Global Precision: 0.20078353261988355
Global Recall: 0.21182373254570475
Global f1score: 0.17237252275208112
50
50
number of selected users 50
Global Trainning Accurancy: 0.21859688683469797
Global Trainning Loss: 2.1836020135879517
Global test accurancy: 0.21772260186864378
Global test_loss: 2.1853470516204836
Global Precision: 0.20535503697122917
Global Recall: 0.21772260186864378
Global f1score: 0.1811339847922924
50
50
number of selected users 50
Global Trainning Accurancy: 0.22608079910293613
Global Trainning Loss: 2.1685061168670656
Global test accurancy: 0.2232009307255868
Global test_loss: 2.170660457611084
Global Precision: 0.2122608587308192
Global Recall: 0.2232009307255868
Global f1score: 0.18821458389892004
50
50
number of selected users 50
Global Trainning Accurancy: 0.23105786163297565
Global Trainning Loss: 2.154638242721558
Global test accurancy: 0.22448612092949177
Global test_loss: 2.15727343082428
Global Precision: 0.21393295097906523
Global Recall: 0.22448612092949177
Global f1score: 0.19049450165426862
50
50
number of selected users 50
Global Trainning Accurancy: 0.2345979402035726
Global Trainning Loss: 2.1417808723449707
Global test accurancy: 0.22910083630657196
Global test_loss: 2.1449904441833496
Global Precision: 0.21528542930634154
Global Recall: 0.22910083630657196
Global f1score: 0.19525781393919908
50
50
number of selected users 50
Global Trainning Accurancy: 0.23808256719146736
Global Trainning Loss: 2.1297622394561766
Global test accurancy: 0.23283669239441632
Global test_loss: 2.1335984325408934
Global Precision: 0.21630346858055888
Global Recall: 0.23283669239441632
Global f1score: 0.1987600369829167
50
50
number of selected users 50
Global Trainning Accurancy: 0.24135105214614755
Global Trainning Loss: 2.1183856439590456
Global test accurancy: 0.23637057640096754
Global test_loss: 2.122850565910339
Global Precision: 0.22260619412846666
Global Recall: 0.23637057640096754
Global f1score: 0.20288678979367436
50
50
number of selected users 50
Global Trainning Accurancy: 0.245401521761531
Global Trainning Loss: 2.1075407791137697
Global test accurancy: 0.24212108148898914
Global test_loss: 2.1126102352142335
Global Precision: 0.23030383380023675
Global Recall: 0.24212108148898914
Global f1score: 0.20868087882123826
50
50
number of selected users 50
Global Trainning Accurancy: 0.250081822304991
Global Trainning Loss: 2.0971984481811523
Global test accurancy: 0.24765751210991824
Global test_loss: 2.102822365760803
Global Precision: 0.2324935713020455
Global Recall: 0.24765751210991824
Global f1score: 0.21454047759743994
50
50
number of selected users 50
Global Trainning Accurancy: 0.25512346104659195
Global Trainning Loss: 2.08729389667511
Global test accurancy: 0.25383648662258407
Global test_loss: 2.09338360786438
Global Precision: 0.2542487693080211
Global Recall: 0.25383648662258407
Global f1score: 0.22127750604055463
50
50
number of selected users 50
Global Trainning Accurancy: 0.260625647202057
Global Trainning Loss: 2.0777887010574343
Global test accurancy: 0.25815179210929223
Global test_loss: 2.084238648414612
Global Precision: 0.26207037919973164
Global Recall: 0.25815179210929223
Global f1score: 0.22612558196888935
50
50
number of selected users 50
Global Trainning Accurancy: 0.2664516036413172
Global Trainning Loss: 2.0685683822631837
Global test accurancy: 0.26473473720282065
Global test_loss: 2.0752739453315736
Global Precision: 0.27412035754043335
Global Recall: 0.26473473720282065
Global f1score: 0.23342399547319176
50
50
number of selected users 50
Global Trainning Accurancy: 0.2713363870536718
Global Trainning Loss: 2.0595984625816346
Global test accurancy: 0.27041434854202856
Global test_loss: 2.0664564514160157
Global Precision: 0.28485437063676283
Global Recall: 0.27041434854202856
Global f1score: 0.24028371299519574
50
50
number of selected users 50
Global Trainning Accurancy: 0.2766370925814267
Global Trainning Loss: 2.0508273816108704
Global test accurancy: 0.2740661953944553
Global test_loss: 2.0577221417427065
Global Precision: 0.28805262954538896
Global Recall: 0.2740661953944553
Global f1score: 0.24536812135560687
50
50
number of selected users 50
Global Trainning Accurancy: 0.28121305934317486
Global Trainning Loss: 2.042221281528473
Global test accurancy: 0.2793898550363172
Global test_loss: 2.0490913438796996
Global Precision: 0.2921871516343428
Global Recall: 0.2793898550363172
Global f1score: 0.2525382524909634
50
50
number of selected users 50
Global Trainning Accurancy: 0.2863146199245769
Global Trainning Loss: 2.0337520956993105
Global test accurancy: 0.2845724438660959
Global test_loss: 2.0405715465545655
Global Precision: 0.29648958165177314
Global Recall: 0.2845724438660959
Global f1score: 0.2595847749894169
50
50
number of selected users 50
Global Trainning Accurancy: 0.29043637217969426
Global Trainning Loss: 2.0253933382034304
Global test accurancy: 0.2898342313033103
Global test_loss: 2.03212956905365
Global Precision: 0.30096519162935037
Global Recall: 0.2898342313033103
Global f1score: 0.26648387232184884
50
50
number of selected users 50
Global Trainning Accurancy: 0.2937943753703748
Global Trainning Loss: 2.017200915813446
Global test accurancy: 0.2936562620749322
Global test_loss: 2.023816270828247
Global Precision: 0.3113655854245254
Global Recall: 0.2936562620749322
Global f1score: 0.2721755322425648
50
50
number of selected users 50
Global Trainning Accurancy: 0.29747918940153156
Global Trainning Loss: 2.009187173843384
Global test accurancy: 0.29792233820351416
Global test_loss: 2.015639851093292
Global Precision: 0.3153774559761181
Global Recall: 0.29792233820351416
Global f1score: 0.27761568009692567
50
50
number of selected users 50
Global Trainning Accurancy: 0.30116743167590465
Global Trainning Loss: 2.0013895559310915
Global test accurancy: 0.3022780084365705
Global test_loss: 2.0076838374137878
Global Precision: 0.31620747539861205
Global Recall: 0.3022780084365705
Global f1score: 0.2832627604700084
50
50
number of selected users 50
Global Trainning Accurancy: 0.30522050704247305
Global Trainning Loss: 1.9937978959083558
Global test accurancy: 0.30571523085375785
Global test_loss: 1.9999026703834533
Global Precision: 0.3209868951480447
Global Recall: 0.30571523085375785
Global f1score: 0.2876851827871894
50
50
number of selected users 50
Global Trainning Accurancy: 0.30831961012475245
Global Trainning Loss: 1.9863153147697448
Global test accurancy: 0.3082949436889512
Global test_loss: 1.9922717785835267
Global Precision: 0.32193930587183733
Global Recall: 0.3082949436889512
Global f1score: 0.2916755157944506
50
50
number of selected users 50
Global Trainning Accurancy: 0.31163669230693386
Global Trainning Loss: 1.9789475297927857
Global test accurancy: 0.3116717573524153
Global test_loss: 1.9847698593139649
Global Precision: 0.3227560982706005
Global Recall: 0.3116717573524153
Global f1score: 0.2955639601767992
50
50
number of selected users 50
Global Trainning Accurancy: 0.31516838967260313
Global Trainning Loss: 1.9717114520072938
Global test accurancy: 0.31521075540384313
Global test_loss: 1.9773623466491699
Global Precision: 0.3250722463253065
Global Recall: 0.31521075540384313
Global f1score: 0.29961435917402574
50
50
number of selected users 50
Global Trainning Accurancy: 0.31869906173853774
Global Trainning Loss: 1.9646254110336303
Global test accurancy: 0.31789210739595625
Global test_loss: 1.97006103515625
Global Precision: 0.32673441562645805
Global Recall: 0.31789210739595625
Global f1score: 0.3033579529961678
50
50
number of selected users 50
Global Trainning Accurancy: 0.32168213113578603
Global Trainning Loss: 1.957661349773407
Global test accurancy: 0.32154863373972326
Global test_loss: 1.9628868365287782
Global Precision: 0.3320941555347763
Global Recall: 0.32154863373972326
Global f1score: 0.3082791227309204
50
50
number of selected users 50
Global Trainning Accurancy: 0.32506751330535544
Global Trainning Loss: 1.9508010149002075
Global test accurancy: 0.32639189044564015
Global test_loss: 1.9558300971984863
Global Precision: 0.3374168844431188
Global Recall: 0.32639189044564015
Global f1score: 0.3142745947240906
50
50
number of selected users 50
Global Trainning Accurancy: 0.32712959791656554
Global Trainning Loss: 1.944044017791748
Global test accurancy: 0.3302009772512291
Global test_loss: 1.9488636875152587
Global Precision: 0.34126092616493264
Global Recall: 0.3302009772512291
Global f1score: 0.3191470695328074
50
50
number of selected users 50
Global Trainning Accurancy: 0.331526889852721
Global Trainning Loss: 1.9373810243606568
Global test accurancy: 0.3332082164885498
Global test_loss: 1.9419958949089051
Global Precision: 0.34363399047694104
Global Recall: 0.3332082164885498
Global f1score: 0.3228840009016301
50
50
number of selected users 50
Global Trainning Accurancy: 0.3344685395305186
Global Trainning Loss: 1.9308377027511596
Global test accurancy: 0.3356338552743194
Global test_loss: 1.9353024792671203
Global Precision: 0.345521785728444
Global Recall: 0.3356338552743194
Global f1score: 0.325909893662855
50
50
number of selected users 50
Global Trainning Accurancy: 0.33729666006126346
Global Trainning Loss: 1.9244330644607544
Global test accurancy: 0.3384362930005359
Global test_loss: 1.928722312450409
Global Precision: 0.34886470535164166
Global Recall: 0.3384362930005359
Global f1score: 0.3294437368892387
50
50
number of selected users 50
Global Trainning Accurancy: 0.34057740881779874
Global Trainning Loss: 1.9181152510643005
Global test accurancy: 0.34062416157169767
Global test_loss: 1.9221882843971252
Global Precision: 0.3507963177648882
Global Recall: 0.34062416157169767
Global f1score: 0.3321274315556764
50
50
number of selected users 50
Global Trainning Accurancy: 0.34324671521445904
Global Trainning Loss: 1.9119594955444337
Global test accurancy: 0.3450858573675499
Global test_loss: 1.9158943700790405
Global Precision: 0.35478713345278556
Global Recall: 0.3450858573675499
Global f1score: 0.33713569349729755
50
50
number of selected users 50
Global Trainning Accurancy: 0.346272918211323
Global Trainning Loss: 1.9059370350837708
Global test accurancy: 0.3475360428982704
Global test_loss: 1.9097301745414734
Global Precision: 0.3565191709910632
Global Recall: 0.3475360428982704
Global f1score: 0.3399689682508595
50
50
number of selected users 50
Global Trainning Accurancy: 0.34950920730400686
Global Trainning Loss: 1.9000592064857482
Global test accurancy: 0.3499461938461946
Global test_loss: 1.9037533545494079
Global Precision: 0.358832685295157
Global Recall: 0.3499461938461946
Global f1score: 0.3427204126035025
50
50
number of selected users 50
Global Trainning Accurancy: 0.35228297293961897
Global Trainning Loss: 1.8943323612213134
Global test accurancy: 0.3515426023924005
Global test_loss: 1.8980202674865723
Global Precision: 0.3600790127235633
Global Recall: 0.3515426023924005
Global f1score: 0.34455220930092806
50
50
number of selected users 50
Global Trainning Accurancy: 0.3549890626275549
Global Trainning Loss: 1.8888047075271606
Global test accurancy: 0.3550811273108781
Global test_loss: 1.8924937319755555
Global Precision: 0.36375373341956974
Global Recall: 0.3550811273108781
Global f1score: 0.3484296911787256
50
50
number of selected users 50
Global Trainning Accurancy: 0.35669496917873117
Global Trainning Loss: 1.883447813987732
Global test accurancy: 0.35786509694850605
Global test_loss: 1.8872185111045838
Global Precision: 0.3663477743575696
Global Recall: 0.35786509694850605
Global f1score: 0.3515720672850253
50
50
number of selected users 50
Global Trainning Accurancy: 0.3600028612911999
Global Trainning Loss: 1.8782439827919006
Global test accurancy: 0.3608704778067765
Global test_loss: 1.8821426057815551
Global Precision: 0.37000570912692254
Global Recall: 0.3608704778067765
Global f1score: 0.35501221236919595
50
50
number of selected users 50
Global Trainning Accurancy: 0.3618798796872915
Global Trainning Loss: 1.873215765953064
Global test accurancy: 0.36317442754910095
Global test_loss: 1.87727064371109
Global Precision: 0.3719722473896906
Global Recall: 0.36317442754910095
Global f1score: 0.35746819715799844
50
50
number of selected users 50
Global Trainning Accurancy: 0.3639766546233755
Global Trainning Loss: 1.868337163925171
Global test accurancy: 0.3642977534449522
Global test_loss: 1.872579071521759
Global Precision: 0.37230925269740267
Global Recall: 0.3642977534449522
Global f1score: 0.3585350455395681
50
50
number of selected users 50
Global Trainning Accurancy: 0.3663644851633789
Global Trainning Loss: 1.8635518884658813
Global test accurancy: 0.3666373375971025
Global test_loss: 1.8680232977867126
Global Precision: 0.3748565011725271
Global Recall: 0.3666373375971025
Global f1score: 0.36108066611188006
50
50
number of selected users 50
Global Trainning Accurancy: 0.3682249161703822
Global Trainning Loss: 1.858886513710022
Global test accurancy: 0.36954681019941626
Global test_loss: 1.86365074634552
Global Precision: 0.37751829531675657
Global Recall: 0.36954681019941626
Global f1score: 0.3643253820837354
50
50
number of selected users 50
Global Trainning Accurancy: 0.3708104908014462
Global Trainning Loss: 1.8543058013916016
Global test accurancy: 0.3726188340475432
Global test_loss: 1.8593536972999574
Global Precision: 0.38096299306222664
Global Recall: 0.3726188340475432
Global f1score: 0.36765892982048454
50
50
number of selected users 50
Global Trainning Accurancy: 0.37314394368188836
Global Trainning Loss: 1.8498227977752686
Global test accurancy: 0.37495728230506226
Global test_loss: 1.8551682353019714
Global Precision: 0.3830871349950453
Global Recall: 0.37495728230506226
Global f1score: 0.3698843730830862
50
50
number of selected users 50
Global Trainning Accurancy: 0.37527166048098853
Global Trainning Loss: 1.845388741493225
Global test accurancy: 0.377241576754985
Global test_loss: 1.8510866355895996
Global Precision: 0.386197191050007
Global Recall: 0.377241576754985
Global f1score: 0.3725250024688145
50
50
number of selected users 50
Global Trainning Accurancy: 0.37756885705100435
Global Trainning Loss: 1.8410686898231505
Global test accurancy: 0.378376573949005
Global test_loss: 1.8471079754829407
Global Precision: 0.38700525119278945
Global Recall: 0.378376573949005
Global f1score: 0.3735993046727699
50
50
number of selected users 50
Global Trainning Accurancy: 0.3799335801108779
Global Trainning Loss: 1.8368244457244873
Global test accurancy: 0.3810606438419023
Global test_loss: 1.843238787651062
Global Precision: 0.3898561914646668
Global Recall: 0.3810606438419023
Global f1score: 0.3763536550098359
50
50
number of selected users 50
Global Trainning Accurancy: 0.3815446317083035
Global Trainning Loss: 1.8326948595046997
Global test accurancy: 0.3833627995979822
Global test_loss: 1.8394995403289796
Global Precision: 0.39229386143139516
Global Recall: 0.3833627995979822
Global f1score: 0.3788406892512155
50
50
number of selected users 50
Global Trainning Accurancy: 0.38269352748818547
Global Trainning Loss: 1.8285787653923036
Global test accurancy: 0.3857678800555482
Global test_loss: 1.8358072876930236
Global Precision: 0.3947078926066356
Global Recall: 0.3857678800555482
Global f1score: 0.3815184255160744
50
50
number of selected users 50
Global Trainning Accurancy: 0.38497512570914794
Global Trainning Loss: 1.8245543837547302
Global test accurancy: 0.38834191718949046
Global test_loss: 1.8322013640403747
Global Precision: 0.39747081346464125
Global Recall: 0.38834191718949046
Global f1score: 0.38446230303521894
50
50
number of selected users 50
Global Trainning Accurancy: 0.38785881701150376
Global Trainning Loss: 1.8205364632606507
Global test accurancy: 0.389408641702198
Global test_loss: 1.8286614322662353
Global Precision: 0.39834002695419296
Global Recall: 0.389408641702198
Global f1score: 0.3854883936774171
50
50
number of selected users 50
Global Trainning Accurancy: 0.3896908737337188
Global Trainning Loss: 1.8165352368354797
Global test accurancy: 0.39105562640968394
Global test_loss: 1.8251198196411134
Global Precision: 0.400191966628771
Global Recall: 0.39105562640968394
Global f1score: 0.38722349579310883
50
50
number of selected users 50
Global Trainning Accurancy: 0.39145423860981915
Global Trainning Loss: 1.8125425052642823
Global test accurancy: 0.39353290583230405
Global test_loss: 1.8216112804412843
Global Precision: 0.4023265700092144
Global Recall: 0.39353290583230405
Global f1score: 0.38979073601747743
50
50
number of selected users 50
Global Trainning Accurancy: 0.39310526771607224
Global Trainning Loss: 1.8085958075523376
Global test accurancy: 0.39434981821959336
Global test_loss: 1.8181669306755066
Global Precision: 0.4029776515836388
Global Recall: 0.39434981821959336
Global f1score: 0.3905493540281363
50
50
number of selected users 50
Global Trainning Accurancy: 0.39568181838636296
Global Trainning Loss: 1.804723961353302
Global test accurancy: 0.39549833388125
Global test_loss: 1.8148085427284242
Global Precision: 0.40393344333166364
Global Recall: 0.39549833388125
Global f1score: 0.39176278807225173
50
50
number of selected users 50
Global Trainning Accurancy: 0.39808749030592344
Global Trainning Loss: 1.8008561754226684
Global test accurancy: 0.3968906009869979
Global test_loss: 1.8114632439613343
Global Precision: 0.40510334977202045
Global Recall: 0.3968906009869979
Global f1score: 0.39314413992190383
50
50
number of selected users 50
Global Trainning Accurancy: 0.400476724992677
Global Trainning Loss: 1.796979513168335
Global test accurancy: 0.3984391108250163
Global test_loss: 1.8081809520721435
Global Precision: 0.40616597588782194
Global Recall: 0.3984391108250163
Global f1score: 0.39469861503220094
50
50
number of selected users 50
Global Trainning Accurancy: 0.40237523996512536
Global Trainning Loss: 1.7931069946289062
Global test accurancy: 0.4007157950086258
Global test_loss: 1.804931743144989
Global Precision: 0.40831859754278427
Global Recall: 0.4007157950086258
Global f1score: 0.39710847848544056
50
50
number of selected users 50
Global Trainning Accurancy: 0.4044688100102529
Global Trainning Loss: 1.7892308831214905
Global test accurancy: 0.40271280151131367
Global test_loss: 1.8017134594917297
Global Precision: 0.41017238165143094
Global Recall: 0.40271280151131367
Global f1score: 0.3991618737995538
50
50
number of selected users 50
Global Trainning Accurancy: 0.4067591646985751
Global Trainning Loss: 1.7853519797325135
Global test accurancy: 0.4054428363176825
Global test_loss: 1.7984995222091675
Global Precision: 0.41301658020361465
Global Recall: 0.4054428363176825
Global f1score: 0.40190129717093487
50
50
number of selected users 50
Global Trainning Accurancy: 0.4088912456075065
Global Trainning Loss: 1.781426830291748
Global test accurancy: 0.40720507112207543
Global test_loss: 1.7953579306602478
Global Precision: 0.4148092396244246
Global Recall: 0.40720507112207543
Global f1score: 0.40391718867729964
50
50
number of selected users 50
Global Trainning Accurancy: 0.4111714577532414
Global Trainning Loss: 1.7775961756706238
Global test accurancy: 0.4099520074398929
Global test_loss: 1.7922597312927246
Global Precision: 0.41742758545573183
Global Recall: 0.4099520074398929
Global f1score: 0.4065293950266863
50
50
number of selected users 50
Global Trainning Accurancy: 0.4131428599683023
Global Trainning Loss: 1.773789393901825
Global test accurancy: 0.41227286446341915
Global test_loss: 1.7891234970092773
Global Precision: 0.41913703996401214
Global Recall: 0.41227286446341915
Global f1score: 0.4086545498399481
50
50
number of selected users 50
Global Trainning Accurancy: 0.41563209040003374
Global Trainning Loss: 1.7699222493171691
Global test accurancy: 0.41408689576603386
Global test_loss: 1.7860546851158141
Global Precision: 0.4206576869786183
Global Recall: 0.41408689576603386
Global f1score: 0.41054331537755484
50
50
number of selected users 50
Global Trainning Accurancy: 0.41806977667768436
Global Trainning Loss: 1.7661648797988891
Global test accurancy: 0.41639686193507075
Global test_loss: 1.7831249356269836
Global Precision: 0.423188666820718
Global Recall: 0.41639686193507075
Global f1score: 0.4131673638065135
50
50
number of selected users 50
Global Trainning Accurancy: 0.4196854760653475
Global Trainning Loss: 1.762447612285614
Global test accurancy: 0.41804462640815016
Global test_loss: 1.7802995038032532
Global Precision: 0.4243346322296869
Global Recall: 0.41804462640815016
Global f1score: 0.41481228682316906
50
50
number of selected users 50
Global Trainning Accurancy: 0.42178826635046557
Global Trainning Loss: 1.7588154315948485
Global test accurancy: 0.4187963639373751
Global test_loss: 1.777392132282257
Global Precision: 0.42432107241746453
Global Recall: 0.4187963639373751
Global f1score: 0.4152687667292757
50
50
number of selected users 50
Global Trainning Accurancy: 0.42433198771144387
Global Trainning Loss: 1.7552659344673156
Global test accurancy: 0.4193384464959448
Global test_loss: 1.7746982192993164
Global Precision: 0.4249659749421357
Global Recall: 0.4193384464959448
Global f1score: 0.4157893970088284
50
50
number of selected users 50
Global Trainning Accurancy: 0.42558877584381793
Global Trainning Loss: 1.7516414618492127
Global test accurancy: 0.42093417910732
Global test_loss: 1.771917450428009
Global Precision: 0.4260566169017089
Global Recall: 0.42093417910732
Global f1score: 0.4170916671080345
50
50
number of selected users 50
Global Trainning Accurancy: 0.4270751130453144
Global Trainning Loss: 1.7478097510337829
Global test accurancy: 0.42219802269754425
Global test_loss: 1.7691417360305786
Global Precision: 0.4270019809424944
Global Recall: 0.42219802269754425
Global f1score: 0.4183879451847522
50
50
number of selected users 50
Global Trainning Accurancy: 0.4288426783453853
Global Trainning Loss: 1.7441269493103027
Global test accurancy: 0.4230413967444984
Global test_loss: 1.7665599060058594
Global Precision: 0.4281293401818054
Global Recall: 0.4230413967444984
Global f1score: 0.4194625423809162
50
50
number of selected users 50
Global Trainning Accurancy: 0.42937517985303403
Global Trainning Loss: 1.7408166313171387
Global test accurancy: 0.42273664811340034
Global test_loss: 1.7641863012313843
Global Precision: 0.4272708231385392
Global Recall: 0.42273664811340034
Global f1score: 0.41882837568037734
50
50
number of selected users 50
Global Trainning Accurancy: 0.43067578563898173
Global Trainning Loss: 1.7369100427627564
Global test accurancy: 0.4258835925612889
Global test_loss: 1.7614945554733277
Global Precision: 0.4313351711803988
Global Recall: 0.4258835925612889
Global f1score: 0.42261224503723294
50
50
number of selected users 50
Global Trainning Accurancy: 0.43285596513433466
Global Trainning Loss: 1.733408148288727
Global test accurancy: 0.427266295498864
Global test_loss: 1.75917151927948
Global Precision: 0.43258994218795527
Global Recall: 0.427266295498864
Global f1score: 0.42398432441051886
50
50
number of selected users 50
Global Trainning Accurancy: 0.4343372104774421
Global Trainning Loss: 1.729824151992798
Global test accurancy: 0.4281653921237681
Global test_loss: 1.7567653465270996
Global Precision: 0.43373398249729744
Global Recall: 0.4281653921237681
Global f1score: 0.42496608560301025
50
50
number of selected users 50
Global Trainning Accurancy: 0.4364912831077027
Global Trainning Loss: 1.7264759182929992
Global test accurancy: 0.42979312617869214
Global test_loss: 1.7545127511024474
Global Precision: 0.43497301728510523
Global Recall: 0.42979312617869214
Global f1score: 0.426340090640455
50
50
number of selected users 50
Global Trainning Accurancy: 0.4383087389186375
Global Trainning Loss: 1.7229501271247865
Global test accurancy: 0.43148620257455406
Global test_loss: 1.7522571516036987
Global Precision: 0.43651438432131706
Global Recall: 0.43148620257455406
Global f1score: 0.4279722757790512
50
50
number of selected users 50
Global Trainning Accurancy: 0.43916380301625235
Global Trainning Loss: 1.719323880672455
Global test accurancy: 0.4323555354060179
Global test_loss: 1.7501275491714479
Global Precision: 0.43778822501246856
Global Recall: 0.4323555354060179
Global f1score: 0.4291466739154174
50
50
number of selected users 50
Global Trainning Accurancy: 0.4410489879210688
Global Trainning Loss: 1.715759449005127
Global test accurancy: 0.43231877546848296
Global test_loss: 1.7480058574676514
Global Precision: 0.4378873877552891
Global Recall: 0.43231877546848296
Global f1score: 0.42908951161567827
50
50
number of selected users 50
Global Trainning Accurancy: 0.4421471914358026
Global Trainning Loss: 1.7123281240463257
Global test accurancy: 0.43464960509555456
Global test_loss: 1.7462464237213136
Global Precision: 0.4410029566304257
Global Recall: 0.43464960509555456
Global f1score: 0.43174369453874717
50
50
number of selected users 50
Global Trainning Accurancy: 0.4442434534934582
Global Trainning Loss: 1.7089668941497802
Global test accurancy: 0.4343395584950976
Global test_loss: 1.7443055868148805
Global Precision: 0.43989977913585904
Global Recall: 0.4343395584950976
Global f1score: 0.43074683912542727
50
50
number of selected users 50
Global Trainning Accurancy: 0.4460685100049804
Global Trainning Loss: 1.7055227184295654
Global test accurancy: 0.43599500203197034
Global test_loss: 1.7424055004119874
Global Precision: 0.44138272244939325
Global Recall: 0.43599500203197034
Global f1score: 0.4325493019177547
50
50
number of selected users 50
Global Trainning Accurancy: 0.44765579912520237
Global Trainning Loss: 1.7019765448570252
Global test accurancy: 0.43708348199092073
Global test_loss: 1.740716404914856
Global Precision: 0.442566033060889
Global Recall: 0.43708348199092073
Global f1score: 0.43389842579126714
50
50
number of selected users 50
Global Trainning Accurancy: 0.44896003553486874
Global Trainning Loss: 1.6986772751808166
Global test accurancy: 0.43718415915286496
Global test_loss: 1.7389608812332153
Global Precision: 0.4426104131445432
Global Recall: 0.43718415915286496
Global f1score: 0.43394099438649575
50
50
number of selected users 50
Global Trainning Accurancy: 0.45124438786155996
Global Trainning Loss: 1.6954166436195373
Global test accurancy: 0.43958291286745055
Global test_loss: 1.7374509072303772
Global Precision: 0.44468282171645224
Global Recall: 0.43958291286745055
Global f1score: 0.43589431913465015
50
50
number of selected users 50
Global Trainning Accurancy: 0.45275025168401223
Global Trainning Loss: 1.6919404339790345
Global test accurancy: 0.4389872553456463
Global test_loss: 1.735699462890625
Global Precision: 0.44416134091288395
Global Recall: 0.4389872553456463
Global f1score: 0.43534077458204856
50
50
number of selected users 50
Global Trainning Accurancy: 0.4551541572738193
Global Trainning Loss: 1.6885971069335937
Global test accurancy: 0.44129690161078794
Global test_loss: 1.734292550086975
Global Precision: 0.4463333445400115
Global Recall: 0.44129690161078794
Global f1score: 0.4380305770829857
50
50
number of selected users 50
Global Trainning Accurancy: 0.4558434099964259
Global Trainning Loss: 1.6851666045188904
Global test accurancy: 0.4416254288257567
Global test_loss: 1.7326183199882508
Global Precision: 0.4469382207203356
Global Recall: 0.4416254288257567
Global f1score: 0.43835769389743395
50
50
number of selected users 50
Global Trainning Accurancy: 0.4584126995942005
Global Trainning Loss: 1.6814106059074403
Global test accurancy: 0.44134332786466285
Global test_loss: 1.7309082984924316
Global Precision: 0.4471082143975647
Global Recall: 0.44134332786466285
Global f1score: 0.43831990906858986
50
50
number of selected users 50
Global Trainning Accurancy: 0.4594349471650057
Global Trainning Loss: 1.6783042192459106
Global test accurancy: 0.44322042858457106
Global test_loss: 1.7298174047470092
Global Precision: 0.44891594042060373
Global Recall: 0.44322042858457106
Global f1score: 0.4402415619321471
50
50
number of selected users 50
Global Trainning Accurancy: 0.46183517192649387
Global Trainning Loss: 1.6747093057632447
Global test accurancy: 0.44311530458366466
Global test_loss: 1.728112554550171
Global Precision: 0.44852071019612255
Global Recall: 0.44311530458366466
Global f1score: 0.439931149614688
50
50
number of selected users 50
Global Trainning Accurancy: 0.463548832978773
Global Trainning Loss: 1.6715073442459107
Global test accurancy: 0.44474303742817545
Global test_loss: 1.7268673539161683
Global Precision: 0.45055028163443417
Global Recall: 0.44474303742817545
Global f1score: 0.44189639898738453
50
50
number of selected users 50
Global Trainning Accurancy: 0.4654502681275307
Global Trainning Loss: 1.6678400993347169
Global test accurancy: 0.44682045693625283
Global test_loss: 1.725312647819519
Global Precision: 0.4523796890836639
Global Recall: 0.44682045693625283
Global f1score: 0.4438647431495263
50
50
number of selected users 50
Global Trainning Accurancy: 0.4670866116456906
Global Trainning Loss: 1.6642142629623413
Global test accurancy: 0.4476867048871781
Global test_loss: 1.7239817571640015
Global Precision: 0.453193333490646
Global Recall: 0.4476867048871781
Global f1score: 0.4448351544698788
50
50
number of selected users 50
Global Trainning Accurancy: 0.4682261690395126
Global Trainning Loss: 1.6608492946624756
Global test accurancy: 0.44985153599960287
Global test_loss: 1.7227443385124206
Global Precision: 0.4557405951966379
Global Recall: 0.44985153599960287
Global f1score: 0.44704787523776374
50
50
number of selected users 50
Global Trainning Accurancy: 0.4704316027138588
Global Trainning Loss: 1.6571688842773438
Global test accurancy: 0.4505952798389702
Global test_loss: 1.721465184688568
Global Precision: 0.4564815666844966
Global Recall: 0.4505952798389702
Global f1score: 0.4477982716479808
50
50
number of selected users 50
Global Trainning Accurancy: 0.4715595244014103
Global Trainning Loss: 1.6535190105438233
Global test accurancy: 0.45190206850034986
Global test_loss: 1.7203780317306518
Global Precision: 0.4575497088174651
Global Recall: 0.45190206850034986
Global f1score: 0.4492733312535483
50
50
number of selected users 50
Global Trainning Accurancy: 0.47333992919021517
Global Trainning Loss: 1.6499362540245057
Global test accurancy: 0.45359889298589806
Global test_loss: 1.7195652651786804
Global Precision: 0.45963467205559355
Global Recall: 0.45359889298589806
Global f1score: 0.45133789207465885
50
50
number of selected users 50
Global Trainning Accurancy: 0.47442670038858875
Global Trainning Loss: 1.646577503681183
Global test accurancy: 0.4548558367360493
Global test_loss: 1.7186636757850646
Global Precision: 0.46067620968955353
Global Recall: 0.4548558367360493
Global f1score: 0.45237241420350693
50
50
number of selected users 50
Global Trainning Accurancy: 0.47577906088112415
Global Trainning Loss: 1.642864875793457
Global test accurancy: 0.4533378960178365
Global test_loss: 1.7177593541145324
Global Precision: 0.45910645776349196
Global Recall: 0.4533378960178365
Global f1score: 0.4508090828106238
50
50
number of selected users 50
Global Trainning Accurancy: 0.4765780353359505
Global Trainning Loss: 1.639347972869873
Global test accurancy: 0.4556970947867324
Global test_loss: 1.716977560520172
Global Precision: 0.46182362304332186
Global Recall: 0.4556970947867324
Global f1score: 0.45331415514136014
50
50
number of selected users 50
Global Trainning Accurancy: 0.4784542677276168
Global Trainning Loss: 1.635464253425598
Global test accurancy: 0.4571776781770753
Global test_loss: 1.7161988496780396
Global Precision: 0.46375129787632396
Global Recall: 0.4571776781770753
Global f1score: 0.4551864230540101
50
50
number of selected users 50
Global Trainning Accurancy: 0.48028154761864394
Global Trainning Loss: 1.6321604871749877
Global test accurancy: 0.4583058390114917
Global test_loss: 1.7161087799072265
Global Precision: 0.465118861891367
Global Recall: 0.4583058390114917
Global f1score: 0.4561293845527246
50
50
number of selected users 50
Global Trainning Accurancy: 0.4822070749810111
Global Trainning Loss: 1.628172459602356
Global test accurancy: 0.4572382709733466
Global test_loss: 1.7154186844825745
Global Precision: 0.4639850349254932
Global Recall: 0.4572382709733466
Global f1score: 0.4554453539989483
50
50
number of selected users 50
Global Trainning Accurancy: 0.48320890002698674
Global Trainning Loss: 1.6247253870964051
Global test accurancy: 0.4588814381842772
Global test_loss: 1.715103075504303
Global Precision: 0.4656334983300302
Global Recall: 0.4588814381842772
Global f1score: 0.4568984702340311
50
50
number of selected users 50
Global Trainning Accurancy: 0.485532901728795
Global Trainning Loss: 1.6208531951904297
Global test accurancy: 0.45941001125429437
Global test_loss: 1.7144834446907042
Global Precision: 0.46645640946069544
Global Recall: 0.45941001125429437
Global f1score: 0.4575565358082882
50
50
number of selected users 50
Global Trainning Accurancy: 0.4864939477787721
Global Trainning Loss: 1.617043821811676
Global test accurancy: 0.4605114251041114
Global test_loss: 1.7143627548217772
Global Precision: 0.46746219418749635
Global Recall: 0.4605114251041114
Global f1score: 0.45869186722440064
50
50
number of selected users 50
Global Trainning Accurancy: 0.4875467203513218
Global Trainning Loss: 1.6133236908912658
Global test accurancy: 0.4606065754435995
Global test_loss: 1.7140034198760987
Global Precision: 0.46703644294721913
Global Recall: 0.4606065754435995
Global f1score: 0.4583209645394327
50
50
number of selected users 50
Global Trainning Accurancy: 0.4881847481289458
Global Trainning Loss: 1.61048565864563
Global test accurancy: 0.46047566814227914
Global test_loss: 1.7149713301658631
Global Precision: 0.4681801947149754
Global Recall: 0.46047566814227914
Global f1score: 0.45870063655343907
50
50
number of selected users 50
Global Trainning Accurancy: 0.4913138726448069
Global Trainning Loss: 1.605353775024414
Global test accurancy: 0.46202614337350734
Global test_loss: 1.7134908866882324
Global Precision: 0.468246820367351
Global Recall: 0.46202614337350734
Global f1score: 0.4593907350912484
50
50
number of selected users 50
Global Trainning Accurancy: 0.4916734829226495
Global Trainning Loss: 1.6027497935295105
Global test accurancy: 0.46403223684678857
Global test_loss: 1.714549629688263
Global Precision: 0.4706802044665706
Global Recall: 0.46403223684678857
Global f1score: 0.4613743045936221
50
50
number of selected users 50
Global Trainning Accurancy: 0.4936010807722123
Global Trainning Loss: 1.5984323382377625
Global test accurancy: 0.46518686685890537
Global test_loss: 1.714793918132782
Global Precision: 0.47305251824044625
Global Recall: 0.46518686685890537
Global f1score: 0.46345021090784017
50
50
number of selected users 50
Global Trainning Accurancy: 0.4954491165749953
Global Trainning Loss: 1.594401364326477
Global test accurancy: 0.4647993264462709
Global test_loss: 1.7152211928367616
Global Precision: 0.4718725025084761
Global Recall: 0.4647993264462709
Global f1score: 0.4627862314576766
50
50
number of selected users 50
Global Trainning Accurancy: 0.49672084110006376
Global Trainning Loss: 1.5902340030670166
Global test accurancy: 0.4658028035036067
Global test_loss: 1.7158535766601561
Global Precision: 0.47298898853749155
Global Recall: 0.4658028035036067
Global f1score: 0.463770782029375
50
50
number of selected users 50
Global Trainning Accurancy: 0.49765659740938323
Global Trainning Loss: 1.5873965907096863
Global test accurancy: 0.465685065374596
Global test_loss: 1.7169783210754395
Global Precision: 0.47336442006425944
Global Recall: 0.465685065374596
Global f1score: 0.46337544468853603
50
50
number of selected users 50
Global Trainning Accurancy: 0.5009777611941948
Global Trainning Loss: 1.5816958951950073
Global test accurancy: 0.466487620665537
Global test_loss: 1.716333291530609
Global Precision: 0.4731885056204475
Global Recall: 0.466487620665537
Global f1score: 0.46475478366827444
50
50
number of selected users 50
Global Trainning Accurancy: 0.5010031928310856
Global Trainning Loss: 1.5795547199249267
Global test accurancy: 0.4658596390999413
Global test_loss: 1.718994517326355
Global Precision: 0.4734274209231384
Global Recall: 0.4658596390999413
Global f1score: 0.46321665659734995
50
50
number of selected users 50
Global Trainning Accurancy: 0.5022866944645996
Global Trainning Loss: 1.5761132836341858
Global test accurancy: 0.46531029822467995
Global test_loss: 1.7206798601150513
Global Precision: 0.4730110786965049
Global Recall: 0.46531029822467995
Global f1score: 0.462243053322499
50
50
number of selected users 50
Global Trainning Accurancy: 0.505846955964788
Global Trainning Loss: 1.571110622882843
Global test accurancy: 0.46625164260542207
Global test_loss: 1.7209055399894715
Global Precision: 0.47429289503423766
Global Recall: 0.46625164260542207
Global f1score: 0.46394861876966353
50
50
number of selected users 50
Global Trainning Accurancy: 0.5070463592520242
Global Trainning Loss: 1.5659168243408204
Global test accurancy: 0.4665665205673465
Global test_loss: 1.721353874206543
Global Precision: 0.4745024975392538
Global Recall: 0.4665665205673465
Global f1score: 0.46447044775627133
50
50
number of selected users 50
Global Trainning Accurancy: 0.5083779462568726
Global Trainning Loss: 1.5603790593147278
Global test accurancy: 0.46703048786351026
Global test_loss: 1.7215843844413756
Global Precision: 0.4739481672590568
Global Recall: 0.46703048786351026
Global f1score: 0.4647407562080805
50
50
number of selected users 50
Global Trainning Accurancy: 0.5100449349343295
Global Trainning Loss: 1.5558609366416931
Global test accurancy: 0.46780842356228536
Global test_loss: 1.723253970146179
Global Precision: 0.47444577874596366
Global Recall: 0.46780842356228536
Global f1score: 0.4656085790606901
50
50
number of selected users 50
Global Trainning Accurancy: 0.5126004256436464
Global Trainning Loss: 1.5526232028007507
Global test accurancy: 0.4673809519088514
Global test_loss: 1.726113154888153
Global Precision: 0.47437829290779127
Global Recall: 0.4673809519088514
Global f1score: 0.465337533294892
50
50
number of selected users 50
Global Trainning Accurancy: 0.5122009015791497
Global Trainning Loss: 1.5515147542953491
Global test accurancy: 0.4688029545646202
Global test_loss: 1.7311527371406554
Global Precision: 0.4770289498024858
Global Recall: 0.4688029545646202
Global f1score: 0.46640188597098964
50
50
number of selected users 50
Global Trainning Accurancy: 0.5149474078164004
Global Trainning Loss: 1.5445832681655884
Global test accurancy: 0.46811806374689763
Global test_loss: 1.730494019985199
Global Precision: 0.4751474331356975
Global Recall: 0.46811806374689763
Global f1score: 0.4658103083947888
50
50
number of selected users 50
Global Trainning Accurancy: 0.5165310935295166
Global Trainning Loss: 1.540794425010681
Global test accurancy: 0.4685221112190122
Global test_loss: 1.7331046915054322
Global Precision: 0.4766833005098326
Global Recall: 0.4685221112190122
Global f1score: 0.46644606876260913
50
50
number of selected users 50
Global Trainning Accurancy: 0.5178569066997106
Global Trainning Loss: 1.5364268851280212
Global test accurancy: 0.4691066056824369
Global test_loss: 1.7369244503974914
Global Precision: 0.47660675597971913
Global Recall: 0.4691066056824369
Global f1score: 0.46688348686736714
50
50
number of selected users 50
Global Trainning Accurancy: 0.5188728306907846
Global Trainning Loss: 1.531393210887909
Global test accurancy: 0.4694250243645858
Global test_loss: 1.7391977190971375
Global Precision: 0.477901234899907
Global Recall: 0.4694250243645858
Global f1score: 0.46807412151896993
50
50
number of selected users 50
Global Trainning Accurancy: 0.5210339281952127
Global Trainning Loss: 1.5271263360977172
Global test accurancy: 0.46960393124652444
Global test_loss: 1.7416114807128906
Global Precision: 0.47769169576567055
Global Recall: 0.46960393124652444
Global f1score: 0.46757618350027597
50
50
number of selected users 50
Global Trainning Accurancy: 0.5227768589776765
Global Trainning Loss: 1.5219200944900513
Global test accurancy: 0.4684538766125291
Global test_loss: 1.7446400809288025
Global Precision: 0.4752537064496699
Global Recall: 0.4684538766125291
Global f1score: 0.4657625670361521
50
50
number of selected users 50
Global Trainning Accurancy: 0.5243544932274126
Global Trainning Loss: 1.5191641855239868
Global test accurancy: 0.4694007497404542
Global test_loss: 1.7492534708976746
Global Precision: 0.47673613561550027
Global Recall: 0.4694007497404542
Global f1score: 0.4667189716206103
50
50
number of selected users 50
Global Trainning Accurancy: 0.526056192989771
Global Trainning Loss: 1.51429936170578
Global test accurancy: 0.47036542778199825
Global test_loss: 1.7535071086883545
Global Precision: 0.4791566310775206
Global Recall: 0.47036542778199825
Global f1score: 0.46874690029902416
50
50
number of selected users 50
Global Trainning Accurancy: 0.5276716399880674
Global Trainning Loss: 1.509028935432434
Global test accurancy: 0.47066606597916566
Global test_loss: 1.756161367893219
Global Precision: 0.4787011080379975
Global Recall: 0.47066606597916566
Global f1score: 0.46892026914697604
50
50
number of selected users 50
Global Trainning Accurancy: 0.5293696621916265
Global Trainning Loss: 1.5043066334724426
Global test accurancy: 0.4704362929822549
Global test_loss: 1.7598324942588806
Global Precision: 0.47779461094265396
Global Recall: 0.4704362929822549
Global f1score: 0.46818170299612333
50
50
number of selected users 50
Global Trainning Accurancy: 0.5297101545933413
Global Trainning Loss: 1.5017329812049867
Global test accurancy: 0.4681865192191821
Global test_loss: 1.7644242763519287
Global Precision: 0.4760887596694653
Global Recall: 0.4681865192191821
Global f1score: 0.46526206695364286
50
50
number of selected users 50
Global Trainning Accurancy: 0.5316502897631296
Global Trainning Loss: 1.497691216468811
Global test accurancy: 0.469600442526461
Global test_loss: 1.771155002117157
Global Precision: 0.47637178190371254
Global Recall: 0.469600442526461
Global f1score: 0.4662225098565384
50
50
number of selected users 50
Global Trainning Accurancy: 0.5331737864758223
Global Trainning Loss: 1.4908766674995422
Global test accurancy: 0.4714447785168839
Global test_loss: 1.7734793949127197
Global Precision: 0.4790708098886495
Global Recall: 0.4714447785168839
Global f1score: 0.46939344489054147
50
50
number of selected users 50
Global Trainning Accurancy: 0.5361547056718255
Global Trainning Loss: 1.4863536882400512
Global test accurancy: 0.46962510355478704
Global test_loss: 1.7774987268447875
Global Precision: 0.47779761296684325
Global Recall: 0.46962510355478704
Global f1score: 0.468148971882709
50
50
number of selected users 50
Global Trainning Accurancy: 0.5372573328713769
Global Trainning Loss: 1.4816434049606324
Global test accurancy: 0.47031307481271206
Global test_loss: 1.7827776455879212
Global Precision: 0.47738572852078287
Global Recall: 0.47031307481271206
Global f1score: 0.4685107679290607
50
50
number of selected users 50
Global Trainning Accurancy: 0.5394349145184884
Global Trainning Loss: 1.476660659313202
Global test accurancy: 0.4706183166153844
Global test_loss: 1.7885068678855895
Global Precision: 0.47746063929712274
Global Recall: 0.4706183166153844
Global f1score: 0.4680873489783813
50
50
number of selected users 50
Global Trainning Accurancy: 0.5391035950537901
Global Trainning Loss: 1.476356587409973
Global test accurancy: 0.47041348771181474
Global test_loss: 1.7987366366386413
Global Precision: 0.47804172888422525
Global Recall: 0.47041348771181474
Global f1score: 0.46749983199628864
50
50
number of selected users 50
Global Trainning Accurancy: 0.5407479653750985
Global Trainning Loss: 1.4711490225791932
Global test accurancy: 0.47136409972888155
Global test_loss: 1.8043041157722473
Global Precision: 0.4795783924420489
Global Recall: 0.47136409972888155
Global f1score: 0.4691596278976542
50
50
number of selected users 50
Global Trainning Accurancy: 0.5428480062455755
Global Trainning Loss: 1.466659197807312
Global test accurancy: 0.47057490795650464
Global test_loss: 1.8107204461097717
Global Precision: 0.47769246536368937
Global Recall: 0.47057490795650464
Global f1score: 0.46809513743454034
50
50
number of selected users 50
Global Trainning Accurancy: 0.5447190407347302
Global Trainning Loss: 1.4615753650665284
Global test accurancy: 0.4698973994855942
Global test_loss: 1.8188259625434875
Global Precision: 0.4769731644999333
Global Recall: 0.4698973994855942
Global f1score: 0.4680157331834629
50
50
number of selected users 50
Global Trainning Accurancy: 0.5450066690145091
Global Trainning Loss: 1.460200092792511
Global test accurancy: 0.4683049231648408
Global test_loss: 1.829311580657959
Global Precision: 0.4758160096097506
Global Recall: 0.4683049231648408
Global f1score: 0.4657779950053118
50
50
number of selected users 50
Global Trainning Accurancy: 0.5464735963336457
Global Trainning Loss: 1.45515394449234
Global test accurancy: 0.46770496537891215
Global test_loss: 1.835957465171814
Global Precision: 0.47539760005982834
Global Recall: 0.46770496537891215
Global f1score: 0.4649785163311266
50
50
number of selected users 50
Global Trainning Accurancy: 0.5480821691853824
Global Trainning Loss: 1.4499561309814453
Global test accurancy: 0.46908385272006753
Global test_loss: 1.8431703758239746
Global Precision: 0.4768853391850888
Global Recall: 0.46908385272006753
Global f1score: 0.46650755702309044
50
50
number of selected users 50
Global Trainning Accurancy: 0.5499922158110283
Global Trainning Loss: 1.445977954864502
Global test accurancy: 0.46674814244726526
Global test_loss: 1.8507790112495421
Global Precision: 0.47496375213956843
Global Recall: 0.46674814244726526
Global f1score: 0.465144869387495
50
50
number of selected users 50
Global Trainning Accurancy: 0.5516449275464304
Global Trainning Loss: 1.4409733366966249
Global test accurancy: 0.4660800718381212
Global test_loss: 1.8607524371147155
Global Precision: 0.474082153763046
Global Recall: 0.4660800718381212
Global f1score: 0.4643870375689941
50
50
number of selected users 50
Global Trainning Accurancy: 0.553897987065396
Global Trainning Loss: 1.4354093503952026
Global test accurancy: 0.4662814878180882
Global test_loss: 1.869516499042511
Global Precision: 0.47367908090420874
Global Recall: 0.4662814878180882
Global f1score: 0.46452075780850116
50
50
number of selected users 50
Global Trainning Accurancy: 0.5538646977874401
Global Trainning Loss: 1.4344954895973205
Global test accurancy: 0.4663613783253788
Global test_loss: 1.881714973449707
Global Precision: 0.4744872620934095
Global Recall: 0.4663613783253788
Global f1score: 0.4643289684327916
50
50
number of selected users 50
Global Trainning Accurancy: 0.5574537703366375
Global Trainning Loss: 1.4276245498657227
Global test accurancy: 0.4661309613194286
Global test_loss: 1.8886223196983338
Global Precision: 0.47470077838092717
Global Recall: 0.4661309613194286
Global f1score: 0.46497682756088643
50
50
number of selected users 50
Global Trainning Accurancy: 0.5595316105828808
Global Trainning Loss: 1.4246889209747315
Global test accurancy: 0.46588713524099645
Global test_loss: 1.8996264672279357
Global Precision: 0.4737585095169228
Global Recall: 0.46588713524099645
Global f1score: 0.46425507012863415
50
50
number of selected users 50
Global Trainning Accurancy: 0.5601660995969686
Global Trainning Loss: 1.4229461789131164
Global test accurancy: 0.46719397831469867
Global test_loss: 1.914352011680603
Global Precision: 0.4751117560694722
Global Recall: 0.46719397831469867
Global f1score: 0.4650010350373924
50
50
number of selected users 50
Global Trainning Accurancy: 0.5602471854350076
Global Trainning Loss: 1.4218475008010865
Global test accurancy: 0.4629593994478832
Global test_loss: 1.9281180691719055
Global Precision: 0.4728622496509616
Global Recall: 0.4629593994478832
Global f1score: 0.46160268583672104
50
50
number of selected users 50
Global Trainning Accurancy: 0.5629013270869414
Global Trainning Loss: 1.4140096044540404
Global test accurancy: 0.4628895636506545
Global test_loss: 1.936337516307831
Global Precision: 0.4707385246485379
Global Recall: 0.4628895636506545
Global f1score: 0.46104547402290486
50
50
number of selected users 50
Global Trainning Accurancy: 0.5647306606990744
Global Trainning Loss: 1.4135475778579711
Global test accurancy: 0.46436067413863635
Global test_loss: 1.951452226638794
Global Precision: 0.47299751772841475
Global Recall: 0.46436067413863635
Global f1score: 0.46252280237568644
50
50
number of selected users 50
Global Trainning Accurancy: 0.5667383686881787
Global Trainning Loss: 1.408980712890625
Global test accurancy: 0.4639254674605867
Global test_loss: 1.9611655855178833
Global Precision: 0.47270021872512935
Global Recall: 0.4639254674605867
Global f1score: 0.46220303199177054
50
50
number of selected users 50
Global Trainning Accurancy: 0.5679576147474255
Global Trainning Loss: 1.4045671939849853
Global test accurancy: 0.46293144371436823
Global test_loss: 1.9734800386428832
Global Precision: 0.47142410431316784
Global Recall: 0.46293144371436823
Global f1score: 0.4611585094629055
50
50
number of selected users 50
Global Trainning Accurancy: 0.5696278579840882
Global Trainning Loss: 1.3997655987739563
Global test accurancy: 0.46235089465592194
Global test_loss: 1.9842212867736817
Global Precision: 0.47029640249047705
Global Recall: 0.46235089465592194
Global f1score: 0.4605767181435801
50
50
number of selected users 50
Global Trainning Accurancy: 0.571722381945012
Global Trainning Loss: 1.3969185543060303
Global test accurancy: 0.4621895365650246
Global test_loss: 1.9964942049980163
Global Precision: 0.46995794950964564
Global Recall: 0.4621895365650246
Global f1score: 0.46027541894197677
50
50
number of selected users 50
Global Trainning Accurancy: 0.5731989884051918
Global Trainning Loss: 1.3926665139198304
Global test accurancy: 0.46131716330454714
Global test_loss: 2.0070016622543334
Global Precision: 0.4691138359909638
Global Recall: 0.46131716330454714
Global f1score: 0.4594076584504536
50
50
number of selected users 50
Global Trainning Accurancy: 0.5756921710360029
Global Trainning Loss: 1.3881839394569397
Global test accurancy: 0.46139478878424395
Global test_loss: 2.0181872606277467
Global Precision: 0.4688032650390723
Global Recall: 0.46139478878424395
Global f1score: 0.45955897973119253
50
50
number of selected users 50
Global Trainning Accurancy: 0.5759525020874002
Global Trainning Loss: 1.3859906220436096
Global test accurancy: 0.46264839215299264
Global test_loss: 2.032025353908539
Global Precision: 0.47018345628637115
Global Recall: 0.46264839215299264
Global f1score: 0.46058283867181543
50
50
number of selected users 50
Global Trainning Accurancy: 0.5785209593273737
Global Trainning Loss: 1.381153838634491
Global test accurancy: 0.4624719984063418
Global test_loss: 2.0429425573349
Global Precision: 0.47008921207389814
Global Recall: 0.4624719984063418
Global f1score: 0.4606870733093109
50
50
number of selected users 50
Global Trainning Accurancy: 0.5806735945614817
Global Trainning Loss: 1.3761066222190856
Global test accurancy: 0.4605777391050137
Global test_loss: 2.052531731128693
Global Precision: 0.4678233285125579
Global Recall: 0.4605777391050137
Global f1score: 0.45873256463154183
50
50
number of selected users 50
Global Trainning Accurancy: 0.5826404756754033
Global Trainning Loss: 1.3721131229400634
Global test accurancy: 0.4613943197250917
Global test_loss: 2.0634491443634033
Global Precision: 0.46915297497308167
Global Recall: 0.4613943197250917
Global f1score: 0.45964284277557516
50
50
number of selected users 50
Global Trainning Accurancy: 0.5832882238930859
Global Trainning Loss: 1.3685487699508667
Global test accurancy: 0.45961388413955506
Global test_loss: 2.0762398171424867
Global Precision: 0.4671487787322102
Global Recall: 0.45961388413955506
Global f1score: 0.45784924722417536
50
50
number of selected users 50
Global Trainning Accurancy: 0.585171802120916
Global Trainning Loss: 1.3645913457870484
Global test accurancy: 0.45937717182537985
Global test_loss: 2.0877084636688235
Global Precision: 0.4672144595743696
Global Recall: 0.45937717182537985
Global f1score: 0.45766911522957043
50
50
number of selected users 50
Global Trainning Accurancy: 0.5863294285780339
Global Trainning Loss: 1.3615793657302857
Global test accurancy: 0.45794967964859057
Global test_loss: 2.101143174171448
Global Precision: 0.46586393640048424
Global Recall: 0.45794967964859057
Global f1score: 0.45615269661182584
50
50
number of selected users 50
Global Trainning Accurancy: 0.5882983135125759
Global Trainning Loss: 1.356487171649933
Global test accurancy: 0.45808183305752415
Global test_loss: 2.11084038734436
Global Precision: 0.4658480031110161
Global Recall: 0.45808183305752415
Global f1score: 0.45646174588065647
50
50
number of selected users 50
Global Trainning Accurancy: 0.589045954682847
Global Trainning Loss: 1.3532028532028197
Global test accurancy: 0.45739047978382547
Global test_loss: 2.123618633747101
Global Precision: 0.46504212747909573
Global Recall: 0.45739047978382547
Global f1score: 0.45566878470290284
50
50
number of selected users 50
Global Trainning Accurancy: 0.5911006542000704
Global Trainning Loss: 1.3488877773284913
Global test accurancy: 0.45761850347727584
Global test_loss: 2.1372525024414064
Global Precision: 0.46566143542320687
Global Recall: 0.45761850347727584
Global f1score: 0.4560833462264327
50
50
number of selected users 50
Global Trainning Accurancy: 0.5922367087029613
Global Trainning Loss: 1.3448454642295837
Global test accurancy: 0.45716114376534067
Global test_loss: 2.147723159790039
Global Precision: 0.46513520058185615
Global Recall: 0.45716114376534067
Global f1score: 0.4557778380463217
50
50
number of selected users 50
Global Trainning Accurancy: 0.5939496560397683
Global Trainning Loss: 1.3407618141174316
Global test accurancy: 0.457297705841635
Global test_loss: 2.158838930130005
Global Precision: 0.4652756114999704
Global Recall: 0.457297705841635
Global f1score: 0.4558359843015919
50
50
number of selected users 50
Global Trainning Accurancy: 0.5957906615390482
Global Trainning Loss: 1.3360931062698365
Global test accurancy: 0.4566413104519011
Global test_loss: 2.1690564489364625
Global Precision: 0.46393710491802187
Global Recall: 0.4566413104519011
Global f1score: 0.4549867595240102
50
50
number of selected users 50
Global Trainning Accurancy: 0.5967820679838253
Global Trainning Loss: 1.33206880569458
Global test accurancy: 0.4568903168349072
Global test_loss: 2.179957616329193
Global Precision: 0.46445114493602146
Global Recall: 0.4568903168349072
Global f1score: 0.4552064595083119
50
50
number of selected users 50
Global Trainning Accurancy: 0.598333253251985
Global Trainning Loss: 1.3280827760696412
Global test accurancy: 0.45802718754959215
Global test_loss: 2.1898903584480287
Global Precision: 0.46567452490040406
Global Recall: 0.45802718754959215
Global f1score: 0.4562863941479071
exp_no  0
0_dataset_CIFAR10_algorithm_FedProx_model_CNN_10_50_0.2_31_07_2024
