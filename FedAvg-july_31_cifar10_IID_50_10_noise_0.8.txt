============================================================
Summary of training process:
FL Algorithm: FedAvg
model: CNN
optimizer: SGD
Batch size: 124
Global_iters: 200
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
============================================================
/proj/bhuyan24/fed-divergence
CIFAR10
./data/data/noisy/0.8_50_10/train/cifa_train.json
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:18<1:00:50, 18.34s/it]  1%|          | 2/200 [00:27<41:44, 12.65s/it]    2%|▏         | 3/200 [00:35<35:23, 10.78s/it]  2%|▏         | 4/200 [00:43<32:05,  9.83s/it]  2%|▎         | 5/200 [00:52<29:59,  9.23s/it]  3%|▎         | 6/200 [01:00<28:33,  8.83s/it]  4%|▎         | 7/200 [01:08<27:30,  8.55s/it]  4%|▍         | 8/200 [01:16<26:42,  8.35s/it]  4%|▍         | 9/200 [01:23<26:06,  8.20s/it]  5%|▌         | 10/200 [01:31<25:34,  8.08s/it]  6%|▌         | 11/200 [01:39<25:06,  7.97s/it]  6%|▌         | 12/200 [01:47<24:44,  7.90s/it]  6%|▋         | 13/200 [01:54<24:32,  7.88s/it]  7%|▋         | 14/200 [02:02<24:27,  7.89s/it]  8%|▊         | 15/200 [02:10<24:21,  7.90s/it]  8%|▊         | 16/200 [02:18<24:11,  7.89s/it]  8%|▊         | 17/200 [02:26<23:49,  7.81s/it]  9%|▉         | 18/200 [02:33<23:32,  7.76s/it] 10%|▉         | 19/200 [02:41<23:15,  7.71s/it] 10%|█         | 20/200 [02:49<22:58,  7.66s/it] 10%|█         | 21/200 [02:56<22:45,  7.63s/it] 11%|█         | 22/200 [03:04<22:35,  7.61s/it] 12%|█▏        | 23/200 [03:11<22:25,  7.60s/it] 12%|█▏        | 24/200 [03:19<22:16,  7.59s/it] 12%|█▎        | 25/200 [03:26<22:07,  7.59s/it] 13%|█▎        | 26/200 [03:34<21:58,  7.58s/it] 14%|█▎        | 27/200 [03:42<21:51,  7.58s/it] 14%|█▍        | 28/200 [03:49<21:45,  7.59s/it] 14%|█▍        | 29/200 [03:57<21:38,  7.59s/it] 15%|█▌        | 30/200 [04:04<21:31,  7.60s/it] 16%|█▌        | 31/200 [04:12<21:26,  7.61s/it] 16%|█▌        | 32/200 [04:20<21:20,  7.62s/it] 16%|█▋        | 33/200 [04:27<21:12,  7.62s/it] 17%|█▋        | 34/200 [04:35<21:04,  7.62s/it] 18%|█▊        | 35/200 [04:43<20:59,  7.63s/it] 18%|█▊        | 36/200 [04:50<20:53,  7.64s/it] 18%|█▊        | 37/200 [04:58<20:45,  7.64s/it] 19%|█▉        | 38/200 [05:06<20:35,  7.62s/it] 20%|█▉        | 39/200 [05:13<20:27,  7.62s/it] 20%|██        | 40/200 [05:21<20:19,  7.62s/it] 20%|██        | 41/200 [05:28<20:11,  7.62s/it] 21%|██        | 42/200 [05:36<20:04,  7.62s/it] 22%|██▏       | 43/200 [05:44<19:59,  7.64s/it] 22%|██▏       | 44/200 [05:51<19:52,  7.65s/it] 22%|██▎       | 45/200 [05:59<19:47,  7.66s/it] 23%|██▎       | 46/200 [06:07<19:41,  7.67s/it] 24%|██▎       | 47/200 [06:14<19:36,  7.69s/it] 24%|██▍       | 48/200 [06:22<19:33,  7.72s/it] 24%|██▍       | 49/200 [06:30<19:26,  7.73s/it] 25%|██▌       | 50/200 [06:38<19:21,  7.74s/it] 26%|██▌       | 51/200 [06:46<19:16,  7.76s/it] 26%|██▌       | 52/200 [06:53<19:10,  7.78s/it] 26%|██▋       | 53/200 [07:01<19:03,  7.78s/it] 27%|██▋       | 54/200 [07:09<18:54,  7.77s/it] 28%|██▊       | 55/200 [07:17<18:46,  7.77s/it] 28%|██▊       | 56/200 [07:24<18:34,  7.74s/it] 28%|██▊       | 57/200 [07:32<18:25,  7.73s/it] 29%|██▉       | 58/200 [07:40<18:17,  7.73s/it] 30%|██▉       | 59/200 [07:48<18:11,  7.74s/it] 30%|███       | 60/200 [07:55<18:05,  7.76s/it] 30%|███       | 61/200 [08:03<18:03,  7.79s/it] 31%|███       | 62/200 [08:11<17:55,  7.79s/it] 32%|███▏      | 63/200 [08:19<17:47,  7.79s/it] 32%|███▏      | 64/200 [08:27<17:40,  7.80s/it] 32%|███▎      | 65/200 [08:34<17:33,  7.80s/it] 33%|███▎      | 66/200 [08:42<17:26,  7.81s/it] 34%|███▎      | 67/200 [08:50<17:19,  7.82s/it] 34%|███▍      | 68/200 [08:58<17:12,  7.82s/it] 34%|███▍      | 69/200 [09:06<17:04,  7.82s/it] 35%|███▌      | 70/200 [09:14<16:58,  7.84s/it] 36%|███▌      | 71/200 [09:21<16:50,  7.83s/it] 36%|███▌      | 72/200 [09:29<16:43,  7.84s/it] 36%|███▋      | 73/200 [09:37<16:42,  7.89s/it] 37%|███▋      | 74/200 [09:45<16:37,  7.92s/it] 38%|███▊      | 75/200 [09:53<16:26,  7.89s/it] 38%|███▊      | 76/200 [10:01<16:17,  7.88s/it] 38%|███▊      | 77/200 [10:09<16:10,  7.89s/it] 39%|███▉      | 78/200 [10:17<16:04,  7.90s/it] 40%|███▉      | 79/200 [10:25<15:56,  7.91s/it] 40%|████      | 80/200 [10:33<15:46,  7.89s/it] 40%|████      | 81/200 [10:41<15:40,  7.90s/it] 41%|████      | 82/200 [10:48<15:32,  7.90s/it] 42%|████▏     | 83/200 [10:56<15:28,  7.94s/it] 42%|████▏     | 84/200 [11:04<15:22,  7.96s/it] 42%|████▎     | 85/200 [11:12<15:11,  7.92s/it] 43%|████▎     | 86/200 [11:20<15:01,  7.91s/it] 44%|████▎     | 87/200 [11:28<14:54,  7.91s/it] 44%|████▍     | 88/200 [11:36<14:47,  7.92s/it] 44%|████▍     | 89/200 [11:44<14:38,  7.92s/it] 45%|████▌     | 90/200 [11:52<14:33,  7.94s/it] 46%|████▌     | 91/200 [12:00<14:27,  7.96s/it] 46%|████▌     | 92/200 [12:08<14:19,  7.96s/it] 46%|████▋     | 93/200 [12:16<14:12,  7.97s/it] 47%|████▋     | 94/200 [12:24<14:04,  7.97s/it] 48%|████▊     | 95/200 [12:32<14:06,  8.07s/it] 48%|████▊     | 96/200 [12:40<14:05,  8.13s/it] 48%|████▊     | 97/200 [12:49<14:00,  8.16s/it] 49%|████▉     | 98/200 [12:57<13:55,  8.19s/it] 50%|████▉     | 99/200 [13:05<13:48,  8.20s/it] 50%|█████     | 100/200 [13:13<13:35,  8.16s/it] 50%|█████     | 101/200 [13:21<13:21,  8.10s/it] 51%|█████     | 102/200 [13:29<13:10,  8.07s/it] 52%|█████▏    | 103/200 [13:37<13:07,  8.12s/it] 52%|█████▏    | 104/200 [13:46<13:02,  8.15s/it] 52%|█████▎    | 105/200 [13:54<12:56,  8.17s/it] 53%|█████▎    | 106/200 [14:02<12:46,  8.16s/it] 54%|█████▎    | 107/200 [14:10<12:39,  8.17s/it] 54%|█████▍    | 108/200 [14:18<12:32,  8.18s/it] 55%|█████▍    | 109/200 [14:27<12:24,  8.18s/it] 55%|█████▌    | 110/200 [14:35<12:13,  8.15s/it] 56%|█████▌    | 111/200 [14:43<12:03,  8.13s/it] 56%|█████▌    | 112/200 [14:51<11:53,  8.11s/it] 56%|█████▋    | 113/200 [14:59<11:44,  8.10s/it] 57%|█████▋    | 114/200 [15:07<11:37,  8.11s/it] 57%|█████▊    | 115/200 [15:15<11:28,  8.10s/it] 58%|█████▊    | 116/200 [15:23<11:19,  8.09s/it] 58%|█████▊    | 117/200 [15:31<11:09,  8.07s/it] 59%|█████▉    | 118/200 [15:39<11:02,  8.08s/it] 60%|█████▉    | 119/200 [15:47<10:54,  8.08s/it] 60%|██████    | 120/200 [15:55<10:48,  8.10s/it] 60%|██████    | 121/200 [16:04<10:40,  8.11s/it] 61%|██████    | 122/200 [16:12<10:32,  8.11s/it] 62%|██████▏   | 123/200 [16:20<10:24,  8.11s/it] 62%|██████▏   | 124/200 [16:28<10:16,  8.11s/it] 62%|██████▎   | 125/200 [16:36<10:07,  8.10s/it] 63%|██████▎   | 126/200 [16:44<10:00,  8.12s/it] 64%|██████▎   | 127/200 [16:52<09:52,  8.12s/it] 64%|██████▍   | 128/200 [17:00<09:43,  8.10s/it] 64%|██████▍   | 129/200 [17:08<09:35,  8.11s/it] 65%|██████▌   | 130/200 [17:17<09:27,  8.11s/it] 66%|██████▌   | 131/200 [17:25<09:19,  8.11s/it] 66%|██████▌   | 132/200 [17:33<09:16,  8.18s/it] 66%|██████▋   | 133/200 [17:41<09:08,  8.19s/it] 67%|██████▋   | 134/200 [17:50<09:01,  8.20s/it] 68%|██████▊   | 135/200 [17:58<08:52,  8.19s/it] 68%|██████▊   | 136/200 [18:06<08:42,  8.17s/it] 68%|██████▊   | 137/200 [18:14<08:32,  8.14s/it] 69%|██████▉   | 138/200 [18:22<08:24,  8.14s/it] 70%|██████▉   | 139/200 [18:30<08:15,  8.13s/it] 70%|███████   | 140/200 [18:38<08:06,  8.10s/it] 70%|███████   | 141/200 [18:46<07:57,  8.09s/it] 71%|███████   | 142/200 [18:54<07:49,  8.09s/it] 72%|███████▏  | 143/200 [19:02<07:41,  8.10s/it] 72%|███████▏  | 144/200 [19:11<07:36,  8.14s/it] 72%|███████▎  | 145/200 [19:19<07:28,  8.16s/it] 73%|███████▎  | 146/200 [19:27<07:20,  8.16s/it] 74%|███████▎  | 147/200 [19:35<07:13,  8.17s/it] 74%|███████▍  | 148/200 [19:43<07:01,  8.10s/it] 74%|███████▍  | 149/200 [19:51<06:51,  8.06s/it] 75%|███████▌  | 150/200 [19:59<06:41,  8.03s/it] 76%|███████▌  | 151/200 [20:07<06:29,  7.95s/it] 76%|███████▌  | 152/200 [20:15<06:20,  7.92s/it] 76%|███████▋  | 153/200 [20:23<06:10,  7.89s/it] 77%|███████▋  | 154/200 [20:30<06:01,  7.86s/it] 78%|███████▊  | 155/200 [20:38<05:52,  7.84s/it] 78%|███████▊  | 156/200 [20:46<05:44,  7.82s/it] 78%|███████▊  | 157/200 [20:54<05:35,  7.81s/it] 79%|███████▉  | 158/200 [21:02<05:28,  7.82s/it] 80%|███████▉  | 159/200 [21:09<05:20,  7.81s/it] 80%|████████  | 160/200 [21:17<05:12,  7.81s/it] 80%|████████  | 161/200 [21:25<05:04,  7.81s/it] 81%|████████  | 162/200 [21:33<04:54,  7.76s/it] 82%|████████▏ | 163/200 [21:40<04:46,  7.75s/it] 82%|████████▏ | 164/200 [21:48<04:38,  7.73s/it] 82%|████████▎ | 165/200 [21:56<04:30,  7.72s/it] 83%|████████▎ | 166/200 [22:03<04:21,  7.68s/it] 84%|████████▎ | 167/200 [22:11<04:13,  7.68s/it] 84%|████████▍ | 168/200 [22:19<04:05,  7.68s/it] 84%|████████▍ | 169/200 [22:26<03:58,  7.71s/it] 85%|████████▌ | 170/200 [22:34<03:51,  7.72s/it] 86%|████████▌ | 171/200 [22:42<03:43,  7.69s/it] 86%|████████▌ | 172/200 [22:49<03:34,  7.66s/it] 86%|████████▋ | 173/200 [22:57<03:26,  7.66s/it] 87%|████████▋ | 174/200 [23:05<03:18,  7.62s/it] 88%|████████▊ | 175/200 [23:12<03:09,  7.59s/it] 88%|████████▊ | 176/200 [23:20<03:01,  7.57s/it] 88%|████████▊ | 177/200 [23:27<02:53,  7.53s/it] 89%|████████▉ | 178/200 [23:35<02:45,  7.54s/it] 90%|████████▉ | 179/200 [23:42<02:38,  7.54s/it] 90%|█████████ | 180/200 [23:50<02:30,  7.52s/it] 90%|█████████ | 181/200 [23:57<02:22,  7.50s/it] 91%|█████████ | 182/200 [24:05<02:14,  7.50s/it] 92%|█████████▏| 183/200 [24:12<02:07,  7.50s/it] 92%|█████████▏| 184/200 [24:20<02:00,  7.55s/it] 92%|█████████▎| 185/200 [24:27<01:53,  7.59s/it] 93%|█████████▎| 186/200 [24:35<01:46,  7.61s/it] 94%|█████████▎| 187/200 [24:43<01:38,  7.61s/it] 94%|█████████▍| 188/200 [24:50<01:31,  7.63s/it] 94%|█████████▍| 189/200 [24:58<01:23,  7.62s/it] 95%|█████████▌| 190/200 [25:05<01:15,  7.56s/it] 96%|█████████▌| 191/200 [25:13<01:08,  7.58s/it] 96%|█████████▌| 192/200 [25:21<01:00,  7.61s/it] 96%|█████████▋| 193/200 [25:28<00:53,  7.63s/it] 97%|█████████▋| 194/200 [25:36<00:45,  7.62s/it] 98%|█████████▊| 195/200 [25:43<00:37,  7.59s/it] 98%|█████████▊| 196/200 [25:51<00:30,  7.60s/it] 98%|█████████▊| 197/200 [25:59<00:22,  7.58s/it] 99%|█████████▉| 198/200 [26:06<00:15,  7.52s/it]100%|█████████▉| 199/200 [26:13<00:07,  7.49s/it]100%|██████████| 200/200 [26:21<00:00,  7.48s/it]100%|██████████| 200/200 [26:21<00:00,  7.91s/it]
50
50
number of selected users 50
Global Trainning Accurancy: 0.10063176238759938
Global Trainning Loss: 2.3031054401397704
Global test accurancy: 0.10112996285527823
Global test_loss: 2.3030942773818968
Global Precision: 0.01030821324628988
Global Recall: 0.10112996285527823
Global f1score: 0.018697677431556804
50
50
number of selected users 50
Global Trainning Accurancy: 0.10055356075279367
Global Trainning Loss: 2.302954440116882
Global test accurancy: 0.10008361060589036
Global test_loss: 2.3029510879516604
Global Precision: 0.018693262322145916
Global Recall: 0.10008361060589036
Global f1score: 0.021301511270193058
50
50
number of selected users 50
Global Trainning Accurancy: 0.10256121641895267
Global Trainning Loss: 2.3028358840942382
Global test accurancy: 0.1014124103557718
Global test_loss: 2.3028396368026733
Global Precision: 0.020639798307321905
Global Recall: 0.1014124103557718
Global f1score: 0.032860293532884395
50
50
number of selected users 50
Global Trainning Accurancy: 0.10202676706844137
Global Trainning Loss: 2.3027430534362794
Global test accurancy: 0.10053570767212026
Global test_loss: 2.30275354385376
Global Precision: 0.020144898225655784
Global Recall: 0.10053570767212026
Global f1score: 0.0324563938415193
50
50
number of selected users 50
Global Trainning Accurancy: 0.10083496943910719
Global Trainning Loss: 2.302671184539795
Global test accurancy: 0.09960984342213161
Global test_loss: 2.302687473297119
Global Precision: 0.019299028122791127
Global Recall: 0.09960984342213161
Global f1score: 0.024315323867605335
50
50
number of selected users 50
Global Trainning Accurancy: 0.10022873436103595
Global Trainning Loss: 2.302615456581116
Global test accurancy: 0.09942108161995704
Global test_loss: 2.3026375198364257
Global Precision: 0.01636736832724786
Global Recall: 0.09942108161995704
Global f1score: 0.01949514103252968
50
50
number of selected users 50
Global Trainning Accurancy: 0.10043887894581426
Global Trainning Loss: 2.3025723695755005
Global test accurancy: 0.0996826603510243
Global test_loss: 2.3026001119613646
Global Precision: 0.01191397886204763
Global Recall: 0.0996826603510243
Global f1score: 0.0184260382149212
50
50
number of selected users 50
Global Trainning Accurancy: 0.10054207718720182
Global Trainning Loss: 2.302539224624634
Global test accurancy: 0.0996773373996614
Global test_loss: 2.3025723695755005
Global Precision: 0.011919688457025063
Global Recall: 0.0996773373996614
Global f1score: 0.018435108237865445
50
50
number of selected users 50
Global Trainning Accurancy: 0.10136694944419508
Global Trainning Loss: 2.3025137758255005
Global test accurancy: 0.10029341747100294
Global test_loss: 2.302551999092102
Global Precision: 0.021255384037404387
Global Recall: 0.10029341747100294
Global f1score: 0.023637227375391066
50
50
number of selected users 50
Global Trainning Accurancy: 0.10461893554631976
Global Trainning Loss: 2.302494459152222
Global test accurancy: 0.10343380585705947
Global test_loss: 2.302537188529968
Global Precision: 0.021954869315404905
Global Recall: 0.10343380585705947
Global f1score: 0.03354687728754776
50
50
number of selected users 50
Global Trainning Accurancy: 0.10565341331395446
Global Trainning Loss: 2.302479958534241
Global test accurancy: 0.10551648068846853
Global test_loss: 2.3025269317626953
Global Precision: 0.021418244627909283
Global Recall: 0.10551648068846853
Global f1score: 0.035505315256423024
50
50
number of selected users 50
Global Trainning Accurancy: 0.10429255065943963
Global Trainning Loss: 2.3024689292907716
Global test accurancy: 0.1016634402969762
Global test_loss: 2.302520041465759
Global Precision: 0.02053502374578931
Global Recall: 0.1016634402969762
Global f1score: 0.031363560743758304
50
50
number of selected users 50
Global Trainning Accurancy: 0.10206295172174387
Global Trainning Loss: 2.30246045589447
Global test accurancy: 0.10116605592327238
Global test_loss: 2.30251576423645
Global Precision: 0.02131575981345903
Global Recall: 0.10116605592327238
Global f1score: 0.024832810474690752
50
50
number of selected users 50
Global Trainning Accurancy: 0.10159384890731013
Global Trainning Loss: 2.302453761100769
Global test accurancy: 0.10092273213115562
Global test_loss: 2.302513451576233
Global Precision: 0.024544380121718095
Global Recall: 0.10092273213115562
Global f1score: 0.020261930228217366
50
50
number of selected users 50
Global Trainning Accurancy: 0.10103248799207
Global Trainning Loss: 2.3024483203887938
Global test accurancy: 0.10084215200967471
Global test_loss: 2.302512493133545
Global Precision: 0.012170175665923072
Global Recall: 0.10084215200967471
Global f1score: 0.018800784390576388
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024435520172117
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025123739242552
Global Precision: 0.010284697381525045
Global Recall: 0.1007637206371257
Global f1score: 0.01864501374110721
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024391984939574
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025127410888673
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302435121536255
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025130939483645
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302430996894836
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025135612487793
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024268913269044
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025137329101564
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302422480583191
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302513723373413
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024177312850953
Global test accurancy: 0.1007637206371257
Global test_loss: 2.30251353263855
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302412495613098
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302512803077698
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024068546295164
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025117254257204
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024006938934325
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025103569030763
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023938512802125
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025083112716676
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023863410949708
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025057983398436
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302378087043762
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302502555847168
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302369318008423
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024989414215087
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023598480224607
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024945974349977
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023499059677124
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302490220069885
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302339415550232
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024857664108276
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023282861709595
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024811506271363
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023170948028566
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302476644515991
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023057651519774
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302471609115601
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302294135093689
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302465672492981
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3022817611694335
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024591064453124
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3022691345214845
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024520683288574
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.10097482083411845
Global Trainning Loss: 2.3022563600540162
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302444567680359
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.10100718329366537
Global Trainning Loss: 2.302243404388428
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024370431900025
Global Precision: 0.010285651479558395
Global Recall: 0.1007637206371257
Global f1score: 0.018646545846770737
50
50
number of selected users 50
Global Trainning Accurancy: 0.10103499970534827
Global Trainning Loss: 2.302229881286621
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024296283721926
Global Precision: 0.010287842479603534
Global Recall: 0.1007637206371257
Global f1score: 0.0186501458433992
50
50
number of selected users 50
Global Trainning Accurancy: 0.101006222726931
Global Trainning Loss: 2.302216329574585
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024219512939452
Global Precision: 0.010287842479603534
Global Recall: 0.1007637206371257
Global f1score: 0.0186501458433992
50
50
number of selected users 50
Global Trainning Accurancy: 0.10106888821678095
Global Trainning Loss: 2.3022026109695433
Global test accurancy: 0.10069151847106071
Global test_loss: 2.302413945198059
Global Precision: 0.010280435856310702
Global Recall: 0.10069151847106071
Global f1score: 0.018636734340274866
50
50
number of selected users 50
Global Trainning Accurancy: 0.10113150142478997
Global Trainning Loss: 2.302188682556152
Global test accurancy: 0.1006928472569933
Global test_loss: 2.3024058723449707
Global Precision: 0.01236061389044935
Global Recall: 0.1006928472569933
Global f1score: 0.01880813781633794
50
50
number of selected users 50
Global Trainning Accurancy: 0.1011042534683867
Global Trainning Loss: 2.3021745252609254
Global test accurancy: 0.1006193178452286
Global test_loss: 2.3023977184295656
Global Precision: 0.012355737032220511
Global Recall: 0.1006193178452286
Global f1score: 0.01879878737497562
50
50
number of selected users 50
Global Trainning Accurancy: 0.10106641773392175
Global Trainning Loss: 2.302160186767578
Global test accurancy: 0.10052595543552435
Global test_loss: 2.302389078140259
Global Precision: 0.013440979796336476
Global Recall: 0.10052595543552435
Global f1score: 0.018930028122873074
50
50
number of selected users 50
Global Trainning Accurancy: 0.10102921421489028
Global Trainning Loss: 2.3021455144882204
Global test accurancy: 0.10054128302352636
Global test_loss: 2.302380094528198
Global Precision: 0.014890575029011798
Global Recall: 0.10054128302352636
Global f1score: 0.0192587156669523
50
50
number of selected users 50
Global Trainning Accurancy: 0.10095014370761322
Global Trainning Loss: 2.3021308040618895
Global test accurancy: 0.10045760101515816
Global test_loss: 2.3023708343505858
Global Precision: 0.014086474509381912
Global Recall: 0.10045760101515816
Global f1score: 0.019248003985357094
50
50
number of selected users 50
Global Trainning Accurancy: 0.10088664964529112
Global Trainning Loss: 2.3021155738830568
Global test accurancy: 0.10039407542521213
Global test_loss: 2.3023609209060667
Global Precision: 0.016287076174298656
Global Recall: 0.10039407542521213
Global f1score: 0.01960123105571109
50
50
number of selected users 50
Global Trainning Accurancy: 0.10078125158197493
Global Trainning Loss: 2.3021001482009886
Global test accurancy: 0.1004634837946205
Global test_loss: 2.3023509550094605
Global Precision: 0.017768698134785973
Global Recall: 0.1004634837946205
Global f1score: 0.01990230277724756
50
50
number of selected users 50
Global Trainning Accurancy: 0.10074968550507996
Global Trainning Loss: 2.3020846128463743
Global test accurancy: 0.10046255522809201
Global test_loss: 2.3023406410217286
Global Precision: 0.019788870689238198
Global Recall: 0.10046255522809201
Global f1score: 0.020336905443265118
50
50
number of selected users 50
Global Trainning Accurancy: 0.10071862643312465
Global Trainning Loss: 2.3020689201354982
Global test accurancy: 0.10054309017451497
Global test_loss: 2.3023304271698
Global Precision: 0.020456866093656243
Global Recall: 0.10054309017451497
Global f1score: 0.020748849273053937
50
50
number of selected users 50
Global Trainning Accurancy: 0.10101252073248057
Global Trainning Loss: 2.3020526123046876
Global test accurancy: 0.10059338163122007
Global test_loss: 2.3023199367523195
Global Precision: 0.020275412239876323
Global Recall: 0.10059338163122007
Global f1score: 0.02120341045728563
50
50
number of selected users 50
Global Trainning Accurancy: 0.10132628839281033
Global Trainning Loss: 2.302036123275757
Global test accurancy: 0.10079585222067701
Global test_loss: 2.3023094272613527
Global Precision: 0.021708337294648657
Global Recall: 0.10079585222067701
Global f1score: 0.021827624605605406
50
50
number of selected users 50
Global Trainning Accurancy: 0.10177338658629845
Global Trainning Loss: 2.302019248008728
Global test accurancy: 0.10087595477465935
Global test_loss: 2.3022982454299927
Global Precision: 0.021567832779457573
Global Recall: 0.10087595477465935
Global f1score: 0.02242164808911171
50
50
number of selected users 50
Global Trainning Accurancy: 0.10201568423757679
Global Trainning Loss: 2.3020018768310546
Global test accurancy: 0.10104997220808123
Global test_loss: 2.302286810874939
Global Precision: 0.02251005522139009
Global Recall: 0.10104997220808123
Global f1score: 0.023287166014046953
50
50
number of selected users 50
Global Trainning Accurancy: 0.10211217074284423
Global Trainning Loss: 2.301984262466431
Global test accurancy: 0.10141949398657855
Global test_loss: 2.302274718284607
Global Precision: 0.022855187610591614
Global Recall: 0.10141949398657855
Global f1score: 0.024063288389258152
50
50
number of selected users 50
Global Trainning Accurancy: 0.1023028922848713
Global Trainning Loss: 2.301966118812561
Global test accurancy: 0.10131836476504012
Global test_loss: 2.302262601852417
Global Precision: 0.022540170235957006
Global Recall: 0.10131836476504012
Global f1score: 0.024470002326241767
50
50
number of selected users 50
Global Trainning Accurancy: 0.10255096933472714
Global Trainning Loss: 2.3019474363327026
Global test accurancy: 0.10174019095253321
Global test_loss: 2.3022503566741945
Global Precision: 0.02281612457118799
Global Recall: 0.10174019095253321
Global f1score: 0.025226632684694147
50
50
number of selected users 50
Global Trainning Accurancy: 0.10254807200467062
Global Trainning Loss: 2.301928129196167
Global test accurancy: 0.10229821671911347
Global test_loss: 2.3022376680374146
Global Precision: 0.02371099778395558
Global Recall: 0.10229821671911347
Global f1score: 0.02654950739996406
50
50
number of selected users 50
Global Trainning Accurancy: 0.10276985131964153
Global Trainning Loss: 2.3019082832336424
Global test accurancy: 0.10251113816166069
Global test_loss: 2.302224564552307
Global Precision: 0.023516507884070916
Global Recall: 0.10251113816166069
Global f1score: 0.027263303087842222
50
50
number of selected users 50
Global Trainning Accurancy: 0.10292002906716098
Global Trainning Loss: 2.3018879079818726
Global test accurancy: 0.10284336414230387
Global test_loss: 2.302211031913757
Global Precision: 0.02331919650136424
Global Recall: 0.10284336414230387
Global f1score: 0.027797966800839017
50
50
number of selected users 50
Global Trainning Accurancy: 0.10321077177056195
Global Trainning Loss: 2.301867141723633
Global test accurancy: 0.1029708240045028
Global test_loss: 2.3021973085403444
Global Precision: 0.022753650654479436
Global Recall: 0.1029708240045028
Global f1score: 0.02797289746364635
50
50
number of selected users 50
Global Trainning Accurancy: 0.10355236553571466
Global Trainning Loss: 2.301845750808716
Global test accurancy: 0.10325495585756736
Global test_loss: 2.302182846069336
Global Precision: 0.022940816932791333
Global Recall: 0.10325495585756736
Global f1score: 0.028743114296811017
50
50
number of selected users 50
Global Trainning Accurancy: 0.10382070190324079
Global Trainning Loss: 2.3018239641189577
Global test accurancy: 0.10342910697185544
Global test_loss: 2.3021683502197265
Global Precision: 0.023151020483945853
Global Recall: 0.10342910697185544
Global f1score: 0.02953200784426053
50
50
number of selected users 50
Global Trainning Accurancy: 0.10393015303694528
Global Trainning Loss: 2.3018013763427736
Global test accurancy: 0.10426933858032583
Global test_loss: 2.3021532583236692
Global Precision: 0.023715433635335233
Global Recall: 0.10426933858032583
Global f1score: 0.0308007264021814
50
50
number of selected users 50
Global Trainning Accurancy: 0.10400928646626503
Global Trainning Loss: 2.301778111457825
Global test accurancy: 0.10410554039144579
Global test_loss: 2.3021378087997437
Global Precision: 0.023586951928486297
Global Recall: 0.10410554039144579
Global f1score: 0.031228304660038734
50
50
number of selected users 50
Global Trainning Accurancy: 0.10419222033527399
Global Trainning Loss: 2.301754126548767
Global test accurancy: 0.10474738379010014
Global test_loss: 2.302121844291687
Global Precision: 0.02995573246690859
Global Recall: 0.10474738379010014
Global f1score: 0.03220719649811252
50
50
number of selected users 50
Global Trainning Accurancy: 0.10443719444512921
Global Trainning Loss: 2.3017295169830323
Global test accurancy: 0.10497978698036507
Global test_loss: 2.3021050119400024
Global Precision: 0.02965981237222086
Global Recall: 0.10497978698036507
Global f1score: 0.03246827675871137
50
50
number of selected users 50
Global Trainning Accurancy: 0.10446281246329467
Global Trainning Loss: 2.3017043828964234
Global test accurancy: 0.10500324359887662
Global test_loss: 2.302087788581848
Global Precision: 0.02939431138560554
Global Recall: 0.10500324359887662
Global f1score: 0.03267726163695378
50
50
number of selected users 50
Global Trainning Accurancy: 0.10484447112603569
Global Trainning Loss: 2.301678948402405
Global test accurancy: 0.10514910332866066
Global test_loss: 2.3020705080032347
Global Precision: 0.029382309199698354
Global Recall: 0.10514910332866066
Global f1score: 0.033090799424633985
50
50
number of selected users 50
Global Trainning Accurancy: 0.10506910882672821
Global Trainning Loss: 2.3016528034210206
Global test accurancy: 0.10491319075712306
Global test_loss: 2.302052631378174
Global Precision: 0.0294151140096673
Global Recall: 0.10491319075712306
Global f1score: 0.03340272455261676
50
50
number of selected users 50
Global Trainning Accurancy: 0.10543639584891917
Global Trainning Loss: 2.3016261291503906
Global test accurancy: 0.10536644708697737
Global test_loss: 2.3020343542099
Global Precision: 0.030613994069330623
Global Recall: 0.10536644708697737
Global f1score: 0.03408211322194935
50
50
number of selected users 50
Global Trainning Accurancy: 0.10547906595177056
Global Trainning Loss: 2.301598825454712
Global test accurancy: 0.10523168238435635
Global test_loss: 2.3020155143737795
Global Precision: 0.029161822868103315
Global Recall: 0.10523168238435635
Global f1score: 0.03396669148963382
50
50
number of selected users 50
Global Trainning Accurancy: 0.10546440408483518
Global Trainning Loss: 2.301570954322815
Global test accurancy: 0.10548987275451711
Global test_loss: 2.3019963264465333
Global Precision: 0.029165059706764637
Global Recall: 0.10548987275451711
Global f1score: 0.03427597652617636
50
50
number of selected users 50
Global Trainning Accurancy: 0.10543652277682652
Global Trainning Loss: 2.3015425109863283
Global test accurancy: 0.10590684428055543
Global test_loss: 2.3019769096374514
Global Precision: 0.029208746161680368
Global Recall: 0.10590684428055543
Global f1score: 0.03459314223230763
50
50
number of selected users 50
Global Trainning Accurancy: 0.1052375893022474
Global Trainning Loss: 2.3015134286880494
Global test accurancy: 0.10579821208795383
Global test_loss: 2.301957015991211
Global Precision: 0.03129129258600239
Global Recall: 0.10579821208795383
Global f1score: 0.03491138548807433
50
50
number of selected users 50
Global Trainning Accurancy: 0.10551527970097309
Global Trainning Loss: 2.3014837312698364
Global test accurancy: 0.10559327649778885
Global test_loss: 2.301936917304993
Global Precision: 0.032013687548449216
Global Recall: 0.10559327649778885
Global f1score: 0.03527641040657713
50
50
number of selected users 50
Global Trainning Accurancy: 0.10559253140049733
Global Trainning Loss: 2.301453194618225
Global test accurancy: 0.10502262894858388
Global test_loss: 2.301916379928589
Global Precision: 0.031923771389615004
Global Recall: 0.10502262894858388
Global f1score: 0.0352060960879356
50
50
number of selected users 50
Global Trainning Accurancy: 0.10587189321417176
Global Trainning Loss: 2.3014222097396853
Global test accurancy: 0.10521508598940228
Global test_loss: 2.3018955993652344
Global Precision: 0.03532485663398214
Global Recall: 0.10521508598940228
Global f1score: 0.03574910320861946
50
50
number of selected users 50
Global Trainning Accurancy: 0.10576665216168396
Global Trainning Loss: 2.301390347480774
Global test accurancy: 0.10569775380093971
Global test_loss: 2.3018744087219236
Global Precision: 0.032525487198201836
Global Recall: 0.10569775380093971
Global f1score: 0.035984491813003845
50
50
number of selected users 50
Global Trainning Accurancy: 0.10617794117814588
Global Trainning Loss: 2.301357684135437
Global test accurancy: 0.10556062018443421
Global test_loss: 2.301852669715881
Global Precision: 0.03241669591040127
Global Recall: 0.10556062018443421
Global f1score: 0.03600378175116609
50
50
number of selected users 50
Global Trainning Accurancy: 0.10655060232260961
Global Trainning Loss: 2.3013244533538817
Global test accurancy: 0.10573271369769119
Global test_loss: 2.3018309259414673
Global Precision: 0.033297110658273295
Global Recall: 0.10573271369769119
Global f1score: 0.03637517363140452
50
50
number of selected users 50
Global Trainning Accurancy: 0.10639176042193377
Global Trainning Loss: 2.301290364265442
Global test accurancy: 0.10534081283904997
Global test_loss: 2.3018087768554687
Global Precision: 0.03456424622196391
Global Recall: 0.10534081283904997
Global f1score: 0.03674472559441087
50
50
number of selected users 50
Global Trainning Accurancy: 0.10659665023650733
Global Trainning Loss: 2.301255168914795
Global test accurancy: 0.10553181161386885
Global test_loss: 2.3017860651016235
Global Precision: 0.03485601058731976
Global Recall: 0.10553181161386885
Global f1score: 0.03727388763861953
50
50
number of selected users 50
Global Trainning Accurancy: 0.10670524245008327
Global Trainning Loss: 2.301219091415405
Global test accurancy: 0.10544165336561397
Global test_loss: 2.3017626714706423
Global Precision: 0.035127631444143555
Global Recall: 0.10544165336561397
Global f1score: 0.03748483845142045
50
50
number of selected users 50
Global Trainning Accurancy: 0.10716773628773792
Global Trainning Loss: 2.3011819458007814
Global test accurancy: 0.1054459881141587
Global test_loss: 2.301738810539246
Global Precision: 0.04033356702912732
Global Recall: 0.1054459881141587
Global f1score: 0.03831660720986088
50
50
number of selected users 50
Global Trainning Accurancy: 0.1074016814775821
Global Trainning Loss: 2.3011440229415894
Global test accurancy: 0.10567130973088529
Global test_loss: 2.3017141962051393
Global Precision: 0.04062714695945374
Global Recall: 0.10567130973088529
Global f1score: 0.038870652908023814
50
50
number of selected users 50
Global Trainning Accurancy: 0.10767658954826871
Global Trainning Loss: 2.3011050033569336
Global test accurancy: 0.10617432120952958
Global test_loss: 2.301688995361328
Global Precision: 0.04271359921436946
Global Recall: 0.10617432120952958
Global f1score: 0.03966694681616353
50
50
number of selected users 50
Global Trainning Accurancy: 0.10799124635228446
Global Trainning Loss: 2.3010651206970216
Global test accurancy: 0.10615465702762503
Global test_loss: 2.3016628408432007
Global Precision: 0.04456660218797752
Global Recall: 0.10615465702762503
Global f1score: 0.04001782746202544
50
50
number of selected users 50
Global Trainning Accurancy: 0.10819125032124324
Global Trainning Loss: 2.3010242319107057
Global test accurancy: 0.10624044650321791
Global test_loss: 2.3016362476348875
Global Precision: 0.04495375013611033
Global Recall: 0.10624044650321791
Global f1score: 0.04077034166249385
50
50
number of selected users 50
Global Trainning Accurancy: 0.10856401513371064
Global Trainning Loss: 2.3009823608398436
Global test accurancy: 0.10606103407686175
Global test_loss: 2.301608943939209
Global Precision: 0.04965481739691771
Global Recall: 0.10606103407686175
Global f1score: 0.04160277383490209
50
50
number of selected users 50
Global Trainning Accurancy: 0.10868977959085885
Global Trainning Loss: 2.3009394884109495
Global test accurancy: 0.10603874230607618
Global test_loss: 2.3015813541412355
Global Precision: 0.04939182225784109
Global Recall: 0.10603874230607618
Global f1score: 0.04219123719339444
50
50
number of selected users 50
Global Trainning Accurancy: 0.10905990123478054
Global Trainning Loss: 2.30089551448822
Global test accurancy: 0.1070274466242205
Global test_loss: 2.3015528678894044
Global Precision: 0.05207871060042643
Global Recall: 0.1070274466242205
Global f1score: 0.04390006519393907
50
50
number of selected users 50
Global Trainning Accurancy: 0.10967456017148076
Global Trainning Loss: 2.3008507347106932
Global test accurancy: 0.10740830296904924
Global test_loss: 2.301523504257202
Global Precision: 0.053035369318630295
Global Recall: 0.10740830296904924
Global f1score: 0.04480768979738694
50
50
number of selected users 50
Global Trainning Accurancy: 0.11020447199519606
Global Trainning Loss: 2.3008047485351564
Global test accurancy: 0.10823142511712579
Global test_loss: 2.3014936304092406
Global Precision: 0.05210519237900302
Global Recall: 0.10823142511712579
Global f1score: 0.046288347441981854
50
50
number of selected users 50
Global Trainning Accurancy: 0.11082554640469319
Global Trainning Loss: 2.300757284164429
Global test accurancy: 0.10816093105888769
Global test_loss: 2.301462845802307
Global Precision: 0.05259673802352702
Global Recall: 0.10816093105888769
Global f1score: 0.04717850281063289
50
50
number of selected users 50
Global Trainning Accurancy: 0.11057000524280385
Global Trainning Loss: 2.3007084274291993
Global test accurancy: 0.10837345989999957
Global test_loss: 2.301431975364685
Global Precision: 0.05195687173714388
Global Recall: 0.10837345989999957
Global f1score: 0.0480732661712365
50
50
number of selected users 50
Global Trainning Accurancy: 0.1114305445120129
Global Trainning Loss: 2.3006584882736205
Global test accurancy: 0.10888445659749865
Global test_loss: 2.301400752067566
Global Precision: 0.0508085447082617
Global Recall: 0.10888445659749865
Global f1score: 0.04910973219402657
50
50
number of selected users 50
Global Trainning Accurancy: 0.11184654881304235
Global Trainning Loss: 2.300607929229736
Global test accurancy: 0.10834769653717276
Global test_loss: 2.301368947029114
Global Precision: 0.048711479993825066
Global Recall: 0.10834769653717276
Global f1score: 0.04908472463891291
50
50
number of selected users 50
Global Trainning Accurancy: 0.11266744172905262
Global Trainning Loss: 2.3005561780929566
Global test accurancy: 0.10854505361612513
Global test_loss: 2.3013364934921263
Global Precision: 0.05087656781312033
Global Recall: 0.10854505361612513
Global f1score: 0.05033675126670447
50
50
number of selected users 50
Global Trainning Accurancy: 0.1128564628019719
Global Trainning Loss: 2.3005035161972045
Global test accurancy: 0.10825542201950178
Global test_loss: 2.3013038444519043
Global Precision: 0.0503882066813348
Global Recall: 0.10825542201950178
Global f1score: 0.050900722673989277
50
50
number of selected users 50
Global Trainning Accurancy: 0.11331642975286021
Global Trainning Loss: 2.30045015335083
Global test accurancy: 0.10816473871194267
Global test_loss: 2.301270589828491
Global Precision: 0.049901626786723885
Global Recall: 0.10816473871194267
Global f1score: 0.051587312216843983
50
50
number of selected users 50
Global Trainning Accurancy: 0.1138670347334684
Global Trainning Loss: 2.3003955602645876
Global test accurancy: 0.10895024317735141
Global test_loss: 2.3012367630004884
Global Precision: 0.050480298015126324
Global Recall: 0.10895024317735141
Global f1score: 0.052973905650000465
50
50
number of selected users 50
Global Trainning Accurancy: 0.11417617087105872
Global Trainning Loss: 2.3003401517868043
Global test accurancy: 0.10897644179524597
Global test_loss: 2.3012024688720705
Global Precision: 0.05018831590635783
Global Recall: 0.10897644179524597
Global f1score: 0.05383665071003257
50
50
number of selected users 50
Global Trainning Accurancy: 0.11460743322690628
Global Trainning Loss: 2.300283546447754
Global test accurancy: 0.10956811679246113
Global test_loss: 2.301167702674866
Global Precision: 0.050491558004237456
Global Recall: 0.10956811679246113
Global f1score: 0.05493777576748247
50
50
number of selected users 50
Global Trainning Accurancy: 0.1147242676008146
Global Trainning Loss: 2.300226058959961
Global test accurancy: 0.10945525501268137
Global test_loss: 2.301132917404175
Global Precision: 0.050412737019886265
Global Recall: 0.10945525501268137
Global f1score: 0.0554211104673034
50
50
number of selected users 50
Global Trainning Accurancy: 0.11440269548971833
Global Trainning Loss: 2.3001681327819825
Global test accurancy: 0.10948317248833651
Global test_loss: 2.3010975170135497
Global Precision: 0.05007115024181359
Global Recall: 0.10948317248833651
Global f1score: 0.05584979829747508
50
50
number of selected users 50
Global Trainning Accurancy: 0.11454171013547317
Global Trainning Loss: 2.300108065605164
Global test accurancy: 0.10949931577473654
Global test_loss: 2.30106098651886
Global Precision: 0.04962272987289938
Global Recall: 0.10949931577473654
Global f1score: 0.05617865902415815
50
50
number of selected users 50
Global Trainning Accurancy: 0.11510788254556861
Global Trainning Loss: 2.3000468254089355
Global test accurancy: 0.10956448160105944
Global test_loss: 2.3010235834121704
Global Precision: 0.049274112835779225
Global Recall: 0.10956448160105944
Global f1score: 0.056550962459161754
50
50
number of selected users 50
Global Trainning Accurancy: 0.1153404073189975
Global Trainning Loss: 2.299984107017517
Global test accurancy: 0.10953980490240167
Global test_loss: 2.3009872531890867
Global Precision: 0.04868409884006531
Global Recall: 0.10953980490240167
Global f1score: 0.05683704677077005
50
50
number of selected users 50
Global Trainning Accurancy: 0.11523766233147892
Global Trainning Loss: 2.2999209690093996
Global test accurancy: 0.11035583128572958
Global test_loss: 2.3009504175186155
Global Precision: 0.05032624330688781
Global Recall: 0.11035583128572958
Global f1score: 0.05805731989863427
50
50
number of selected users 50
Global Trainning Accurancy: 0.11557118587300708
Global Trainning Loss: 2.2998571968078614
Global test accurancy: 0.11103314739169205
Global test_loss: 2.300912089347839
Global Precision: 0.050317943746828755
Global Recall: 0.11103314739169205
Global f1score: 0.059116580216806325
50
50
number of selected users 50
Global Trainning Accurancy: 0.11550061650642868
Global Trainning Loss: 2.299793200492859
Global test accurancy: 0.11057668632282679
Global test_loss: 2.3008731746673585
Global Precision: 0.05484078820398533
Global Recall: 0.11057668632282679
Global f1score: 0.05959210076567719
50
50
number of selected users 50
Global Trainning Accurancy: 0.11557448850531078
Global Trainning Loss: 2.2997274160385133
Global test accurancy: 0.11100599572903376
Global test_loss: 2.3008326292037964
Global Precision: 0.058286011283229605
Global Recall: 0.11100599572903376
Global f1score: 0.06051384340102416
50
50
number of selected users 50
Global Trainning Accurancy: 0.11572763475061171
Global Trainning Loss: 2.29966103553772
Global test accurancy: 0.11128731967015931
Global test_loss: 2.3007921171188355
Global Precision: 0.058051492603156646
Global Recall: 0.11128731967015931
Global f1score: 0.061199433913574575
50
50
number of selected users 50
Global Trainning Accurancy: 0.11576325376845463
Global Trainning Loss: 2.299595122337341
Global test accurancy: 0.1120254394311475
Global test_loss: 2.3007529830932616
Global Precision: 0.05791121647174044
Global Recall: 0.1120254394311475
Global f1score: 0.061849435204638537
50
50
number of selected users 50
Global Trainning Accurancy: 0.11607996785228629
Global Trainning Loss: 2.299528021812439
Global test accurancy: 0.11231385025258843
Global test_loss: 2.3007132053375243
Global Precision: 0.05945118299257884
Global Recall: 0.11231385025258843
Global f1score: 0.06261109746410572
50
50
number of selected users 50
Global Trainning Accurancy: 0.11636874710807332
Global Trainning Loss: 2.299458932876587
Global test accurancy: 0.11219762214071818
Global test_loss: 2.3006725692749024
Global Precision: 0.06255497297432788
Global Recall: 0.11219762214071818
Global f1score: 0.06326046245932444
50
50
number of selected users 50
Global Trainning Accurancy: 0.11732196864137662
Global Trainning Loss: 2.29938844203949
Global test accurancy: 0.1123213296642876
Global test_loss: 2.300631489753723
Global Precision: 0.06456270729280626
Global Recall: 0.1123213296642876
Global f1score: 0.06380851305565073
50
50
number of selected users 50
Global Trainning Accurancy: 0.1176460778296682
Global Trainning Loss: 2.2993164587020876
Global test accurancy: 0.11244004831564189
Global test_loss: 2.300590696334839
Global Precision: 0.06705423475523527
Global Recall: 0.11244004831564189
Global f1score: 0.06457356482677584
50
50
number of selected users 50
Global Trainning Accurancy: 0.11793071276327041
Global Trainning Loss: 2.2992429161071777
Global test accurancy: 0.1128751212068242
Global test_loss: 2.3005480861663816
Global Precision: 0.06956705456638919
Global Recall: 0.1128751212068242
Global f1score: 0.0655337577163924
50
50
number of selected users 50
Global Trainning Accurancy: 0.11810344595572193
Global Trainning Loss: 2.2991693353652956
Global test accurancy: 0.11309065403294749
Global test_loss: 2.3005055475234983
Global Precision: 0.0744095712684577
Global Recall: 0.11309065403294749
Global f1score: 0.06641388562737917
50
50
number of selected users 50
Global Trainning Accurancy: 0.11848152893611195
Global Trainning Loss: 2.2990962076187134
Global test accurancy: 0.11307716370311621
Global test_loss: 2.3004640102386475
Global Precision: 0.0761424615681505
Global Recall: 0.11307716370311621
Global f1score: 0.0668797358568143
50
50
number of selected users 50
Global Trainning Accurancy: 0.11871267972249473
Global Trainning Loss: 2.299020037651062
Global test accurancy: 0.11337913959203458
Global test_loss: 2.3004198598861696
Global Precision: 0.07428554289220002
Global Recall: 0.11337913959203458
Global f1score: 0.06761147219664815
50
50
number of selected users 50
Global Trainning Accurancy: 0.11911774212603889
Global Trainning Loss: 2.298944501876831
Global test accurancy: 0.1136701118938374
Global test_loss: 2.300375232696533
Global Precision: 0.07582359064468648
Global Recall: 0.1136701118938374
Global f1score: 0.06848703870447825
50
50
number of selected users 50
Global Trainning Accurancy: 0.11968926404274116
Global Trainning Loss: 2.298869462013245
Global test accurancy: 0.11410470319370437
Global test_loss: 2.3003310108184816
Global Precision: 0.07868784202679231
Global Recall: 0.11410470319370437
Global f1score: 0.06968576328443532
50
50
number of selected users 50
Global Trainning Accurancy: 0.12013022486892524
Global Trainning Loss: 2.2987941694259644
Global test accurancy: 0.11346719334446562
Global test_loss: 2.3002868556976317
Global Precision: 0.07816951044707993
Global Recall: 0.11346719334446562
Global f1score: 0.06988860634078078
50
50
number of selected users 50
Global Trainning Accurancy: 0.12029618091732622
Global Trainning Loss: 2.2987192392349245
Global test accurancy: 0.11251149464213243
Global test_loss: 2.300244188308716
Global Precision: 0.07885959348677364
Global Recall: 0.11251149464213243
Global f1score: 0.07007143983121338
50
50
number of selected users 50
Global Trainning Accurancy: 0.12036579958798341
Global Trainning Loss: 2.2986446142196657
Global test accurancy: 0.11239343130231395
Global test_loss: 2.3002033710479735
Global Precision: 0.08095869374988893
Global Recall: 0.11239343130231395
Global f1score: 0.0708518774618277
50
50
number of selected users 50
Global Trainning Accurancy: 0.12074468162314757
Global Trainning Loss: 2.298569722175598
Global test accurancy: 0.11221404739757568
Global test_loss: 2.300161681175232
Global Precision: 0.08401318709966796
Global Recall: 0.11221404739757568
Global f1score: 0.07188735015896915
50
50
number of selected users 50
Global Trainning Accurancy: 0.120927617676337
Global Trainning Loss: 2.2984951639175417
Global test accurancy: 0.11289582693195649
Global test_loss: 2.3001187896728514
Global Precision: 0.0860816862404803
Global Recall: 0.11289582693195649
Global f1score: 0.0731531907087944
50
50
number of selected users 50
Global Trainning Accurancy: 0.12119736975917214
Global Trainning Loss: 2.298421411514282
Global test accurancy: 0.11321464662656754
Global test_loss: 2.3000774908065797
Global Precision: 0.08696415792585199
Global Recall: 0.11321464662656754
Global f1score: 0.07416477248888416
50
50
number of selected users 50
Global Trainning Accurancy: 0.12161677562198661
Global Trainning Loss: 2.298348264694214
Global test accurancy: 0.1140929752166457
Global test_loss: 2.3000361251831056
Global Precision: 0.08845771699851045
Global Recall: 0.1140929752166457
Global f1score: 0.0758275100885436
50
50
number of selected users 50
Global Trainning Accurancy: 0.12153753065766529
Global Trainning Loss: 2.298275270462036
Global test accurancy: 0.11458959694059546
Global test_loss: 2.299993886947632
Global Precision: 0.08845561831587617
Global Recall: 0.11458959694059546
Global f1score: 0.07697043494200088
50
50
number of selected users 50
Global Trainning Accurancy: 0.12187011671916223
Global Trainning Loss: 2.29820339679718
Global test accurancy: 0.11415265113245857
Global test_loss: 2.299952745437622
Global Precision: 0.08623681504471609
Global Recall: 0.11415265113245857
Global f1score: 0.07717201277896427
50
50
number of selected users 50
Global Trainning Accurancy: 0.12186727074160131
Global Trainning Loss: 2.2981306743621825
Global test accurancy: 0.1145861313682177
Global test_loss: 2.2999115085601805
Global Precision: 0.08869440252800999
Global Recall: 0.1145861313682177
Global f1score: 0.07841942232842337
50
50
number of selected users 50
Global Trainning Accurancy: 0.12217638360504755
Global Trainning Loss: 2.2980590867996216
Global test accurancy: 0.11532795007350886
Global test_loss: 2.299872484207153
Global Precision: 0.09040286623371799
Global Recall: 0.11532795007350886
Global f1score: 0.0795414513069483
50
50
number of selected users 50
Global Trainning Accurancy: 0.12237030329810232
Global Trainning Loss: 2.2979878568649292
Global test accurancy: 0.11529124463824703
Global test_loss: 2.299834771156311
Global Precision: 0.08974510866425134
Global Recall: 0.11529124463824703
Global f1score: 0.07992551602946639
50
50
number of selected users 50
Global Trainning Accurancy: 0.12277003644771065
Global Trainning Loss: 2.297917642593384
Global test accurancy: 0.11496509887811672
Global test_loss: 2.2997981977462767
Global Precision: 0.09041944799438001
Global Recall: 0.11496509887811672
Global f1score: 0.08051219543988476
50
50
number of selected users 50
Global Trainning Accurancy: 0.1232183798422105
Global Trainning Loss: 2.297847375869751
Global test accurancy: 0.11537880312012111
Global test_loss: 2.2997625350952147
Global Precision: 0.0930208365837821
Global Recall: 0.11537880312012111
Global f1score: 0.08158608892319887
50
50
number of selected users 50
Global Trainning Accurancy: 0.12332356254669927
Global Trainning Loss: 2.2977778577804564
Global test accurancy: 0.11713594718014728
Global test_loss: 2.2997277069091795
Global Precision: 0.10101064730516317
Global Recall: 0.11713594718014728
Global f1score: 0.08422202274226322
50
50
number of selected users 50
Global Trainning Accurancy: 0.12330136704607718
Global Trainning Loss: 2.2977088642120362
Global test accurancy: 0.117560589801479
Global test_loss: 2.2996945858001707
Global Precision: 0.10028377685037125
Global Recall: 0.117560589801479
Global f1score: 0.08523787184716752
50
50
number of selected users 50
Global Trainning Accurancy: 0.12352198215894382
Global Trainning Loss: 2.2976416778564452
Global test accurancy: 0.11845572215615459
Global test_loss: 2.2996642208099365
Global Precision: 0.10022151365162044
Global Recall: 0.11845572215615459
Global f1score: 0.08657944595872671
50
50
number of selected users 50
Global Trainning Accurancy: 0.12340446443875294
Global Trainning Loss: 2.297574210166931
Global test accurancy: 0.11804375660734172
Global test_loss: 2.299635157585144
Global Precision: 0.09938495648288943
Global Recall: 0.11804375660734172
Global f1score: 0.08650759363413005
50
50
number of selected users 50
Global Trainning Accurancy: 0.12397017807615394
Global Trainning Loss: 2.2975070238113404
Global test accurancy: 0.11766099593810339
Global test_loss: 2.2996065855026244
Global Precision: 0.09799146214747445
Global Recall: 0.11766099593810339
Global f1score: 0.08665129252653136
50
50
number of selected users 50
Global Trainning Accurancy: 0.12385709131078187
Global Trainning Loss: 2.297440905570984
Global test accurancy: 0.11833955609026815
Global test_loss: 2.299579744338989
Global Precision: 0.09876056152922373
Global Recall: 0.11833955609026815
Global f1score: 0.08752414296288093
50
50
number of selected users 50
Global Trainning Accurancy: 0.12413388244819304
Global Trainning Loss: 2.297374768257141
Global test accurancy: 0.11878019865721928
Global test_loss: 2.299555287361145
Global Precision: 0.10208400103580376
Global Recall: 0.11878019865721928
Global f1score: 0.08867878781690136
50
50
number of selected users 50
Global Trainning Accurancy: 0.12439393137934696
Global Trainning Loss: 2.29730770111084
Global test accurancy: 0.1187290843130005
Global test_loss: 2.2995328664779664
Global Precision: 0.1041449048030845
Global Recall: 0.1187290843130005
Global f1score: 0.0892534194623264
50
50
number of selected users 50
Global Trainning Accurancy: 0.12460941320494526
Global Trainning Loss: 2.297238974571228
Global test accurancy: 0.11867674234524875
Global test_loss: 2.299507908821106
Global Precision: 0.10369338897438697
Global Recall: 0.11867674234524875
Global f1score: 0.08945734116087298
50
50
number of selected users 50
Global Trainning Accurancy: 0.12483274227811444
Global Trainning Loss: 2.2971700382232667
Global test accurancy: 0.11836620507155432
Global test_loss: 2.2994883012771608
Global Precision: 0.10431160866541586
Global Recall: 0.11836620507155432
Global f1score: 0.08983901924551738
50
50
number of selected users 50
Global Trainning Accurancy: 0.1250127022523702
Global Trainning Loss: 2.2971036720275877
Global test accurancy: 0.118304844052379
Global test_loss: 2.2994710063934325
Global Precision: 0.10535723882803777
Global Recall: 0.118304844052379
Global f1score: 0.09048266682227693
50
50
number of selected users 50
Global Trainning Accurancy: 0.1250178944530902
Global Trainning Loss: 2.297039909362793
Global test accurancy: 0.11826340203070086
Global test_loss: 2.299456524848938
Global Precision: 0.1058629203898128
Global Recall: 0.11826340203070086
Global f1score: 0.09104249626743394
50
50
number of selected users 50
Global Trainning Accurancy: 0.12492717057985823
Global Trainning Loss: 2.2969767665863037
Global test accurancy: 0.11773233786243995
Global test_loss: 2.2994442272186277
Global Precision: 0.10404098794578333
Global Recall: 0.11773233786243995
Global f1score: 0.09076379397026205
50
50
number of selected users 50
Global Trainning Accurancy: 0.12487171603069151
Global Trainning Loss: 2.2969132900238036
Global test accurancy: 0.11797868282759974
Global test_loss: 2.2994319105148318
Global Precision: 0.10447538148193034
Global Recall: 0.11797868282759974
Global f1score: 0.09132315745698823
50
50
number of selected users 50
Global Trainning Accurancy: 0.12527692806841176
Global Trainning Loss: 2.2968502283096313
Global test accurancy: 0.11794243256932675
Global test_loss: 2.2994207382202148
Global Precision: 0.10935724455938671
Global Recall: 0.11794243256932675
Global f1score: 0.09212200208315274
50
50
number of selected users 50
Global Trainning Accurancy: 0.12520057619749717
Global Trainning Loss: 2.2967869424819947
Global test accurancy: 0.11847311921760037
Global test_loss: 2.2994095945358275
Global Precision: 0.11540645482271872
Global Recall: 0.11847311921760037
Global f1score: 0.09339861360132935
50
50
number of selected users 50
Global Trainning Accurancy: 0.1254834380386215
Global Trainning Loss: 2.2967219972610473
Global test accurancy: 0.11782856443302006
Global test_loss: 2.2993979454040527
Global Precision: 0.11391315206006089
Global Recall: 0.11782856443302006
Global f1score: 0.09308040958740818
50
50
number of selected users 50
Global Trainning Accurancy: 0.12573337222940417
Global Trainning Loss: 2.2966584014892577
Global test accurancy: 0.11780451690441913
Global test_loss: 2.299389290809631
Global Precision: 0.11418787233676463
Global Recall: 0.11780451690441913
Global f1score: 0.09357848948222544
50
50
number of selected users 50
Global Trainning Accurancy: 0.12600562575864085
Global Trainning Loss: 2.2965930223464968
Global test accurancy: 0.11794390159735077
Global test_loss: 2.299379744529724
Global Precision: 0.11424460995210232
Global Recall: 0.11794390159735077
Global f1score: 0.09414891195736526
50
50
number of selected users 50
Global Trainning Accurancy: 0.12597856196058763
Global Trainning Loss: 2.296528487205505
Global test accurancy: 0.11774555526350206
Global test_loss: 2.2993738317489623
Global Precision: 0.11571076699634604
Global Recall: 0.11774555526350206
Global f1score: 0.09449009931094612
50
50
number of selected users 50
Global Trainning Accurancy: 0.12605806694126007
Global Trainning Loss: 2.2964642333984373
Global test accurancy: 0.11735338683797165
Global test_loss: 2.2993707942962645
Global Precision: 0.11477568254803902
Global Recall: 0.11735338683797165
Global f1score: 0.09414523286782563
50
50
number of selected users 50
Global Trainning Accurancy: 0.12633549069479136
Global Trainning Loss: 2.296399312019348
Global test accurancy: 0.11800435613370402
Global test_loss: 2.2993681907653807
Global Precision: 0.11428187696580375
Global Recall: 0.11800435613370402
Global f1score: 0.0949810403792506
50
50
number of selected users 50
Global Trainning Accurancy: 0.12649216571346047
Global Trainning Loss: 2.2963331937789917
Global test accurancy: 0.11784483914737898
Global test_loss: 2.2993652772903443
Global Precision: 0.11102814751511657
Global Recall: 0.11784483914737898
Global f1score: 0.09469408542307889
50
50
number of selected users 50
Global Trainning Accurancy: 0.1265887316323461
Global Trainning Loss: 2.2962653160095217
Global test accurancy: 0.11779640719019069
Global test_loss: 2.299364142417908
Global Precision: 0.1150006898176785
Global Recall: 0.11779640719019069
Global f1score: 0.09520544068829825
50
50
number of selected users 50
Global Trainning Accurancy: 0.1267368091890825
Global Trainning Loss: 2.2961987352371214
Global test accurancy: 0.11843738440247904
Global test_loss: 2.2993650341033938
Global Precision: 0.11542657844342581
Global Recall: 0.11843738440247904
Global f1score: 0.09597203491468383
50
50
number of selected users 50
Global Trainning Accurancy: 0.12653095576965817
Global Trainning Loss: 2.296134839057922
Global test accurancy: 0.1182141263810193
Global test_loss: 2.2993736124038695
Global Precision: 0.1154677170978776
Global Recall: 0.1182141263810193
Global f1score: 0.09598844560084507
50
50
number of selected users 50
Global Trainning Accurancy: 0.12655055280186595
Global Trainning Loss: 2.296069040298462
Global test accurancy: 0.11858689901613823
Global test_loss: 2.299379754066467
Global Precision: 0.11916625720017036
Global Recall: 0.11858689901613823
Global f1score: 0.09671889339976297
50
50
number of selected users 50
Global Trainning Accurancy: 0.12687137583234515
Global Trainning Loss: 2.296002912521362
Global test accurancy: 0.11932542074774263
Global test_loss: 2.2993873977661132
Global Precision: 0.12078249471228603
Global Recall: 0.11932542074774263
Global f1score: 0.09769562606676807
50
50
number of selected users 50
Global Trainning Accurancy: 0.1267645005574408
Global Trainning Loss: 2.2959339094161986
Global test accurancy: 0.11912523001999294
Global test_loss: 2.2993953037261963
Global Precision: 0.12008215974831074
Global Recall: 0.11912523001999294
Global f1score: 0.09766844990355689
50
50
number of selected users 50
Global Trainning Accurancy: 0.12709837090397225
Global Trainning Loss: 2.2958638048171998
Global test accurancy: 0.11987472553482366
Global test_loss: 2.2994044733047487
Global Precision: 0.12136258740127165
Global Recall: 0.11987472553482366
Global f1score: 0.09857365780102524
50
50
number of selected users 50
Global Trainning Accurancy: 0.1272298680580249
Global Trainning Loss: 2.295792717933655
Global test accurancy: 0.12051983171996172
Global test_loss: 2.2994147872924806
Global Precision: 0.12193543144088
Global Recall: 0.12051983171996172
Global f1score: 0.09950519887461476
50
50
number of selected users 50
Global Trainning Accurancy: 0.12734618118224966
Global Trainning Loss: 2.2957185554504393
Global test accurancy: 0.12034927191550211
Global test_loss: 2.2994241857528688
Global Precision: 0.12011901979260581
Global Recall: 0.12034927191550211
Global f1score: 0.09942796484421361
50
50
number of selected users 50
Global Trainning Accurancy: 0.1272461564516708
Global Trainning Loss: 2.2956453275680544
Global test accurancy: 0.12055919373110692
Global test_loss: 2.299437255859375
Global Precision: 0.11724029595830399
Global Recall: 0.12055919373110692
Global f1score: 0.09989386217108973
50
50
number of selected users 50
Global Trainning Accurancy: 0.12727780336791783
Global Trainning Loss: 2.295573801994324
Global test accurancy: 0.12065446615887575
Global test_loss: 2.299457564353943
Global Precision: 0.11785325145505156
Global Recall: 0.12065446615887575
Global f1score: 0.10034878754530793
50
50
number of selected users 50
Global Trainning Accurancy: 0.1271945091406335
Global Trainning Loss: 2.2954998588562012
Global test accurancy: 0.12030394839422513
Global test_loss: 2.299477229118347
Global Precision: 0.12100480709097038
Global Recall: 0.12030394839422513
Global f1score: 0.10022533102843356
50
50
number of selected users 50
Global Trainning Accurancy: 0.1274276618757115
Global Trainning Loss: 2.2954245090484617
Global test accurancy: 0.1199602902101876
Global test_loss: 2.299496865272522
Global Precision: 0.11848739831441707
Global Recall: 0.1199602902101876
Global f1score: 0.09989782634226108
50
50
number of selected users 50
Global Trainning Accurancy: 0.12743585989470305
Global Trainning Loss: 2.295348868370056
Global test accurancy: 0.11992806459743305
Global test_loss: 2.299519100189209
Global Precision: 0.11932536884281467
Global Recall: 0.11992806459743305
Global f1score: 0.10012069189760849
50
50
number of selected users 50
Global Trainning Accurancy: 0.12720939525571592
Global Trainning Loss: 2.2952749729156494
Global test accurancy: 0.12011427824855492
Global test_loss: 2.2995449018478396
Global Precision: 0.1181398099505457
Global Recall: 0.12011427824855492
Global f1score: 0.10052037425133556
50
50
number of selected users 50
Global Trainning Accurancy: 0.12739661941793973
Global Trainning Loss: 2.2951995372772216
Global test accurancy: 0.11991546786951739
Global test_loss: 2.2995708751678468
Global Precision: 0.11924482827014521
Global Recall: 0.11991546786951739
Global f1score: 0.10061080328620768
50
50
number of selected users 50
Global Trainning Accurancy: 0.127111537308982
Global Trainning Loss: 2.295125403404236
Global test accurancy: 0.12037029393753594
Global test_loss: 2.2996009826660155
Global Precision: 0.11982595438712669
Global Recall: 0.12037029393753594
Global f1score: 0.10111139347743263
50
50
number of selected users 50
Global Trainning Accurancy: 0.12713094972840358
Global Trainning Loss: 2.2950499773025514
Global test accurancy: 0.1201624781881339
Global test_loss: 2.2996320009231566
Global Precision: 0.11946733098837635
Global Recall: 0.1201624781881339
Global f1score: 0.10101669915314676
50
50
number of selected users 50
Global Trainning Accurancy: 0.12727605819144236
Global Trainning Loss: 2.29497193813324
Global test accurancy: 0.12002709476396134
Global test_loss: 2.2996614456176756
Global Precision: 0.1184291731307469
Global Recall: 0.12002709476396134
Global f1score: 0.10113145515159176
50
50
number of selected users 50
Global Trainning Accurancy: 0.12735548772665903
Global Trainning Loss: 2.294892749786377
Global test accurancy: 0.1200173125674839
Global test_loss: 2.2996933460235596
Global Precision: 0.12190732842635306
Global Recall: 0.1200173125674839
Global f1score: 0.10152163392228968
50
50
number of selected users 50
Global Trainning Accurancy: 0.12772912815550344
Global Trainning Loss: 2.2948139333724975
Global test accurancy: 0.12010062477809913
Global test_loss: 2.2997242498397825
Global Precision: 0.12567681807186942
Global Recall: 0.12010062477809913
Global f1score: 0.10200032616970117
50
50
number of selected users 50
Global Trainning Accurancy: 0.1277705489146267
Global Trainning Loss: 2.294739980697632
Global test accurancy: 0.12002424412951973
Global test_loss: 2.299760203361511
Global Precision: 0.1253440122384594
Global Recall: 0.12002424412951973
Global f1score: 0.1021967930186502
50
50
number of selected users 50
Global Trainning Accurancy: 0.12757232610049024
Global Trainning Loss: 2.294663677215576
Global test accurancy: 0.11989592043831324
Global test_loss: 2.299798264503479
Global Precision: 0.12401547251755779
Global Recall: 0.11989592043831324
Global f1score: 0.10218694862254694
50
50
number of selected users 50
Global Trainning Accurancy: 0.12779601343995137
Global Trainning Loss: 2.294577569961548
Global test accurancy: 0.11985933930301897
Global test_loss: 2.2998307132720948
Global Precision: 0.12402926298593692
Global Recall: 0.11985933930301897
Global f1score: 0.10245372444210328
50
50
number of selected users 50
Global Trainning Accurancy: 0.12764547263181705
Global Trainning Loss: 2.2944932794570922
Global test accurancy: 0.11958240575484373
Global test_loss: 2.2998679161071776
Global Precision: 0.12374903116668372
Global Recall: 0.11958240575484373
Global f1score: 0.10238306305071411
50
50
number of selected users 50
Global Trainning Accurancy: 0.12790149286651203
Global Trainning Loss: 2.29441294670105
Global test accurancy: 0.12016060663370268
Global test_loss: 2.2999147319793702
Global Precision: 0.12276269253971914
Global Recall: 0.12016060663370268
Global f1score: 0.10305790651201349
50
50
number of selected users 50
Global Trainning Accurancy: 0.12821342026585905
Global Trainning Loss: 2.294326891899109
Global test accurancy: 0.12065252525967647
Global test_loss: 2.2999600267410276
Global Precision: 0.12097314556465981
Global Recall: 0.12065252525967647
Global f1score: 0.10350165979100862
50
50
number of selected users 50
Global Trainning Accurancy: 0.12780965695656743
Global Trainning Loss: 2.2942382764816283
Global test accurancy: 0.120062955337769
Global test_loss: 2.2999994277954103
Global Precision: 0.11983515824252021
Global Recall: 0.120062955337769
Global f1score: 0.10292671793222544
50
50
number of selected users 50
Global Trainning Accurancy: 0.12757475170651053
Global Trainning Loss: 2.294145269393921
Global test accurancy: 0.12108697719827242
Global test_loss: 2.3000364875793458
Global Precision: 0.12001725711945424
Global Recall: 0.12108697719827242
Global f1score: 0.10402422477898234
50
50
number of selected users 50
Global Trainning Accurancy: 0.1277443809887103
Global Trainning Loss: 2.294054217338562
Global test accurancy: 0.12087650661692863
Global test_loss: 2.300081548690796
Global Precision: 0.11998898907266445
Global Recall: 0.12087650661692863
Global f1score: 0.10396943484044135
50
50
number of selected users 50
Global Trainning Accurancy: 0.1279567823186767
Global Trainning Loss: 2.29395613193512
Global test accurancy: 0.12093849109500363
Global test_loss: 2.300127501487732
Global Precision: 0.11883663669013815
Global Recall: 0.12093849109500363
Global f1score: 0.104167304108157
50
50
number of selected users 50
Global Trainning Accurancy: 0.12825047879955642
Global Trainning Loss: 2.2938511562347412
Global test accurancy: 0.12051613696400873
Global test_loss: 2.300165958404541
Global Precision: 0.11925851207426189
Global Recall: 0.12051613696400873
Global f1score: 0.10408946149942379
50
50
number of selected users 50
Global Trainning Accurancy: 0.12855106450992418
Global Trainning Loss: 2.2937495517730713
Global test accurancy: 0.12146321665191609
Global test_loss: 2.3002096462249755
Global Precision: 0.11997257435436844
Global Recall: 0.12146321665191609
Global f1score: 0.10545503583180313
50
50
number of selected users 50
Global Trainning Accurancy: 0.12902407741874514
Global Trainning Loss: 2.2936476850509644
Global test accurancy: 0.12102086929802311
Global test_loss: 2.300264391899109
Global Precision: 0.11774026315679222
Global Recall: 0.12102086929802311
Global f1score: 0.10505025413181272
50
50
number of selected users 50
Global Trainning Accurancy: 0.128956765572749
Global Trainning Loss: 2.293544316291809
Global test accurancy: 0.12050333523812573
Global test_loss: 2.300325117111206
Global Precision: 0.11671353533160525
Global Recall: 0.12050333523812573
Global f1score: 0.10482139924682676
50
50
number of selected users 50
Global Trainning Accurancy: 0.1291491842592742
Global Trainning Loss: 2.2934328651428224
Global test accurancy: 0.12067757408223824
Global test_loss: 2.300392384529114
Global Precision: 0.11691134321869318
Global Recall: 0.12067757408223824
Global f1score: 0.1051343966558992
exp_no  0
0_dataset_CIFAR10_algorithm_FedAvg_model_CNN_10_50_0.8_31_07_2024
