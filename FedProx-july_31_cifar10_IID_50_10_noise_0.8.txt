============================================================
Summary of training process:
FL Algorithm: FedProx
model: CNN
optimizer: SGD
Batch size: 124
Global_iters: 200
Local_iters: 10
experiments: 1
device : 0
Learning rate: 0.01
Proximal hyperparameter 1.0
============================================================
/proj/bhuyan24/fed-divergence
CIFAR10
./data/data/noisy/0.8_50_10/train/cifa_train.json
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:11<39:19, 11.86s/it]  1%|          | 2/200 [00:18<28:31,  8.64s/it]  2%|▏         | 3/200 [00:24<24:58,  7.61s/it]  2%|▏         | 4/200 [00:30<23:14,  7.11s/it]  2%|▎         | 5/200 [00:37<22:09,  6.82s/it]  3%|▎         | 6/200 [00:43<21:22,  6.61s/it]  4%|▎         | 7/200 [00:49<20:48,  6.47s/it]  4%|▍         | 8/200 [00:55<20:25,  6.38s/it]  4%|▍         | 9/200 [01:02<20:06,  6.32s/it]  5%|▌         | 10/200 [01:08<19:54,  6.29s/it]  6%|▌         | 11/200 [01:14<19:41,  6.25s/it]  6%|▌         | 12/200 [01:20<19:30,  6.23s/it]  6%|▋         | 13/200 [01:26<19:23,  6.22s/it]  7%|▋         | 14/200 [01:32<19:15,  6.21s/it]  8%|▊         | 15/200 [01:39<19:07,  6.20s/it]  8%|▊         | 16/200 [01:45<18:59,  6.19s/it]  8%|▊         | 17/200 [01:51<18:49,  6.17s/it]  9%|▉         | 18/200 [01:57<18:40,  6.16s/it] 10%|▉         | 19/200 [02:03<18:34,  6.16s/it] 10%|█         | 20/200 [02:10<18:33,  6.19s/it] 10%|█         | 21/200 [02:16<18:30,  6.20s/it] 11%|█         | 22/200 [02:22<18:26,  6.22s/it] 12%|█▏        | 23/200 [02:28<18:15,  6.19s/it] 12%|█▏        | 24/200 [02:34<18:12,  6.21s/it] 12%|█▎        | 25/200 [02:41<18:06,  6.21s/it] 13%|█▎        | 26/200 [02:47<17:59,  6.21s/it] 14%|█▎        | 27/200 [02:53<17:54,  6.21s/it] 14%|█▍        | 28/200 [02:59<17:48,  6.21s/it] 14%|█▍        | 29/200 [03:05<17:43,  6.22s/it] 15%|█▌        | 30/200 [03:12<17:41,  6.24s/it] 16%|█▌        | 31/200 [03:18<17:31,  6.22s/it] 16%|█▌        | 32/200 [03:24<17:24,  6.22s/it] 16%|█▋        | 33/200 [03:30<17:18,  6.22s/it] 17%|█▋        | 34/200 [03:36<17:07,  6.19s/it] 18%|█▊        | 35/200 [03:43<16:57,  6.17s/it] 18%|█▊        | 36/200 [03:49<16:50,  6.16s/it] 18%|█▊        | 37/200 [03:55<16:42,  6.15s/it] 19%|█▉        | 38/200 [04:01<16:37,  6.15s/it] 20%|█▉        | 39/200 [04:07<16:31,  6.16s/it] 20%|██        | 40/200 [04:13<16:26,  6.17s/it] 20%|██        | 41/200 [04:20<16:23,  6.19s/it] 21%|██        | 42/200 [04:26<16:18,  6.19s/it] 22%|██▏       | 43/200 [04:32<16:12,  6.19s/it] 22%|██▏       | 44/200 [04:38<16:05,  6.19s/it] 22%|██▎       | 45/200 [04:44<15:59,  6.19s/it] 23%|██▎       | 46/200 [04:51<15:51,  6.18s/it] 24%|██▎       | 47/200 [04:57<15:45,  6.18s/it] 24%|██▍       | 48/200 [05:03<15:40,  6.19s/it] 24%|██▍       | 49/200 [05:09<15:33,  6.18s/it] 25%|██▌       | 50/200 [05:15<15:27,  6.18s/it] 26%|██▌       | 51/200 [05:21<15:20,  6.18s/it] 26%|██▌       | 52/200 [05:28<15:10,  6.15s/it] 26%|██▋       | 53/200 [05:34<15:02,  6.14s/it] 27%|██▋       | 54/200 [05:40<14:58,  6.16s/it] 28%|██▊       | 55/200 [05:46<14:51,  6.15s/it] 28%|██▊       | 56/200 [05:52<14:55,  6.22s/it] 28%|██▊       | 57/200 [05:59<14:54,  6.26s/it] 29%|██▉       | 58/200 [06:05<14:45,  6.24s/it] 30%|██▉       | 59/200 [06:11<14:37,  6.23s/it] 30%|███       | 60/200 [06:17<14:28,  6.20s/it] 30%|███       | 61/200 [06:23<14:20,  6.19s/it] 31%|███       | 62/200 [06:30<14:13,  6.19s/it] 32%|███▏      | 63/200 [06:36<14:06,  6.18s/it] 32%|███▏      | 64/200 [06:42<13:59,  6.17s/it] 32%|███▎      | 65/200 [06:48<14:02,  6.24s/it] 33%|███▎      | 66/200 [06:55<14:01,  6.28s/it] 34%|███▎      | 67/200 [07:01<14:00,  6.32s/it] 34%|███▍      | 68/200 [07:07<13:47,  6.27s/it] 34%|███▍      | 69/200 [07:13<13:37,  6.24s/it] 35%|███▌      | 70/200 [07:20<13:31,  6.24s/it] 36%|███▌      | 71/200 [07:26<13:22,  6.22s/it] 36%|███▌      | 72/200 [07:32<13:13,  6.20s/it] 36%|███▋      | 73/200 [07:38<13:04,  6.18s/it] 37%|███▋      | 74/200 [07:44<12:56,  6.16s/it] 38%|███▊      | 75/200 [07:50<12:49,  6.15s/it] 38%|███▊      | 76/200 [07:57<12:42,  6.15s/it] 38%|███▊      | 77/200 [08:03<12:36,  6.15s/it] 39%|███▉      | 78/200 [08:09<12:30,  6.15s/it] 40%|███▉      | 79/200 [08:15<12:35,  6.24s/it] 40%|████      | 80/200 [08:22<12:37,  6.31s/it] 40%|████      | 81/200 [08:28<12:36,  6.35s/it] 41%|████      | 82/200 [08:35<12:29,  6.35s/it] 42%|████▏     | 83/200 [08:41<12:19,  6.32s/it] 42%|████▏     | 84/200 [08:47<12:08,  6.28s/it] 42%|████▎     | 85/200 [08:53<12:00,  6.26s/it] 43%|████▎     | 86/200 [08:59<11:51,  6.24s/it] 44%|████▎     | 87/200 [09:06<11:43,  6.23s/it] 44%|████▍     | 88/200 [09:12<11:35,  6.21s/it] 44%|████▍     | 89/200 [09:18<11:27,  6.19s/it] 45%|████▌     | 90/200 [09:24<11:20,  6.19s/it] 46%|████▌     | 91/200 [09:30<11:13,  6.18s/it] 46%|████▌     | 92/200 [09:37<11:10,  6.21s/it] 46%|████▋     | 93/200 [09:43<11:02,  6.19s/it] 47%|████▋     | 94/200 [09:49<10:56,  6.19s/it] 48%|████▊     | 95/200 [09:55<10:49,  6.18s/it] 48%|████▊     | 96/200 [10:01<10:44,  6.19s/it] 48%|████▊     | 97/200 [10:07<10:38,  6.19s/it] 49%|████▉     | 98/200 [10:14<10:30,  6.18s/it] 50%|████▉     | 99/200 [10:20<10:23,  6.17s/it] 50%|█████     | 100/200 [10:26<10:15,  6.15s/it] 50%|█████     | 101/200 [10:32<10:08,  6.15s/it] 51%|█████     | 102/200 [10:38<10:04,  6.17s/it] 52%|█████▏    | 103/200 [10:44<10:01,  6.20s/it] 52%|█████▏    | 104/200 [10:51<10:01,  6.26s/it] 52%|█████▎    | 105/200 [10:57<09:58,  6.30s/it] 53%|█████▎    | 106/200 [11:03<09:48,  6.26s/it] 54%|█████▎    | 107/200 [11:10<09:39,  6.23s/it] 54%|█████▍    | 108/200 [11:16<09:31,  6.21s/it] 55%|█████▍    | 109/200 [11:22<09:23,  6.19s/it] 55%|█████▌    | 110/200 [11:28<09:16,  6.18s/it] 56%|█████▌    | 111/200 [11:34<09:12,  6.20s/it] 56%|█████▌    | 112/200 [11:41<09:05,  6.20s/it] 56%|█████▋    | 113/200 [11:47<08:59,  6.20s/it] 57%|█████▋    | 114/200 [11:53<08:52,  6.19s/it] 57%|█████▊    | 115/200 [11:59<08:45,  6.19s/it] 58%|█████▊    | 116/200 [12:05<08:38,  6.18s/it] 58%|█████▊    | 117/200 [12:11<08:32,  6.17s/it] 59%|█████▉    | 118/200 [12:18<08:25,  6.17s/it] 60%|█████▉    | 119/200 [12:24<08:22,  6.21s/it] 60%|██████    | 120/200 [12:30<08:16,  6.21s/it] 60%|██████    | 121/200 [12:36<08:09,  6.20s/it] 61%|██████    | 122/200 [12:42<08:02,  6.19s/it] 62%|██████▏   | 123/200 [12:49<07:56,  6.19s/it] 62%|██████▏   | 124/200 [12:55<07:50,  6.19s/it] 62%|██████▎   | 125/200 [13:01<07:44,  6.19s/it] 63%|██████▎   | 126/200 [13:07<07:37,  6.19s/it] 64%|██████▎   | 127/200 [13:13<07:31,  6.19s/it] 64%|██████▍   | 128/200 [13:20<07:25,  6.19s/it] 64%|██████▍   | 129/200 [13:26<07:20,  6.21s/it] 65%|██████▌   | 130/200 [13:32<07:12,  6.18s/it] 66%|██████▌   | 131/200 [13:38<07:05,  6.17s/it] 66%|██████▌   | 132/200 [13:44<06:59,  6.17s/it] 66%|██████▋   | 133/200 [13:50<06:53,  6.17s/it] 67%|██████▋   | 134/200 [13:57<06:47,  6.17s/it] 68%|██████▊   | 135/200 [14:03<06:42,  6.19s/it] 68%|██████▊   | 136/200 [14:09<06:37,  6.20s/it] 68%|██████▊   | 137/200 [14:15<06:31,  6.21s/it] 69%|██████▉   | 138/200 [14:21<06:23,  6.19s/it] 70%|██████▉   | 139/200 [14:28<06:17,  6.18s/it] 70%|███████   | 140/200 [14:34<06:11,  6.19s/it] 70%|███████   | 141/200 [14:40<06:04,  6.17s/it] 71%|███████   | 142/200 [14:46<05:57,  6.16s/it] 72%|███████▏  | 143/200 [14:52<05:51,  6.16s/it] 72%|███████▏  | 144/200 [14:58<05:45,  6.17s/it] 72%|███████▎  | 145/200 [15:05<05:38,  6.16s/it] 73%|███████▎  | 146/200 [15:11<05:32,  6.15s/it] 74%|███████▎  | 147/200 [15:17<05:25,  6.14s/it] 74%|███████▍  | 148/200 [15:23<05:19,  6.15s/it] 74%|███████▍  | 149/200 [15:29<05:14,  6.16s/it] 75%|███████▌  | 150/200 [15:35<05:07,  6.14s/it] 76%|███████▌  | 151/200 [15:41<05:00,  6.14s/it] 76%|███████▌  | 152/200 [15:48<04:55,  6.15s/it] 76%|███████▋  | 153/200 [15:54<04:48,  6.14s/it] 77%|███████▋  | 154/200 [16:00<04:42,  6.14s/it] 78%|███████▊  | 155/200 [16:06<04:35,  6.13s/it] 78%|███████▊  | 156/200 [16:12<04:29,  6.13s/it] 78%|███████▊  | 157/200 [16:18<04:23,  6.14s/it] 79%|███████▉  | 158/200 [16:24<04:18,  6.15s/it] 80%|███████▉  | 159/200 [16:31<04:13,  6.17s/it] 80%|████████  | 160/200 [16:37<04:06,  6.17s/it] 80%|████████  | 161/200 [16:43<04:01,  6.19s/it] 81%|████████  | 162/200 [16:49<03:57,  6.25s/it] 82%|████████▏ | 163/200 [16:56<03:51,  6.25s/it] 82%|████████▏ | 164/200 [17:02<03:44,  6.23s/it] 82%|████████▎ | 165/200 [17:08<03:37,  6.21s/it] 83%|████████▎ | 166/200 [17:14<03:30,  6.20s/it] 84%|████████▎ | 167/200 [17:20<03:23,  6.18s/it] 84%|████████▍ | 168/200 [17:26<03:17,  6.18s/it] 84%|████████▍ | 169/200 [17:33<03:11,  6.17s/it] 85%|████████▌ | 170/200 [17:39<03:04,  6.15s/it] 86%|████████▌ | 171/200 [17:45<02:58,  6.14s/it] 86%|████████▌ | 172/200 [17:51<02:52,  6.15s/it] 86%|████████▋ | 173/200 [17:57<02:46,  6.16s/it] 87%|████████▋ | 174/200 [18:03<02:39,  6.15s/it] 88%|████████▊ | 175/200 [18:10<02:34,  6.19s/it] 88%|████████▊ | 176/200 [18:16<02:28,  6.18s/it] 88%|████████▊ | 177/200 [18:22<02:22,  6.20s/it] 89%|████████▉ | 178/200 [18:28<02:17,  6.25s/it] 90%|████████▉ | 179/200 [18:35<02:12,  6.29s/it] 90%|█████████ | 180/200 [18:41<02:05,  6.29s/it] 90%|█████████ | 181/200 [18:47<01:58,  6.24s/it] 91%|█████████ | 182/200 [18:53<01:51,  6.20s/it] 92%|█████████▏| 183/200 [18:59<01:44,  6.17s/it] 92%|█████████▏| 184/200 [19:06<01:38,  6.17s/it] 92%|█████████▎| 185/200 [19:12<01:32,  6.16s/it] 93%|█████████▎| 186/200 [19:18<01:26,  6.15s/it] 94%|█████████▎| 187/200 [19:24<01:19,  6.14s/it] 94%|█████████▍| 188/200 [19:30<01:13,  6.13s/it] 94%|█████████▍| 189/200 [19:36<01:07,  6.14s/it] 95%|█████████▌| 190/200 [19:42<01:01,  6.14s/it] 96%|█████████▌| 191/200 [19:49<00:55,  6.17s/it] 96%|█████████▌| 192/200 [19:55<00:49,  6.21s/it] 96%|█████████▋| 193/200 [20:01<00:43,  6.23s/it] 97%|█████████▋| 194/200 [20:07<00:37,  6.24s/it] 98%|█████████▊| 195/200 [20:14<00:31,  6.25s/it] 98%|█████████▊| 196/200 [20:20<00:25,  6.26s/it] 98%|█████████▊| 197/200 [20:26<00:18,  6.24s/it] 99%|█████████▉| 198/200 [20:32<00:12,  6.25s/it]100%|█████████▉| 199/200 [20:39<00:06,  6.26s/it]100%|██████████| 200/200 [20:45<00:00,  6.26s/it]100%|██████████| 200/200 [20:45<00:00,  6.23s/it]
50
50
number of selected users 50
Global Trainning Accurancy: 0.10063176238759938
Global Trainning Loss: 2.3031054496765138
Global test accurancy: 0.10112996285527823
Global test_loss: 2.3030942726135253
Global Precision: 0.01030821324628988
Global Recall: 0.10112996285527823
Global f1score: 0.018697677431556804
50
50
number of selected users 50
Global Trainning Accurancy: 0.10055356075279367
Global Trainning Loss: 2.3029544353485107
Global test accurancy: 0.10008361060589036
Global test_loss: 2.302951064109802
Global Precision: 0.018693262322145916
Global Recall: 0.10008361060589036
Global f1score: 0.021301511270193058
50
50
number of selected users 50
Global Trainning Accurancy: 0.10256121641895267
Global Trainning Loss: 2.3028358745574953
Global test accurancy: 0.10133888094400709
Global test_loss: 2.302839608192444
Global Precision: 0.02062544127148388
Global Recall: 0.10133888094400709
Global f1score: 0.032838174092903544
50
50
number of selected users 50
Global Trainning Accurancy: 0.10205886275190304
Global Trainning Loss: 2.302743091583252
Global test accurancy: 0.10053570767212026
Global test_loss: 2.302753529548645
Global Precision: 0.02015169899041248
Global Recall: 0.10053570767212026
Global f1score: 0.03246202107016279
50
50
number of selected users 50
Global Trainning Accurancy: 0.10083496943910719
Global Trainning Loss: 2.3026712322235108
Global test accurancy: 0.09960984342213161
Global test_loss: 2.3026874971389772
Global Precision: 0.019299028122791127
Global Recall: 0.09960984342213161
Global f1score: 0.024315323867605335
50
50
number of selected users 50
Global Trainning Accurancy: 0.10022873436103595
Global Trainning Loss: 2.3026154899597167
Global test accurancy: 0.09942108161995704
Global test_loss: 2.3026374912261964
Global Precision: 0.01636736832724786
Global Recall: 0.09942108161995704
Global f1score: 0.01949514103252968
50
50
number of selected users 50
Global Trainning Accurancy: 0.10043887894581426
Global Trainning Loss: 2.3025723361968993
Global test accurancy: 0.0996826603510243
Global test_loss: 2.3026001024246217
Global Precision: 0.01191397886204763
Global Recall: 0.0996826603510243
Global f1score: 0.0184260382149212
50
50
number of selected users 50
Global Trainning Accurancy: 0.10054207718720182
Global Trainning Loss: 2.302539176940918
Global test accurancy: 0.0996773373996614
Global test_loss: 2.3025724029541017
Global Precision: 0.011919688457025063
Global Recall: 0.0996773373996614
Global f1score: 0.018435108237865445
50
50
number of selected users 50
Global Trainning Accurancy: 0.10136694944419508
Global Trainning Loss: 2.3025137519836427
Global test accurancy: 0.10029341747100294
Global test_loss: 2.3025519943237303
Global Precision: 0.021270041927004994
Global Recall: 0.10029341747100294
Global f1score: 0.02363758657577217
50
50
number of selected users 50
Global Trainning Accurancy: 0.10461893554631976
Global Trainning Loss: 2.302494478225708
Global test accurancy: 0.10343380585705947
Global test_loss: 2.3025371551513674
Global Precision: 0.02196574160340447
Global Recall: 0.10343380585705947
Global f1score: 0.03355371117265168
50
50
number of selected users 50
Global Trainning Accurancy: 0.10567331643150864
Global Trainning Loss: 2.302479958534241
Global test accurancy: 0.10535457312296956
Global test_loss: 2.3025269985198973
Global Precision: 0.02138763618072817
Global Recall: 0.10535457312296956
Global f1score: 0.035454780880796635
50
50
number of selected users 50
Global Trainning Accurancy: 0.10429990174574635
Global Trainning Loss: 2.302468957901001
Global test accurancy: 0.10150827823215026
Global test_loss: 2.302520079612732
Global Precision: 0.020497967636052637
Global Recall: 0.10150827823215026
Global f1score: 0.03131881780509219
50
50
number of selected users 50
Global Trainning Accurancy: 0.10208755196774634
Global Trainning Loss: 2.3024604177474974
Global test accurancy: 0.10116605592327238
Global test_loss: 2.3025157403945924
Global Precision: 0.021277488672853933
Global Recall: 0.10116605592327238
Global f1score: 0.024830903390682453
50
50
number of selected users 50
Global Trainning Accurancy: 0.10161968870059179
Global Trainning Loss: 2.3024537658691404
Global test accurancy: 0.10092273213115562
Global test_loss: 2.3025135040283202
Global Precision: 0.023657381897388387
Global Recall: 0.10092273213115562
Global f1score: 0.020257322118543735
50
50
number of selected users 50
Global Trainning Accurancy: 0.10103248799207
Global Trainning Loss: 2.302448253631592
Global test accurancy: 0.10084215200967471
Global test_loss: 2.3025124788284304
Global Precision: 0.012170175665923072
Global Recall: 0.10084215200967471
Global f1score: 0.018800784390576388
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024434995651246
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302512288093567
Global Precision: 0.010284697381525045
Global Recall: 0.1007637206371257
Global f1score: 0.01864501374110721
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024390935897827
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302512693405151
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024350023269653
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025129795074464
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.30243088722229
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025134801864624
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302426815032959
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302513666152954
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024223852157593
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302513723373413
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302417664527893
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025134372711182
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302412385940552
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025128173828127
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3024067640304566
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302511739730835
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302400679588318
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302510366439819
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302393832206726
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302508282661438
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023863077163695
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302505822181702
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023780155181885
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3025025367736816
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023692750930786
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024989128112794
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023598051071166
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302494568824768
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023498821258546
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302490162849426
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023393535614014
Global test accurancy: 0.1007637206371257
Global test_loss: 2.30248571395874
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302328267097473
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024810647964475
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3023169946670534
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302476601600647
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302305693626404
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024716138839723
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3022940254211424
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302465672492981
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.3022817182540893
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024590730667116
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.1010046715803871
Global Trainning Loss: 2.302269058227539
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302452006340027
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.10097482083411845
Global Trainning Loss: 2.302256283760071
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024445295333864
Global Precision: 0.010283891186373115
Global Recall: 0.1007637206371257
Global f1score: 0.01864366051751119
50
50
number of selected users 50
Global Trainning Accurancy: 0.10100718329366537
Global Trainning Loss: 2.3022432851791383
Global test accurancy: 0.1007637206371257
Global test_loss: 2.3024370288848877
Global Precision: 0.010285651479558395
Global Recall: 0.1007637206371257
Global f1score: 0.018646545846770737
50
50
number of selected users 50
Global Trainning Accurancy: 0.10103499970534827
Global Trainning Loss: 2.302229852676392
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302429618835449
Global Precision: 0.010287842479603534
Global Recall: 0.1007637206371257
Global f1score: 0.0186501458433992
50
50
number of selected users 50
Global Trainning Accurancy: 0.101006222726931
Global Trainning Loss: 2.302216286659241
Global test accurancy: 0.1007637206371257
Global test_loss: 2.302422003746033
Global Precision: 0.010287842479603534
Global Recall: 0.1007637206371257
Global f1score: 0.0186501458433992
50
50
number of selected users 50
Global Trainning Accurancy: 0.10106888821678095
Global Trainning Loss: 2.3022026109695433
Global test accurancy: 0.10069151847106071
Global test_loss: 2.302413945198059
Global Precision: 0.010280435856310702
Global Recall: 0.10069151847106071
Global f1score: 0.018636734340274866
50
50
number of selected users 50
Global Trainning Accurancy: 0.10113150142478997
Global Trainning Loss: 2.3021886777877807
Global test accurancy: 0.1006928472569933
Global test_loss: 2.302405843734741
Global Precision: 0.01236061389044935
Global Recall: 0.1006928472569933
Global f1score: 0.01880813781633794
50
50
number of selected users 50
Global Trainning Accurancy: 0.1011042534683867
Global Trainning Loss: 2.3021745538711547
Global test accurancy: 0.1006193178452286
Global test_loss: 2.302397723197937
Global Precision: 0.012355737032220511
Global Recall: 0.1006193178452286
Global f1score: 0.01879878737497562
50
50
number of selected users 50
Global Trainning Accurancy: 0.10109672076422478
Global Trainning Loss: 2.3021601629257202
Global test accurancy: 0.10052595543552435
Global test_loss: 2.3023890924453734
Global Precision: 0.013440979796336476
Global Recall: 0.10052595543552435
Global f1score: 0.018930028122873074
50
50
number of selected users 50
Global Trainning Accurancy: 0.10102921421489028
Global Trainning Loss: 2.302145471572876
Global test accurancy: 0.10054128302352636
Global test_loss: 2.302380094528198
Global Precision: 0.014890575029011798
Global Recall: 0.10054128302352636
Global f1score: 0.0192587156669523
50
50
number of selected users 50
Global Trainning Accurancy: 0.10095014370761322
Global Trainning Loss: 2.302130765914917
Global test accurancy: 0.10045760101515816
Global test_loss: 2.3023707485198974
Global Precision: 0.014086474509381912
Global Recall: 0.10045760101515816
Global f1score: 0.019248003985357094
50
50
number of selected users 50
Global Trainning Accurancy: 0.10088664964529112
Global Trainning Loss: 2.302115550041199
Global test accurancy: 0.10039407542521213
Global test_loss: 2.302360887527466
Global Precision: 0.016287076174298656
Global Recall: 0.10039407542521213
Global f1score: 0.01960123105571109
50
50
number of selected users 50
Global Trainning Accurancy: 0.10078125158197493
Global Trainning Loss: 2.3021000671386718
Global test accurancy: 0.1004634837946205
Global test_loss: 2.302350916862488
Global Precision: 0.017769758926613265
Global Recall: 0.1004634837946205
Global f1score: 0.019904023631065394
50
50
number of selected users 50
Global Trainning Accurancy: 0.10074968550507996
Global Trainning Loss: 2.302084550857544
Global test accurancy: 0.10046255522809201
Global test_loss: 2.3023406362533567
Global Precision: 0.019788870689238198
Global Recall: 0.10046255522809201
Global f1score: 0.020336905443265118
50
50
number of selected users 50
Global Trainning Accurancy: 0.10071862643312465
Global Trainning Loss: 2.302068762779236
Global test accurancy: 0.10054309017451497
Global test_loss: 2.302330379486084
Global Precision: 0.020456866093656243
Global Recall: 0.10054309017451497
Global f1score: 0.020748849273053937
50
50
number of selected users 50
Global Trainning Accurancy: 0.10101252073248057
Global Trainning Loss: 2.302052617073059
Global test accurancy: 0.10059338163122007
Global test_loss: 2.30231999874115
Global Precision: 0.020620681672987223
Global Recall: 0.10059338163122007
Global f1score: 0.02120654655757283
50
50
number of selected users 50
Global Trainning Accurancy: 0.10132628839281033
Global Trainning Loss: 2.3020360469818115
Global test accurancy: 0.10079585222067701
Global test_loss: 2.302309398651123
Global Precision: 0.021708337294648657
Global Recall: 0.10079585222067701
Global f1score: 0.021827624605605406
50
50
number of selected users 50
Global Trainning Accurancy: 0.10177338658629845
Global Trainning Loss: 2.302019200325012
Global test accurancy: 0.10087595477465935
Global test_loss: 2.3022983503341674
Global Precision: 0.021567832779457573
Global Recall: 0.10087595477465935
Global f1score: 0.02242164808911171
50
50
number of selected users 50
Global Trainning Accurancy: 0.10201568423757679
Global Trainning Loss: 2.3020019578933715
Global test accurancy: 0.10104997220808123
Global test_loss: 2.302286801338196
Global Precision: 0.02251005522139009
Global Recall: 0.10104997220808123
Global f1score: 0.023287166014046953
50
50
number of selected users 50
Global Trainning Accurancy: 0.10211217074284423
Global Trainning Loss: 2.301984281539917
Global test accurancy: 0.10141949398657855
Global test_loss: 2.302274799346924
Global Precision: 0.022903894124414407
Global Recall: 0.10141949398657855
Global f1score: 0.024066981042647418
50
50
number of selected users 50
Global Trainning Accurancy: 0.1023028922848713
Global Trainning Loss: 2.3019661617279055
Global test accurancy: 0.10131836476504012
Global test_loss: 2.3022626399993897
Global Precision: 0.02263796024247843
Global Recall: 0.10131836476504012
Global f1score: 0.024471260412141947
50
50
number of selected users 50
Global Trainning Accurancy: 0.10260925007393029
Global Trainning Loss: 2.3019475746154785
Global test accurancy: 0.10156694961460011
Global test_loss: 2.302250304222107
Global Precision: 0.022461096626342183
Global Recall: 0.10156694961460011
Global f1score: 0.02498975075363722
50
50
number of selected users 50
Global Trainning Accurancy: 0.10254807200467062
Global Trainning Loss: 2.3019282674789427
Global test accurancy: 0.10229821671911347
Global test_loss: 2.302237687110901
Global Precision: 0.02371099778395558
Global Recall: 0.10229821671911347
Global f1score: 0.02654950739996406
50
50
number of selected users 50
Global Trainning Accurancy: 0.10273893941855962
Global Trainning Loss: 2.301908402442932
Global test accurancy: 0.10251113816166069
Global test_loss: 2.3022246408462523
Global Precision: 0.023516507884070916
Global Recall: 0.10251113816166069
Global f1score: 0.027263303087842222
50
50
number of selected users 50
Global Trainning Accurancy: 0.10285949474415404
Global Trainning Loss: 2.3018879795074465
Global test accurancy: 0.10284336414230387
Global test_loss: 2.3022110891342162
Global Precision: 0.023318597713030045
Global Recall: 0.10284336414230387
Global f1score: 0.02779694991332831
50
50
number of selected users 50
Global Trainning Accurancy: 0.10323537201656441
Global Trainning Loss: 2.301867208480835
Global test accurancy: 0.1029708240045028
Global test_loss: 2.302197370529175
Global Precision: 0.02276506293176777
Global Recall: 0.1029708240045028
Global f1score: 0.027977180631297106
50
50
number of selected users 50
Global Trainning Accurancy: 0.1035232957682728
Global Trainning Loss: 2.301845917701721
Global test accurancy: 0.10325728726748751
Global test_loss: 2.302183060646057
Global Precision: 0.022929071516448934
Global Recall: 0.10325728726748751
Global f1score: 0.028688995952289405
50
50
number of selected users 50
Global Trainning Accurancy: 0.10383795539185751
Global Trainning Loss: 2.3018241596221922
Global test accurancy: 0.10360476182547639
Global test_loss: 2.3021685791015627
Global Precision: 0.02323943275153202
Global Recall: 0.10360476182547639
Global f1score: 0.02958279849545441
50
50
number of selected users 50
Global Trainning Accurancy: 0.10387792801938922
Global Trainning Loss: 2.301801700592041
Global test accurancy: 0.10426933858032583
Global test_loss: 2.302153491973877
Global Precision: 0.023715433635335233
Global Recall: 0.10426933858032583
Global f1score: 0.0308007264021814
50
50
number of selected users 50
Global Trainning Accurancy: 0.10401223501410523
Global Trainning Loss: 2.30177830696106
Global test accurancy: 0.10419644948235487
Global test_loss: 2.302138066291809
Global Precision: 0.0236206751372317
Global Recall: 0.10419644948235487
Global f1score: 0.03125563225506915
50
50
number of selected users 50
Global Trainning Accurancy: 0.10416604232480278
Global Trainning Loss: 2.3017543745040894
Global test accurancy: 0.10474738379010014
Global test_loss: 2.3021220350265503
Global Precision: 0.029961592575108074
Global Recall: 0.10474738379010014
Global f1score: 0.032209030126713854
50
50
number of selected users 50
Global Trainning Accurancy: 0.10446929717386115
Global Trainning Loss: 2.3017298221588134
Global test accurancy: 0.10497978698036507
Global test_loss: 2.302105255126953
Global Precision: 0.029692562992748278
Global Recall: 0.10497978698036507
Global f1score: 0.032482131247950206
50
50
number of selected users 50
Global Trainning Accurancy: 0.10446140430017305
Global Trainning Loss: 2.30170458316803
Global test accurancy: 0.10500324359887662
Global test_loss: 2.302087969779968
Global Precision: 0.029405862668336245
Global Recall: 0.10500324359887662
Global f1score: 0.03268227691563587
50
50
number of selected users 50
Global Trainning Accurancy: 0.10483173227253251
Global Trainning Loss: 2.3016791486740114
Global test accurancy: 0.10505651073606806
Global test_loss: 2.3020706939697266
Global Precision: 0.029332397000991765
Global Recall: 0.10505651073606806
Global f1score: 0.03302234708779386
50
50
number of selected users 50
Global Trainning Accurancy: 0.10500989044800653
Global Trainning Loss: 2.3016529989242556
Global test accurancy: 0.10491319075712306
Global test_loss: 2.302052855491638
Global Precision: 0.0294247342923168
Global Recall: 0.10491319075712306
Global f1score: 0.03340663186984381
50
50
number of selected users 50
Global Trainning Accurancy: 0.10538772821422838
Global Trainning Loss: 2.3016263389587404
Global test accurancy: 0.10527075330707307
Global test_loss: 2.302034521102905
Global Precision: 0.030562305847989326
Global Recall: 0.10527075330707307
Global f1score: 0.034012138434163335
50
50
number of selected users 50
Global Trainning Accurancy: 0.10552215381893079
Global Trainning Loss: 2.3015990018844605
Global test accurancy: 0.10523168238435635
Global test_loss: 2.3020156478881835
Global Precision: 0.029165719400975607
Global Recall: 0.10523168238435635
Global f1score: 0.03396944249947639
50
50
number of selected users 50
Global Trainning Accurancy: 0.10549531598591709
Global Trainning Loss: 2.3015711212158205
Global test accurancy: 0.10548987275451711
Global test_loss: 2.3019965362548827
Global Precision: 0.02917934485091547
Global Recall: 0.10548987275451711
Global f1score: 0.03428537489001076
50
50
number of selected users 50
Global Trainning Accurancy: 0.10540745300938466
Global Trainning Loss: 2.3015427780151367
Global test accurancy: 0.10590684428055543
Global test_loss: 2.3019770622253417
Global Precision: 0.02921941348739189
Global Recall: 0.10590684428055543
Global f1score: 0.034599893421413765
50
50
number of selected users 50
Global Trainning Accurancy: 0.10521034134584413
Global Trainning Loss: 2.301513590812683
Global test accurancy: 0.10579821208795383
Global test_loss: 2.3019572353363036
Global Precision: 0.03129129258600239
Global Recall: 0.10579821208795383
Global f1score: 0.03491138548807433
50
50
number of selected users 50
Global Trainning Accurancy: 0.10551527970097309
Global Trainning Loss: 2.3014839458465577
Global test accurancy: 0.10559327649778885
Global test_loss: 2.3019370937347414
Global Precision: 0.032974990409800856
Global Recall: 0.10559327649778885
Global f1score: 0.03528125057383965
50
50
number of selected users 50
Global Trainning Accurancy: 0.10564816449197001
Global Trainning Loss: 2.301453356742859
Global test accurancy: 0.10502262894858388
Global test_loss: 2.301916551589966
Global Precision: 0.031923771389615004
Global Recall: 0.10502262894858388
Global f1score: 0.0352060960879356
50
50
number of selected users 50
Global Trainning Accurancy: 0.1058796792669815
Global Trainning Loss: 2.30142231464386
Global test accurancy: 0.10521508598940228
Global test_loss: 2.3018958377838135
Global Precision: 0.03532439517701507
Global Recall: 0.10521508598940228
Global f1score: 0.03574917544930063
50
50
number of selected users 50
Global Trainning Accurancy: 0.10579695519198698
Global Trainning Loss: 2.301390447616577
Global test accurancy: 0.10569775380093971
Global test_loss: 2.3018746185302734
Global Precision: 0.032525487198201836
Global Recall: 0.10569775380093971
Global f1score: 0.035984491813003845
50
50
number of selected users 50
Global Trainning Accurancy: 0.10617794117814588
Global Trainning Loss: 2.3013579130172728
Global test accurancy: 0.10553200484873393
Global test_loss: 2.3018530654907225
Global Precision: 0.03243290391351623
Global Recall: 0.10553200484873393
Global f1score: 0.03602153543771137
50
50
number of selected users 50
Global Trainning Accurancy: 0.10658270505134156
Global Trainning Loss: 2.3013246726989744
Global test accurancy: 0.10573271369769119
Global test_loss: 2.301831307411194
Global Precision: 0.03329383795197629
Global Recall: 0.10573271369769119
Global f1score: 0.036372299450199305
50
50
number of selected users 50
Global Trainning Accurancy: 0.10641793843240498
Global Trainning Loss: 2.3012905406951902
Global test accurancy: 0.10534081283904997
Global test_loss: 2.3018090295791627
Global Precision: 0.03456020180635541
Global Recall: 0.10534081283904997
Global f1score: 0.03674150914635721
50
50
number of selected users 50
Global Trainning Accurancy: 0.10659665023650733
Global Trainning Loss: 2.3012553739547728
Global test accurancy: 0.10560534102563356
Global test_loss: 2.3017862033843994
Global Precision: 0.0348820494735486
Global Recall: 0.10560534102563356
Global f1score: 0.03731181026619683
50
50
number of selected users 50
Global Trainning Accurancy: 0.10670524245008327
Global Trainning Loss: 2.30121931552887
Global test accurancy: 0.10544165336561397
Global test_loss: 2.3017629194259643
Global Precision: 0.03513047185590668
Global Recall: 0.10544165336561397
Global f1score: 0.037487261051249034
50
50
number of selected users 50
Global Trainning Accurancy: 0.10720009507919769
Global Trainning Loss: 2.301182174682617
Global test accurancy: 0.10553731231507195
Global test_loss: 2.3017390251159666
Global Precision: 0.04036638407410159
Global Recall: 0.10553731231507195
Global f1score: 0.03836492811616688
50
50
number of selected users 50
Global Trainning Accurancy: 0.1074016814775821
Global Trainning Loss: 2.301144208908081
Global test accurancy: 0.10567130973088529
Global test_loss: 2.301714496612549
Global Precision: 0.0406302801875154
Global Recall: 0.10567130973088529
Global f1score: 0.038873074426679706
50
50
number of selected users 50
Global Trainning Accurancy: 0.10765252215958039
Global Trainning Loss: 2.301105194091797
Global test accurancy: 0.10617432120952958
Global test_loss: 2.301689224243164
Global Precision: 0.04271359921436946
Global Recall: 0.10617432120952958
Global f1score: 0.03966694681616353
50
50
number of selected users 50
Global Trainning Accurancy: 0.10796408514297315
Global Trainning Loss: 2.3010653686523437
Global test accurancy: 0.10615465702762503
Global test_loss: 2.301663222312927
Global Precision: 0.04446840473460086
Global Recall: 0.10615465702762503
Global f1score: 0.040009763966832194
50
50
number of selected users 50
Global Trainning Accurancy: 0.10825132496488729
Global Trainning Loss: 2.3010244846343992
Global test accurancy: 0.10624044650321791
Global test_loss: 2.301636610031128
Global Precision: 0.04496084264183765
Global Recall: 0.10624044650321791
Global f1score: 0.040775702239595105
50
50
number of selected users 50
Global Trainning Accurancy: 0.10856401513371064
Global Trainning Loss: 2.300982518196106
Global test accurancy: 0.10606103407686175
Global test_loss: 2.3016091871261595
Global Precision: 0.04965775764136758
Global Recall: 0.10606103407686175
Global f1score: 0.04160535445870332
50
50
number of selected users 50
Global Trainning Accurancy: 0.10868977959085885
Global Trainning Loss: 2.300939636230469
Global test accurancy: 0.10603874230607618
Global test_loss: 2.301581506729126
Global Precision: 0.04938551714320265
Global Recall: 0.10603874230607618
Global f1score: 0.042184889572365165
50
50
number of selected users 50
Global Trainning Accurancy: 0.10905990123478054
Global Trainning Loss: 2.3008957433700563
Global test accurancy: 0.10695524445815552
Global test_loss: 2.3015529489517212
Global Precision: 0.05215534446660371
Global Recall: 0.10695524445815552
Global f1score: 0.04385782181063456
50
50
number of selected users 50
Global Trainning Accurancy: 0.10970214637837732
Global Trainning Loss: 2.3008509254455567
Global test accurancy: 0.10749839305913933
Global test_loss: 2.3015236902236937
Global Precision: 0.052955175953683986
Global Recall: 0.10749839305913933
Global f1score: 0.04482196396326304
50
50
number of selected users 50
Global Trainning Accurancy: 0.11014832917161137
Global Trainning Loss: 2.300804924964905
Global test accurancy: 0.10823142511712579
Global test_loss: 2.3014938640594482
Global Precision: 0.05210519237900302
Global Recall: 0.10823142511712579
Global f1score: 0.046288347441981854
50
50
number of selected users 50
Global Trainning Accurancy: 0.11079829844828992
Global Trainning Loss: 2.3007575988769533
Global test accurancy: 0.10816093105888769
Global test_loss: 2.3014629459381104
Global Precision: 0.0525964914206829
Global Recall: 0.10816093105888769
Global f1score: 0.0471785077528979
50
50
number of selected users 50
Global Trainning Accurancy: 0.11062798604369752
Global Trainning Loss: 2.3007087087631226
Global test accurancy: 0.10837345989999957
Global test_loss: 2.30143214225769
Global Precision: 0.05195843133278636
Global Recall: 0.10837345989999957
Global f1score: 0.04807452858152467
50
50
number of selected users 50
Global Trainning Accurancy: 0.11143350599979569
Global Trainning Loss: 2.3006586408615113
Global test accurancy: 0.10888445659749865
Global test_loss: 2.3014008378982544
Global Precision: 0.05075324094565542
Global Recall: 0.10888445659749865
Global f1score: 0.049101721834972946
50
50
number of selected users 50
Global Trainning Accurancy: 0.11184880758932939
Global Trainning Loss: 2.3006080198287964
Global test accurancy: 0.10834769653717276
Global test_loss: 2.3013689136505127
Global Precision: 0.04837900282035865
Global Recall: 0.10834769653717276
Global f1score: 0.0490788417702769
50
50
number of selected users 50
Global Trainning Accurancy: 0.112696427236299
Global Trainning Loss: 2.3005564403533936
Global test accurancy: 0.1086164821875537
Global test_loss: 2.301336574554443
Global Precision: 0.05083048265663285
Global Recall: 0.1086164821875537
Global f1score: 0.050367837221633995
50
50
number of selected users 50
Global Trainning Accurancy: 0.11288471138954252
Global Trainning Loss: 2.3005037260055543
Global test accurancy: 0.10825542201950178
Global test_loss: 2.3013039016723633
Global Precision: 0.05038379859544236
Global Recall: 0.10825542201950178
Global f1score: 0.05090492058371191
50
50
number of selected users 50
Global Trainning Accurancy: 0.11334149240950182
Global Trainning Loss: 2.300450382232666
Global test accurancy: 0.10824839273429927
Global test_loss: 2.301270718574524
Global Precision: 0.05015838092624037
Global Recall: 0.10824839273429927
Global f1score: 0.05175965814378826
50
50
number of selected users 50
Global Trainning Accurancy: 0.11387092561201109
Global Trainning Loss: 2.3003957939147948
Global test accurancy: 0.10895024317735141
Global test_loss: 2.3012368297576904
Global Precision: 0.05043665516367272
Global Recall: 0.10895024317735141
Global f1score: 0.05296867680920116
50
50
number of selected users 50
Global Trainning Accurancy: 0.11403041480908496
Global Trainning Loss: 2.30034038066864
Global test accurancy: 0.10897644179524597
Global test_loss: 2.301202712059021
Global Precision: 0.05018831590635783
Global Recall: 0.10897644179524597
Global f1score: 0.05383665071003257
50
50
number of selected users 50
Global Trainning Accurancy: 0.11457780359727665
Global Trainning Loss: 2.300283660888672
Global test accurancy: 0.10966157473638637
Global test_loss: 2.301167826652527
Global Precision: 0.05050924271714367
Global Recall: 0.10966157473638637
Global f1score: 0.054966271533714584
50
50
number of selected users 50
Global Trainning Accurancy: 0.11473965410204826
Global Trainning Loss: 2.3002261304855347
Global test accurancy: 0.10945525501268137
Global test_loss: 2.301132850646973
Global Precision: 0.05040948912533725
Global Recall: 0.10945525501268137
Global f1score: 0.0554176361848571
50
50
number of selected users 50
Global Trainning Accurancy: 0.11445764474937785
Global Trainning Loss: 2.300168261528015
Global test accurancy: 0.10960502243563601
Global test_loss: 2.301097421646118
Global Precision: 0.0502356377031449
Global Recall: 0.10960502243563601
Global f1score: 0.05597648083187133
50
50
number of selected users 50
Global Trainning Accurancy: 0.1145121243366566
Global Trainning Loss: 2.3001085329055786
Global test accurancy: 0.1096911634007947
Global test_loss: 2.301061010360718
Global Precision: 0.049669528653435456
Global Recall: 0.1096911634007947
Global f1score: 0.05624088193202165
50
50
number of selected users 50
Global Trainning Accurancy: 0.11508121979481134
Global Trainning Loss: 2.300047154426575
Global test accurancy: 0.10956448160105944
Global test_loss: 2.3010236501693724
Global Precision: 0.04924248989449852
Global Recall: 0.10956448160105944
Global f1score: 0.05653475204478811
50
50
number of selected users 50
Global Trainning Accurancy: 0.11527727712947758
Global Trainning Loss: 2.2999845218658446
Global test accurancy: 0.10971960288219965
Global test_loss: 2.30098726272583
Global Precision: 0.050524481103170676
Global Recall: 0.10971960288219965
Global f1score: 0.057096356265514424
50
50
number of selected users 50
Global Trainning Accurancy: 0.11522529184875592
Global Trainning Loss: 2.2999214792251585
Global test accurancy: 0.11035491929302552
Global test_loss: 2.3009505224227906
Global Precision: 0.05030888133599783
Global Recall: 0.11035491929302552
Global f1score: 0.05803406914906967
50
50
number of selected users 50
Global Trainning Accurancy: 0.11552491600948592
Global Trainning Loss: 2.2998575496673586
Global test accurancy: 0.11112164296691329
Global test_loss: 2.3009121084213255
Global Precision: 0.05035012760367437
Global Recall: 0.11112164296691329
Global f1score: 0.05915837493972245
50
50
number of selected users 50
Global Trainning Accurancy: 0.11556577096814252
Global Trainning Loss: 2.2997936105728147
Global test accurancy: 0.11061485426175809
Global test_loss: 2.300873346328735
Global Precision: 0.05485187544272049
Global Recall: 0.11061485426175809
Global f1score: 0.05960818031221184
50
50
number of selected users 50
Global Trainning Accurancy: 0.11558547093076817
Global Trainning Loss: 2.299728217124939
Global test accurancy: 0.1110349044087058
Global test_loss: 2.300833435058594
Global Precision: 0.058321060699835975
Global Recall: 0.1110349044087058
Global f1score: 0.06054216977059482
50
50
number of selected users 50
Global Trainning Accurancy: 0.11580867179568849
Global Trainning Loss: 2.2996617555618286
Global test accurancy: 0.11126862104406281
Global test_loss: 2.300792751312256
Global Precision: 0.057894060969117554
Global Recall: 0.11126862104406281
Global f1score: 0.061116879757679855
50
50
number of selected users 50
Global Trainning Accurancy: 0.11578518972527156
Global Trainning Loss: 2.299594841003418
Global test accurancy: 0.11181675945833726
Global test_loss: 2.3007523584365845
Global Precision: 0.057957532693610946
Global Recall: 0.11181675945833726
Global f1score: 0.06169609616147193
50
50
number of selected users 50
Global Trainning Accurancy: 0.11608471004561205
Global Trainning Loss: 2.299528470039368
Global test accurancy: 0.11221769640643459
Global test_loss: 2.300713143348694
Global Precision: 0.05944484153680656
Global Recall: 0.11221769640643459
Global f1score: 0.06253760461675628
50
50
number of selected users 50
Global Trainning Accurancy: 0.11636500000891502
Global Trainning Loss: 2.2994597721099854
Global test accurancy: 0.11220434842673821
Global test_loss: 2.30067262172699
Global Precision: 0.06185166946643544
Global Recall: 0.11220434842673821
Global f1score: 0.063267888156963
50
50
number of selected users 50
Global Trainning Accurancy: 0.11728984498311347
Global Trainning Loss: 2.2993891954422
Global test accurancy: 0.1123213296642876
Global test_loss: 2.3006315660476684
Global Precision: 0.06457396447389799
Global Recall: 0.1123213296642876
Global f1score: 0.0638081659278283
50
50
number of selected users 50
Global Trainning Accurancy: 0.11773084705261672
Global Trainning Loss: 2.299317092895508
Global test accurancy: 0.1123474557230493
Global test_loss: 2.300590543746948
Global Precision: 0.06711035149490052
Global Recall: 0.1123474557230493
Global f1score: 0.06451158913627383
50
50
number of selected users 50
Global Trainning Accurancy: 0.11800710797548537
Global Trainning Loss: 2.2992434549331664
Global test accurancy: 0.11296394618091947
Global test_loss: 2.300548610687256
Global Precision: 0.06930741275669541
Global Recall: 0.11296394618091947
Global f1score: 0.06551456882559334
50
50
number of selected users 50
Global Trainning Accurancy: 0.11818688282534402
Global Trainning Loss: 2.2991699504852297
Global test accurancy: 0.11316817341279245
Global test_loss: 2.3005059909820558
Global Precision: 0.07441716246934628
Global Recall: 0.11316817341279245
Global f1score: 0.06643020896130991
50
50
number of selected users 50
Global Trainning Accurancy: 0.1184909668721136
Global Trainning Loss: 2.2990964889526366
Global test accurancy: 0.11298370575919098
Global test_loss: 2.300463914871216
Global Precision: 0.07607960493828773
Global Recall: 0.11298370575919098
Global f1score: 0.06679992319611013
50
50
number of selected users 50
Global Trainning Accurancy: 0.11874761134132669
Global Trainning Loss: 2.299020128250122
Global test accurancy: 0.11373870868651079
Global test_loss: 2.3004195928573608
Global Precision: 0.0748421566188945
Global Recall: 0.11373870868651079
Global f1score: 0.06779849345237911
50
50
number of selected users 50
Global Trainning Accurancy: 0.11914223809859674
Global Trainning Loss: 2.29894455909729
Global test accurancy: 0.11394935370775702
Global test_loss: 2.3003747177124025
Global Precision: 0.07624232579719258
Global Recall: 0.11394935370775702
Global f1score: 0.06864961974180292
50
50
number of selected users 50
Global Trainning Accurancy: 0.11965063336899566
Global Trainning Loss: 2.2988695192337034
Global test accurancy: 0.11383850037038656
Global test_loss: 2.300330681800842
Global Precision: 0.07869548835934231
Global Recall: 0.11383850037038656
Global f1score: 0.06954606008526876
50
50
number of selected users 50
Global Trainning Accurancy: 0.11995265645968164
Global Trainning Loss: 2.2987945747375487
Global test accurancy: 0.11346103071594238
Global test_loss: 2.3002873134613036
Global Precision: 0.07813751002103278
Global Recall: 0.11346103071594238
Global f1score: 0.06986558408155767
50
50
number of selected users 50
Global Trainning Accurancy: 0.12039295325982277
Global Trainning Loss: 2.2987188386917112
Global test accurancy: 0.1124929430652484
Global test_loss: 2.3002437114715577
Global Precision: 0.07889300891510351
Global Recall: 0.1124929430652484
Global f1score: 0.07006326930576187
50
50
number of selected users 50
Global Trainning Accurancy: 0.12035933522834298
Global Trainning Loss: 2.2986436653137208
Global test accurancy: 0.112562444866609
Global test_loss: 2.300202465057373
Global Precision: 0.08156581690806934
Global Recall: 0.112562444866609
Global f1score: 0.07114219738302936
50
50
number of selected users 50
Global Trainning Accurancy: 0.12082574439606626
Global Trainning Loss: 2.298569359779358
Global test accurancy: 0.1120502521416456
Global test_loss: 2.3001619482040407
Global Precision: 0.08317519019150645
Global Recall: 0.1120502521416456
Global f1score: 0.07158649054591662
50
50
number of selected users 50
Global Trainning Accurancy: 0.1209441830254636
Global Trainning Loss: 2.298495202064514
Global test accurancy: 0.11297425830450551
Global test_loss: 2.3001194286346434
Global Precision: 0.08596235351978972
Global Recall: 0.11297425830450551
Global f1score: 0.07321108928913658
50
50
number of selected users 50
Global Trainning Accurancy: 0.12127402811655474
Global Trainning Loss: 2.29842173576355
Global test accurancy: 0.11313621525401851
Global test_loss: 2.3000780868530275
Global Precision: 0.08678891910096652
Global Recall: 0.11313621525401851
Global f1score: 0.07410686671370767
50
50
number of selected users 50
Global Trainning Accurancy: 0.12167502591733811
Global Trainning Loss: 2.298349027633667
Global test accurancy: 0.11357390092507266
Global test_loss: 2.3000371932983397
Global Precision: 0.08636443964318918
Global Recall: 0.11357390092507266
Global f1score: 0.07523093403156803
50
50
number of selected users 50
Global Trainning Accurancy: 0.12148818459117044
Global Trainning Loss: 2.298276128768921
Global test accurancy: 0.11458959694059546
Global test_loss: 2.299995098114014
Global Precision: 0.08858339922258691
Global Recall: 0.11458959694059546
Global f1score: 0.07697697737069739
50
50
number of selected users 50
Global Trainning Accurancy: 0.12177941479022428
Global Trainning Loss: 2.2982035875320435
Global test accurancy: 0.11434272743387172
Global test_loss: 2.2999531650543212
Global Precision: 0.08625678991120188
Global Recall: 0.11434272743387172
Global f1score: 0.0773228045652626
50
50
number of selected users 50
Global Trainning Accurancy: 0.12183802385048041
Global Trainning Loss: 2.2981314849853516
Global test accurancy: 0.11458411116619749
Global test_loss: 2.2999127292633057
Global Precision: 0.08869652194594903
Global Recall: 0.11458411116619749
Global f1score: 0.07839591386242954
50
50
number of selected users 50
Global Trainning Accurancy: 0.1220857073464879
Global Trainning Loss: 2.2980598735809328
Global test accurancy: 0.11528836970519904
Global test_loss: 2.2998734283447266
Global Precision: 0.08914079166958644
Global Recall: 0.11528836970519904
Global f1score: 0.07952133218237971
50
50
number of selected users 50
Global Trainning Accurancy: 0.1222615857250043
Global Trainning Loss: 2.297988533973694
Global test accurancy: 0.11528493024808419
Global test_loss: 2.2998354482650756
Global Precision: 0.08986495176339922
Global Recall: 0.11528493024808419
Global f1score: 0.07990723897920196
50
50
number of selected users 50
Global Trainning Accurancy: 0.12274630564131701
Global Trainning Loss: 2.2979174995422365
Global test accurancy: 0.11504403666055091
Global test_loss: 2.299797921180725
Global Precision: 0.09039562433792536
Global Recall: 0.11504403666055091
Global f1score: 0.08061606784037663
50
50
number of selected users 50
Global Trainning Accurancy: 0.12323972980121031
Global Trainning Loss: 2.297847924232483
Global test accurancy: 0.11519622171859077
Global test_loss: 2.2997622966766356
Global Precision: 0.09286522244857046
Global Recall: 0.11519622171859077
Global f1score: 0.08150906177452939
50
50
number of selected users 50
Global Trainning Accurancy: 0.12338058249284194
Global Trainning Loss: 2.2977785730361937
Global test accurancy: 0.11713555386647963
Global test_loss: 2.2997275590896606
Global Precision: 0.10096342467381861
Global Recall: 0.11713555386647963
Global f1score: 0.08422916650391059
50
50
number of selected users 50
Global Trainning Accurancy: 0.12344808078393027
Global Trainning Loss: 2.2977095651626587
Global test accurancy: 0.11775751204036812
Global test_loss: 2.2996947145462037
Global Precision: 0.10086293556444624
Global Recall: 0.11775751204036812
Global f1score: 0.08549634257883038
50
50
number of selected users 50
Global Trainning Accurancy: 0.12329018899973901
Global Trainning Loss: 2.2976421308517456
Global test accurancy: 0.11863991151128013
Global test_loss: 2.299664545059204
Global Precision: 0.10032238757729335
Global Recall: 0.11863991151128013
Global f1score: 0.08669247755794153
50
50
number of selected users 50
Global Trainning Accurancy: 0.1232850679233429
Global Trainning Loss: 2.297575902938843
Global test accurancy: 0.11821982657096682
Global test_loss: 2.299636263847351
Global Precision: 0.09947810054857821
Global Recall: 0.11821982657096682
Global f1score: 0.08660120031840794
50
50
number of selected users 50
Global Trainning Accurancy: 0.12399804122773157
Global Trainning Loss: 2.297508563995361
Global test accurancy: 0.11766099593810339
Global test_loss: 2.2996073627471922
Global Precision: 0.09820891398219556
Global Recall: 0.11766099593810339
Global f1score: 0.0866470329502571
50
50
number of selected users 50
Global Trainning Accurancy: 0.12401246188949763
Global Trainning Loss: 2.2974417400360108
Global test accurancy: 0.11824864699935905
Global test_loss: 2.299581165313721
Global Precision: 0.09873737317072472
Global Recall: 0.11824864699935905
Global f1score: 0.08749153995059915
50
50
number of selected users 50
Global Trainning Accurancy: 0.12426659531442842
Global Trainning Loss: 2.2973759031295775
Global test accurancy: 0.11823721212043801
Global test_loss: 2.299555549621582
Global Precision: 0.10146021925519176
Global Recall: 0.11823721212043801
Global f1score: 0.0882289452212839
50
50
number of selected users 50
Global Trainning Accurancy: 0.12450730992689034
Global Trainning Loss: 2.2973095607757568
Global test accurancy: 0.11855783977185815
Global test_loss: 2.2995315551757813
Global Precision: 0.10414031350793795
Global Recall: 0.11855783977185815
Global f1score: 0.08925084217131367
50
50
number of selected users 50
Global Trainning Accurancy: 0.12453052691569463
Global Trainning Loss: 2.2972426557540895
Global test accurancy: 0.11882186089567179
Global test_loss: 2.2995087242126466
Global Precision: 0.10363866601903066
Global Recall: 0.11882186089567179
Global f1score: 0.08971708493409052
50
50
number of selected users 50
Global Trainning Accurancy: 0.1247864829200176
Global Trainning Loss: 2.2971746492385865
Global test accurancy: 0.1184181464988412
Global test_loss: 2.2994877386093138
Global Precision: 0.10344022724046005
Global Recall: 0.1184181464988412
Global f1score: 0.08972970725891645
50
50
number of selected users 50
Global Trainning Accurancy: 0.12479265145704527
Global Trainning Loss: 2.2971050214767454
Global test accurancy: 0.11838744726417393
Global test_loss: 2.299470109939575
Global Precision: 0.10502131515751277
Global Recall: 0.11838744726417393
Global f1score: 0.09036732212458364
50
50
number of selected users 50
Global Trainning Accurancy: 0.12481314743884502
Global Trainning Loss: 2.2970402145385744
Global test accurancy: 0.11841581679061726
Global test_loss: 2.2994554805755616
Global Precision: 0.10624097492717156
Global Recall: 0.11841581679061726
Global f1score: 0.0911003594144303
50
50
number of selected users 50
Global Trainning Accurancy: 0.12501734546523371
Global Trainning Loss: 2.2969760274887085
Global test accurancy: 0.11775159032134758
Global test_loss: 2.299442090988159
Global Precision: 0.10421383553601825
Global Recall: 0.11775159032134758
Global f1score: 0.09087736666883127
50
50
number of selected users 50
Global Trainning Accurancy: 0.12493685553927379
Global Trainning Loss: 2.2969133138656614
Global test accurancy: 0.11815515341583503
Global test_loss: 2.2994301748275756
Global Precision: 0.10481474887414717
Global Recall: 0.11815515341583503
Global f1score: 0.0915685973370064
50
50
number of selected users 50
Global Trainning Accurancy: 0.12524216534845187
Global Trainning Loss: 2.2968504810333252
Global test accurancy: 0.11828891562952523
Global test_loss: 2.299419097900391
Global Precision: 0.11150238661208373
Global Recall: 0.11828891562952523
Global f1score: 0.09257257380395634
50
50
number of selected users 50
Global Trainning Accurancy: 0.12540154289145936
Global Trainning Loss: 2.2967861700057983
Global test accurancy: 0.11828929679184716
Global test_loss: 2.2994070863723755
Global Precision: 0.11543677003990925
Global Recall: 0.11828929679184716
Global f1score: 0.09336704309226374
50
50
number of selected users 50
Global Trainning Accurancy: 0.12546306509629687
Global Trainning Loss: 2.2967218589782714
Global test accurancy: 0.11778627387906344
Global test_loss: 2.2993955993652344
Global Precision: 0.1140546333353826
Global Recall: 0.11778627387906344
Global f1score: 0.09314657016679011
50
50
number of selected users 50
Global Trainning Accurancy: 0.12561344481352943
Global Trainning Loss: 2.2966593360900878
Global test accurancy: 0.11794410555444737
Global test_loss: 2.2993879461288453
Global Precision: 0.11438673170339898
Global Recall: 0.11794410555444737
Global f1score: 0.09358050368409358
50
50
number of selected users 50
Global Trainning Accurancy: 0.12589948044068577
Global Trainning Loss: 2.296594834327698
Global test accurancy: 0.11753755043976896
Global test_loss: 2.299379768371582
Global Precision: 0.1129723198021709
Global Recall: 0.11753755043976896
Global f1score: 0.0936024437063355
50
50
number of selected users 50
Global Trainning Accurancy: 0.12594974440255294
Global Trainning Loss: 2.2965299606323244
Global test accurancy: 0.11784264264214284
Global test_loss: 2.299373106956482
Global Precision: 0.11565101340594375
Global Recall: 0.11784264264214284
Global f1score: 0.09457886947750319
50
50
number of selected users 50
Global Trainning Accurancy: 0.12610044886147256
Global Trainning Loss: 2.2964661741256713
Global test accurancy: 0.11769261944631529
Global test_loss: 2.2993707704544066
Global Precision: 0.11514034051689086
Global Recall: 0.11769261944631529
Global f1score: 0.09443887047079569
50
50
number of selected users 50
Global Trainning Accurancy: 0.1263021537832615
Global Trainning Loss: 2.2964010524749754
Global test accurancy: 0.11814615199893805
Global test_loss: 2.2993701267242432
Global Precision: 0.11394873342826475
Global Recall: 0.11814615199893805
Global f1score: 0.09519618979018266
50
50
number of selected users 50
Global Trainning Accurancy: 0.12655887991119674
Global Trainning Loss: 2.2963338804244997
Global test accurancy: 0.11831825352262396
Global test_loss: 2.299367570877075
Global Precision: 0.11464940919630966
Global Recall: 0.11831825352262396
Global f1score: 0.09552323049642406
50
50
number of selected users 50
Global Trainning Accurancy: 0.1266273495088341
Global Trainning Loss: 2.296268277168274
Global test accurancy: 0.117870755165608
Global test_loss: 2.2993682432174682
Global Precision: 0.11540459964653095
Global Recall: 0.117870755165608
Global f1score: 0.09512102789872323
50
50
number of selected users 50
Global Trainning Accurancy: 0.126747456195355
Global Trainning Loss: 2.296201229095459
Global test accurancy: 0.11815121290908831
Global test_loss: 2.2993703269958496
Global Precision: 0.1151827444963525
Global Recall: 0.11815121290908831
Global f1score: 0.09566223633641062
50
50
number of selected users 50
Global Trainning Accurancy: 0.12657795413326658
Global Trainning Loss: 2.2961347484588623
Global test accurancy: 0.11785656005978169
Global test_loss: 2.299375014305115
Global Precision: 0.1145789895386139
Global Recall: 0.11785656005978169
Global f1score: 0.09556104862166087
50
50
number of selected users 50
Global Trainning Accurancy: 0.12673881709005375
Global Trainning Loss: 2.2960701751708985
Global test accurancy: 0.118431983630521
Global test_loss: 2.2993838500976564
Global Precision: 0.12051062889078422
Global Recall: 0.118431983630521
Global f1score: 0.09677508437243056
50
50
number of selected users 50
Global Trainning Accurancy: 0.126806582984474
Global Trainning Loss: 2.296002163887024
Global test accurancy: 0.11880581388747882
Global test_loss: 2.2993923091888426
Global Precision: 0.11947994371499772
Global Recall: 0.11880581388747882
Global f1score: 0.09723454405946215
50
50
number of selected users 50
Global Trainning Accurancy: 0.12692020584609318
Global Trainning Loss: 2.295935273170471
Global test accurancy: 0.11899398006831279
Global test_loss: 2.2994023752212525
Global Precision: 0.1191720325670351
Global Recall: 0.11899398006831279
Global f1score: 0.09753538096783762
50
50
number of selected users 50
Global Trainning Accurancy: 0.12716829097861201
Global Trainning Loss: 2.2958623600006103
Global test accurancy: 0.11904665576159956
Global test_loss: 2.299407892227173
Global Precision: 0.11797391290372287
Global Recall: 0.11904665576159956
Global f1score: 0.09788066346636949
50
50
number of selected users 50
Global Trainning Accurancy: 0.12731645837482342
Global Trainning Loss: 2.2957903814315794
Global test accurancy: 0.11975898418309838
Global test_loss: 2.2994158554077146
Global Precision: 0.11704137821583858
Global Recall: 0.11975898418309838
Global f1score: 0.0987338381733645
50
50
number of selected users 50
Global Trainning Accurancy: 0.12709181377314424
Global Trainning Loss: 2.2957177209854125
Global test accurancy: 0.12050040400299943
Global test_loss: 2.2994286108016966
Global Precision: 0.11629278444747011
Global Recall: 0.12050040400299943
Global f1score: 0.09965136091624079
50
50
number of selected users 50
Global Trainning Accurancy: 0.12728359200037542
Global Trainning Loss: 2.295644998550415
Global test accurancy: 0.12064452096593473
Global test_loss: 2.2994437837600707
Global Precision: 0.11707225659507746
Global Recall: 0.12064452096593473
Global f1score: 0.10002938498720673
50
50
number of selected users 50
Global Trainning Accurancy: 0.1270927057283251
Global Trainning Loss: 2.2955742359161375
Global test accurancy: 0.12068664664563457
Global test_loss: 2.299463973045349
Global Precision: 0.11786289687262748
Global Recall: 0.12068664664563457
Global f1score: 0.10033585813774454
50
50
number of selected users 50
Global Trainning Accurancy: 0.12699512221517087
Global Trainning Loss: 2.295502042770386
Global test accurancy: 0.12031966624925133
Global test_loss: 2.299483494758606
Global Precision: 0.11834104638363212
Global Recall: 0.12031966624925133
Global f1score: 0.10000232034398729
50
50
number of selected users 50
Global Trainning Accurancy: 0.12715829489768973
Global Trainning Loss: 2.2954290533065795
Global test accurancy: 0.1202489202836289
Global test_loss: 2.299505877494812
Global Precision: 0.12124246638307555
Global Recall: 0.1202489202836289
Global f1score: 0.10020072143610573
50
50
number of selected users 50
Global Trainning Accurancy: 0.12728512909172346
Global Trainning Loss: 2.295354504585266
Global test accurancy: 0.1201614625746534
Global test_loss: 2.2995270681381226
Global Precision: 0.12170574433642672
Global Recall: 0.1201614625746534
Global f1score: 0.10037405529088822
50
50
number of selected users 50
Global Trainning Accurancy: 0.12749812996476112
Global Trainning Loss: 2.295282163619995
Global test accurancy: 0.1199649383672516
Global test_loss: 2.2995526456832884
Global Precision: 0.12030277338661506
Global Recall: 0.1199649383672516
Global f1score: 0.10042550434614636
50
50
number of selected users 50
Global Trainning Accurancy: 0.12739397492962012
Global Trainning Loss: 2.295210237503052
Global test accurancy: 0.1195645015743308
Global test_loss: 2.299582781791687
Global Precision: 0.117944201648808
Global Recall: 0.1195645015743308
Global f1score: 0.10015144694236644
50
50
number of selected users 50
Global Trainning Accurancy: 0.1274340017562567
Global Trainning Loss: 2.2951361751556396
Global test accurancy: 0.12054640991084893
Global test_loss: 2.2996096324920656
Global Precision: 0.11968070494353228
Global Recall: 0.12054640991084893
Global f1score: 0.10125947720873427
50
50
number of selected users 50
Global Trainning Accurancy: 0.1272998255388596
Global Trainning Loss: 2.2950587129592894
Global test accurancy: 0.12037417895792069
Global test_loss: 2.2996376895904542
Global Precision: 0.11829635421332248
Global Recall: 0.12037417895792069
Global f1score: 0.10113374752602916
50
50
number of selected users 50
Global Trainning Accurancy: 0.12715713019639316
Global Trainning Loss: 2.294981198310852
Global test accurancy: 0.12074079361559246
Global test_loss: 2.2996653747558593
Global Precision: 0.12217458152563414
Global Recall: 0.12074079361559246
Global f1score: 0.10208835913469419
50
50
number of selected users 50
Global Trainning Accurancy: 0.12739478952032418
Global Trainning Loss: 2.294903221130371
Global test accurancy: 0.11968368207287998
Global test_loss: 2.2996951961517333
Global Precision: 0.12148339528917841
Global Recall: 0.11968368207287998
Global f1score: 0.10130473450277099
50
50
number of selected users 50
Global Trainning Accurancy: 0.12752542071619813
Global Trainning Loss: 2.29482355594635
Global test accurancy: 0.11965151335924278
Global test_loss: 2.299727144241333
Global Precision: 0.1226445493256646
Global Recall: 0.11965151335924278
Global f1score: 0.10138995040330673
50
50
number of selected users 50
Global Trainning Accurancy: 0.12757772490556601
Global Trainning Loss: 2.2947464752197266
Global test accurancy: 0.11974654134372216
Global test_loss: 2.2997610425949095
Global Precision: 0.12298633074557905
Global Recall: 0.11974654134372216
Global f1score: 0.10185412892634979
50
50
number of selected users 50
Global Trainning Accurancy: 0.127575369083421
Global Trainning Loss: 2.2946667289733886
Global test accurancy: 0.11956719402644442
Global test_loss: 2.299797525405884
Global Precision: 0.12269889080074783
Global Recall: 0.11956719402644442
Global f1score: 0.10189244598264476
50
50
number of selected users 50
Global Trainning Accurancy: 0.1277393550644536
Global Trainning Loss: 2.2945826292037963
Global test accurancy: 0.11928540918539532
Global test_loss: 2.29983389377594
Global Precision: 0.12339530047836472
Global Recall: 0.11928540918539532
Global f1score: 0.10196150932302833
50
50
number of selected users 50
Global Trainning Accurancy: 0.12766612988185594
Global Trainning Loss: 2.294499945640564
Global test accurancy: 0.11968528183211195
Global test_loss: 2.299873628616333
Global Precision: 0.12019775237394738
Global Recall: 0.11968528183211195
Global f1score: 0.10242404838301919
50
50
number of selected users 50
Global Trainning Accurancy: 0.12797344864693935
Global Trainning Loss: 2.2944137048721314
Global test accurancy: 0.12066877275709965
Global test_loss: 2.2999128770828245
Global Precision: 0.12152515007200693
Global Recall: 0.12066877275709965
Global f1score: 0.10353921117266217
50
50
number of selected users 50
Global Trainning Accurancy: 0.1277986937807065
Global Trainning Loss: 2.2943289613723756
Global test accurancy: 0.12039373895304444
Global test_loss: 2.2999522256851197
Global Precision: 0.12086329362770094
Global Recall: 0.12039373895304444
Global f1score: 0.10331836971239651
50
50
number of selected users 50
Global Trainning Accurancy: 0.12755896485799373
Global Trainning Loss: 2.2942350912094116
Global test accurancy: 0.12089329218523016
Global test_loss: 2.299987678527832
Global Precision: 0.12157746589185035
Global Recall: 0.12089329218523016
Global f1score: 0.1040121416141649
50
50
number of selected users 50
Global Trainning Accurancy: 0.1276227781049123
Global Trainning Loss: 2.2941439247131346
Global test accurancy: 0.12123406197719358
Global test_loss: 2.300032229423523
Global Precision: 0.12146237082728964
Global Recall: 0.12123406197719358
Global f1score: 0.10427000677970302
50
50
number of selected users 50
Global Trainning Accurancy: 0.127536403657067
Global Trainning Loss: 2.294049611091614
Global test accurancy: 0.1208527285189808
Global test_loss: 2.300072922706604
Global Precision: 0.12001284981028673
Global Recall: 0.1208527285189808
Global f1score: 0.1038923049969098
50
50
number of selected users 50
Global Trainning Accurancy: 0.12787029088639812
Global Trainning Loss: 2.2939557313919066
Global test accurancy: 0.1207022924819748
Global test_loss: 2.300121192932129
Global Precision: 0.1192624098566934
Global Recall: 0.1207022924819748
Global f1score: 0.10402445011650792
50
50
number of selected users 50
Global Trainning Accurancy: 0.12817854017101035
Global Trainning Loss: 2.2938493061065675
Global test accurancy: 0.1205446756626022
Global test_loss: 2.3001582527160647
Global Precision: 0.1193837010715312
Global Recall: 0.1205446756626022
Global f1score: 0.10423527750238365
50
50
number of selected users 50
Global Trainning Accurancy: 0.12840850233296522
Global Trainning Loss: 2.29374858379364
Global test accurancy: 0.1212972821961148
Global test_loss: 2.3002046298980714
Global Precision: 0.12088276721574998
Global Recall: 0.1212972821961148
Global f1score: 0.10543640060136795
50
50
number of selected users 50
Global Trainning Accurancy: 0.12874891442396022
Global Trainning Loss: 2.2936507654190064
Global test accurancy: 0.1211938522066951
Global test_loss: 2.300267939567566
Global Precision: 0.11994753191025437
Global Recall: 0.1211938522066951
Global f1score: 0.1054206154247238
50
50
number of selected users 50
Global Trainning Accurancy: 0.1287652432182364
Global Trainning Loss: 2.2935433912277223
Global test accurancy: 0.12063645816113552
Global test_loss: 2.3003298568725588
Global Precision: 0.11676850480454196
Global Recall: 0.12063645816113552
Global f1score: 0.10489118004024842
50
50
number of selected users 50
Global Trainning Accurancy: 0.1293517021330187
Global Trainning Loss: 2.2934361028671266
Global test accurancy: 0.12102043299243952
Global test_loss: 2.300401930809021
Global Precision: 0.11755592437740375
Global Recall: 0.12102043299243952
Global f1score: 0.1054381053500852
exp_no  0
0_dataset_CIFAR10_algorithm_FedProx_model_CNN_10_50_0.8_31_07_2024
